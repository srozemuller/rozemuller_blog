


















































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































[{"categories":["Security","Zero Trust"],"contents":"In earlier posts I created several conditional access policies in an automated way. During the time policies may change, users will be added to a specific Azure role, groups are added and so many more.\nAll those changes effect on how conditional access policies act. Because of that, it is recommended to check your conditional access policies environment so now and then.\nIn this blog post, I show to test your conditional access enviroment in an automated way based on different scenario\u0026rsquo;s.\nThis blog post is part of the Zero to Zero Trust series Table Of Contents Main idea Authentication Check Conditional Access policies automated Scenarios Automated check Main idea The main idea of the post is to check the current available conditional access policies based on some scenario\u0026rsquo;s. Checking policies is very usefull from different perspectives. First of all you want to know if the current conditional acces policies work. On the other site, you also want to know if you missed some scenario\u0026rsquo;s.\nIn this post, the starting point is a scenario and we will check if the scenario hits a policy or not. If a specific scenario does not hit any policy you properly missing a conditional access policy.\nAuthentication Checking conditional access policies is not available in the Graph API. To check conditional access policies, we need to authenticate to https://main.iam.ad.ext.azure.com/. Earlier, I showed how to use the Azure AD internal API and how to authenticate.\nIn this blog post, I use the access token that returns from the blog post above.\nCheck Conditional Access policies automated To check conditional access policies automated we need the following API endpoint: https://main.iam.ad.ext.azure.com/api/WhatIf/Evaluate The API endpoint accepts a JSON body that contains the full scenario.\nScenarios As said, the starting point is a scenario. In this post, I test the following scenarios:\nBreak Glass account not MFA (should hit a policy) Admin account biometric (should hit strong auth MFA policy) All users MFA (should hit MFA policy) User log in to Azure Virtual Desktop App with High Risk (should hit MFA policy, but does not hit) (Alert!) The scenarios are each stored in a JSON file. The JSON file content is provided as a body to the evaulate endpoint in the automated check.\nThe scenarios I used are stored at my GitHub\nAutomated check This is the part where the scenarios are checked one by one. This is simple done by the following PowerShell script. First, the JSON file content is read. Next is sending a POST request to the evaluation API \u0026quot;https://main.iam.ad.ext.azure.com/api/WhatIf/Evaluate\u0026quot;\n$body = Get-Content ./Identity/ZeroTrust/ConditionalAccessCheck/admin.highrisk.json $uri = \u0026#34;https://main.iam.ad.ext.azure.com/api/WhatIf/Evaluate\u0026#34; $request = Invoke-WebRequest -uri $uri -Headers $headers -Method post -Body $body ($request.content | ConvertFrom-Json).policies After requesting the API, the API responds a state per policy. An example is stored below, based on the admin at High Risk scenario.\npolicyId : e3ffffd1-1220-4398-a098-46420a3e4cd1 policyName : CA004: Require multifactor authentication for all users unsatisfiedConditions : {0} applied : True controls : @{controlsOr=True; blockAccess=False; challengeWithMfa=True; requireAuthStrength=; compliantDevice=False; domainJoinedDevice=False; approvedClientApp=False; claimProviderControlIds=System.Object[]; requireCompliantApp=False; requirePasswordChange=False; requiredFederatedAuthMethod=0} sessionControls : @{appEnforced=False; cas=False; cloudAppSecuritySessionControlType=0; signInFrequencyTimeSpan=; signInFrequency=0; persistentBrowserSessionMode=0; continuousAccessEvaluation=0; resiliencyDefaults=0; secureSignIn=False; secureApp=False; networkAccessSecurity=} baselineType : 0 policyState : 2 authenticationStrength : filters : @{actorAppAttributes=System.Object[]; actorAppFilterPresent=False; actorAppFilterProblem=False; targetAppAttributes=System.Object[]; targetAppFilterPresent=False; targetAppFilterProblem=False} In the response above we see the \u0026ldquo;CA004: Require multifactor authentication for all users\u0026rdquo; is applied. Interesting in these is the combination applied and policy state (in this case 2). A policy can have one of this states:\nOff (1) On (2) ReportOnly (3) Based on the response I created a small filter that searches for applied policies that are enabled. The policy name and applied state are returned.\n($request.content | ConvertFrom-Json).policies | Where-Object {$_.policyState -eq 2 -and $_.applied} | Select-Object policyName, applied policyName applied ---------- ------- CA004: Require multifactor authentication for all users True CA001: Require strong multifactor authentication for admins True CA102: Block legacy authentication for all users True Based on you own logics it is now possible to create an alert if the correct policy not shows up. Small example:\n$appliedPolicies = ($request.content | ConvertFrom-Json).policies | Where-Object {$_.policyState -eq 2 -and $_.applied} | Select-Object policyName, applied if (\u0026#34;CA003\u0026#34; -notin $appliedPolicies.policyName) { Write-Error \u0026#34;CA003 is not applied\u0026#34; } Thank you for reading my blog check conditional access policy effect automated. I hope you got a bit inspired. Enjoy your day and happy automating üëã ","date":"May 8, 2023","image":null,"permalink":"/check-conditional-access-policy-effect-automated/","title":"Check Conditional Access Policy Effect Automated"},{"categories":["Security","Intune","Zero Trust"],"contents":"Emergency accounts are a fundamental part of any security strategy. They are used to access systems when the primary account is unavailable or locked out. In the past, emergency accounts were often called ‚Äúbreak glass‚Äù accounts. In an earlier post, I explained how to create an emergency account based on a user account with a password. In this post, I will show you how to automate the creation of a passwordless, multi-layered emergency account using Graph API. Table Of Contents The idea The setup Client certificate Create a private key Create a certificate signing request (CSR) Create a self-signed certificate Convert the certificate to a PFX file Create an app registration Graph API permissions explained Create service principal Set the Graph API permissions and admin consent automated Conditional Access Log in process The idea The idea is to log in with a service principal using a client certificate in the first place. With Zero Trust in mind, the principal has the minimal needed permissions that only allow adding users to an Azure Role-enabled group. The group is empty by default and assigned to the MFA Conditional Access policy exclusion list.\nIn the case of an emergency, a person connects to the tenant using the service principal. After logging in, users are added to the exclusion group (layer 1).\nThereafter, the person can log in using the user account as the emergency user (layer 2).\nA big pro of this approach is that the emergency account is passwordless. Also, the emergency account is multi-layered. Even when the service principal is compromised, the attacker can only add users to the exclusion group and needs an extra existing user to log in. The attacker can‚Äôt change the Conditional Access policy.\nThis is a major improvement over a single high-privileged break glass account with only a password.\nThe setup Assuming you already have a Conditional Access policy with MFA enabled, we need the following:\nA service principal with the following permissions:\nRoleManagement.ReadWrite.Directory Users.Read.All (optional), If you know the user object ID of the user you want to add to the exclusion group, you can skip this permission to find the user object id. A client certificate with key. Instead of using an application ID and secret, we will use a client certificate. This is more secure than using a secret.\nAn empty Azure Role-enabled AD group that is assigned to the MFA Conditional Access policy exclusion list.\nIf you don\u0026rsquo;t have a Conditional Access policy with MFA enabled, I would suggest reading my post Protect Privileged Accounts the Zero Trust Way Automated.\nClient certificate The first step is creating a client certificate. Several ways are available, I use OpenSSL to create a self-signed certificate with a private key. Creating a certificate consists of the following steps:\nCreate a private key. Create a certificate signing request (CSR). Create a self-signed certificate. Convert the certificate to a PFX file. Create a private key You can use the following command to create a private key.\nopenssl genrsa -des3 -out cert.key 2048 Create a certificate signing request (CSR) You can use the following command to create a CSR with the created private key from above.\nopenssl req -new -key cert.key -out cert.csr Create a self-signed certificate You can use the following command to create a self-signed certificate using the request and private key. The created .crt file is the certificate and will be uploaded to the Azure AD application.\nopenssl x509 -signkey cert.key -in cert.csr -req -days 365 -out cert.crt Convert the certificate to a PFX file You can use the following command to convert the certificate to a PFX file that contains the private key. This is the certificate that is used to log in with the service principal and distributed to a device. Using a private key avoids that the certificate can be shared easily.\nopenssl pkcs12 -inkey cert.key -in cert.crt -export -out cert.pfx For more information about creating a client certificate, see Create a self signed certificate with OpenSSL.\nCreate an app registration The next step is creating a service principal. In the following steps, I create the following:\nAn app registration with the correct Graph API permissions A service principal based on the app registration An admin consent for the app registration Upload client certificate to the application Graph API permissions explained The Graph API permission landscape consists of two parts from an automation perspective.\nIn the basics, we have an app registration and a service principal. If not using automation only the app registration is needed. You log in interactively with the app registration and grant permissions on behalf of the user.\nIf using automation you need a service principal. The service principal is created based on the app registration and has its own Graph API permissions called oauth2PermissionGrants. This is the admin consent.\nThe code below creates the app registration in the first place. The permissions are the Graph API resource permissions found in the Microsoft Graph API permissions reference. I used the application permissions.\n$appUrl = \u0026#34;https://graph.microsoft.com/beta/applications\u0026#34; $appBody = @{ \u0026#34;displayName\u0026#34; = \u0026#34;EmergencyAccess\u0026#34; \u0026#34;signInAudience\u0026#34; = \u0026#34;AzureADMyOrg\u0026#34; \u0026#34;requiredResourceAccess\u0026#34; = @( @{ \u0026#34;resourceAppId\u0026#34; = \u0026#34;00000003-0000-0000-c000-000000000000\u0026#34; \u0026#34;resourceAccess\u0026#34; = @( @{ \u0026#34;id\u0026#34; = \u0026#34;9e3f62cf-ca93-4989-b6ce-bf83c28f9fe8\u0026#34; # RoleManagement.ReadWrite.Directory \u0026#34;type\u0026#34; = \u0026#34;Role\u0026#34; }, @{ \u0026#34;id\u0026#34; = \u0026#34;df021288-bdef-4463-88db-98f22de89214\u0026#34; # Users.Read.All \u0026#34;type\u0026#34; = \u0026#34;Role\u0026#34; } ) } ) } | ConvertTo-Json -Depth 5 $appRequest = Invoke-WebRequest -Method POST -Uri $appUrl -Headers $authHeader -Body $appBody $appOutput = $appRequest.Content | ConvertFrom-Json In the screenshot below, I created the app registration with the two permissions. As you can see it is missing the admin consent. Create service principal Creating a service principal based on the app registration is quite easy using the command below.\nI just tell the Graph API to create a service principal based on the app registration created above. I use the $appOutput variable from the app registration to create the service principal.\n$spUrl = \u0026#34;https://graph.microsoft.com/beta/servicePrincipals\u0026#34; $spRequest = Invoke-WebRequest -Method POST -Uri $spUrl -Headers $authHeader -Body (@{ \u0026#34;appId\u0026#34; = $appOutput.appId } | ConvertTo-Json) $spOutput = $spRequest.Content | ConvertFrom-Json $spOutput Set the Graph API permissions and admin consent automated In the next step, we need to grant the service principal the permissions. This is called admin consent.\nThe code below creates the admin consent. The $spOutput variable is the service principal created above. The $appOutput variable is the app registration created above.\nWhile adding permissions, you tell the service principal which permissions it needs under which application. In the example below, I tell the service principal that it needs the permissions from the enterprise application 00000003-0000-0000-c000-000000000000 (Microsoft Graph). The permissions in the application are RoleManagement.ReadWrite.Directory and Users.Read.All.\nIn the step below, I first request the internal ID from the Graph API application (resourceId).\n$graphSpUrl = \u0026#34;https://graph.microsoft.com/beta/servicePrincipals?`$filter=appId eq \u0026#39;00000003-0000-0000-c000-000000000000\u0026#39;\u0026#34; $grapSpRequest = Invoke-WebRequest -Method GET -Uri $graphSpUrl -Headers $authHeader $grapshspOutput = ($grapSpRequest.Content | ConvertFrom-Json).value In the body below all pieces are put together. The clientId is the service principal ID. The resourceId is the internal ID of the Graph API application. The scope is holding the API permissions from the Graph API application. The startTime and expiryTime are ignored but required.\n$body = @{ \u0026#34;clientId\u0026#34; = $spOutput.id # EmergencyAccess Service Principal ID \u0026#34;resourceId\u0026#34; = $grapshspOutput.id # Graph Service Principal ID \u0026#34;consentType\u0026#34; = \u0026#34;AllPrincipals\u0026#34; \u0026#34;scope\u0026#34; = \u0026#34;RoleManagement.ReadWrite.Directory Users.Read.All\u0026#34; startTime = Get-Date expiryTime = Get-Date } | ConvertTo-Json $content = Invoke-WebRequest -Uri \u0026#34;https://graph.microsoft.com/beta/oauth2PermissionGrants\u0026#34; -Headers $authHeader -Method POST -Body $body $content.Content | ConvertFrom-Json https://learn.microsoft.com/en-us/graph/api/resources/oauth2permissiongrant?view=graph-rest-beta\nConditional Access https://learn.microsoft.com/en-us/graph/api/conditionalaccesspolicy-update?view=graph-rest-1.0\u0026amp;tabs=http\nLog in process Log in with the service principal using the client certificate. Get the user object ID of the user you want to add to the exclusion group. Add the user to the exclusion group. Log in with the user account as the emergency user. Thank you for reading my blog passwordless, multi-layered break glass alternative automated. I hope you got a bit inspired. Enjoy your day and happy automating üëã ","date":"April 14, 2023","image":"http://localhost:1313/passwordless-multilayered-break-glass-alternative-automated/image.jpg","permalink":"/passwordless-multilayered-break-glass-alternative-automated/","title":"Passwordless, Multi-layered Break Glass Alternative Automated"},{"categories":["Security","Intune","Zero Trust"],"contents":"Emergency accounts are a fundamental part of any security strategy. They are used to access systems when the primary account is unavailable or locked out. In the past, emergency accounts were often called ‚Äúbreak glass‚Äù accounts. In an earlier post, I explained how to create an emergency account based on a user account with a password. In this post, I will show you how to automate the creation of a passwordless, multi-layered emergency account using Graph API.\nTable Of Contents The idea The setup Create an app registration Graph API permissions explained Create service principal Set the Graph API permissions and admin consent automated Client certificate Create a private key Create a certificate signing request (CSR) Create a self-signed certificate Convert the certificate to a PFX file Upload the certificate to the application Azure AD Role assignable group Add group to MFA Conditional Access policy Log in process Log in with the service principal using the client certificate Convert certificate in JWT token with PowerShell Use JWT token to Microsoft login Get the user object ID of the user you want to add to the exclusion group Add the user to the exclusion group. Summary The idea The idea is to log in with a service principal using a client certificate in the first place. With Zero Trust in mind, the principal has the minimal needed permissions that only allow adding users to an Azure Role-assignable group. The group is empty by default and assigned to the MFA Conditional Access policy exclusion list.\nIn the case of an emergency, a person connects to the tenant using the service principal. After logging in, users are added to the exclusion group (layer 1).\nThereafter, the person can log in using the user account as the emergency user (layer 2).\nA big pro of this approach is that the emergency account is passwordless. Also, the emergency account is multi-layered. Even when the service principal is compromised, the attacker can only add users to the exclusion group and needs an extra existing user to log in. The attacker can‚Äôt change the Conditional Access policy.\nThis is a major improvement over a single high-privileged break glass account with only a password.\nThe setup Assuming you already have a Conditional Access policy with MFA enabled, we need the following:\nA service principal with the following permissions: RoleManagement.ReadWrite.Directory Users.Read.All (optional), If you know the user object ID of the user you want to add to the exclusion group, you can skip this permission to find the user object id. I consciously chose to not add the Group.Read.All as a permission. This to make this solution as secure as possible. In the case of a breach, you still don\u0026rsquo;t know to which group you need to add members to.\nA client certificate with key. Instead of using an application ID and secret, we will use a client certificate. This is more secure than using a secret. An empty Azure Role-enabled AD group that is assigned to the MFA Conditional Access policy exclusion list. If you don\u0026rsquo;t have a Conditional Access policy with MFA enabled, I would suggest reading my post Protect Privileged Accounts the Zero Trust Way Automated.\nCreate an app registration The next step is creating a service principal. In the following steps, I create the following:\nAn app registration with the correct Graph API permissions A service principal based on the app registration An admin consent for the app registration Graph API permissions explained The Graph API permission landscape consists of two parts from an automation perspective.\nIn the basics, we have an app registration and a service principal. If not using automation only the app registration is needed. You log in interactively with the app registration and grant permissions on behalf of the user.\nIf using automation you need a service principal. The service principal is created based on the app registration.\nThe code below creates the app registration in the first place. The permissions are the Graph API OAuthPermissions, not confusing with the application roles. I used the application permissions. Using the OAuthPermissions IDs allows the creation of an admin consent under application permissions instead of delegated permissions.\n$appUrl = \u0026#34;https://graph.microsoft.com/beta/applications\u0026#34; $appBody = @{ \u0026#34;displayName\u0026#34; = \u0026#34;EmergencyAccess\u0026#34; \u0026#34;signInAudience\u0026#34; = \u0026#34;AzureADMyOrg\u0026#34; \u0026#34;requiredResourceAccess\u0026#34; = @( @{ \u0026#34;resourceAppId\u0026#34; = \u0026#34;00000003-0000-0000-c000-000000000000\u0026#34; \u0026#34;resourceAccess\u0026#34; = @( @{ \u0026#34;id\u0026#34; = \u0026#34;9e3f62cf-ca93-4989-b6ce-bf83c28f9fe8\u0026#34; # RoleManagement.ReadWrite.Directory \u0026#34;type\u0026#34; = \u0026#34;Role\u0026#34; }, @{ \u0026#34;id\u0026#34; = \u0026#34;df021288-bdef-4463-88db-98f22de89214\u0026#34; # User.Read.All \u0026#34;type\u0026#34; = \u0026#34;Role\u0026#34; } ) } ) } | ConvertTo-Json -Depth 5 $appRequest = Invoke-WebRequest -Method POST -Uri $appUrl -Headers $authHeader -Body $appBody $appOutput = $appRequest.Content | ConvertFrom-Json In the screenshot below, I created the app registration with the two permissions. As you can see it is missing the admin consent.\nCreate service principal Creating a service principal based on the app registration is quite easy using the command below.\nI just tell the Graph API to create a service principal based on the app registration created above. I use the $appOutput variable from the app registration to create the service principal.\n$spUrl = \u0026#34;https://graph.microsoft.com/beta/servicePrincipals\u0026#34; $spRequest = Invoke-WebRequest -Method POST -Uri $spUrl -Headers $authHeader -Body (@{ \u0026#34;appId\u0026#34; = $appOutput.appId } | ConvertTo-Json) $spOutput = $spRequest.Content | ConvertFrom-Json $spOutput Set the Graph API permissions and admin consent automated In the next step, we need to grant the service principal the permissions. This is called admin consent.\nThe code below creates the admin consent. The $spOutput variable is the service principal created above. The $appOutput variable is the app registration created above.\nWhile adding permissions, you tell the service principal which permissions it needs under which application. In the example below, I tell the service principal that it needs the permissions from the enterprise application 00000003-0000-0000-c000-000000000000 (Microsoft Graph). The permissions in the application are RoleManagement.ReadWrite.Directory and Users.Read.All.\nIn the step below, I first request the internal ID from the Graph API application (resourceId). While the appId is always the same, the internal ID differs in a tenant.\n$graphSpUrl = \u0026#34;https://graph.microsoft.com/beta/servicePrincipals?`$filter=appId eq \u0026#39;00000003-0000-0000-c000-000000000000\u0026#39;\u0026#34; $grapSpRequest = Invoke-WebRequest -Method GET -Uri $graphSpUrl -Headers $authHeader $grapshspOutput = ($grapSpRequest.Content | ConvertFrom-Json).value In the body below all pieces are put together. The clientId is the service principal ID. The resourceId is the internal ID of the Graph API application. The scope is holding the API permissions from the Graph API application. The startTime and expiryTime are ignored but required.\n$assignUrl = \u0026#34;https://graph.microsoft.com/beta/servicePrincipals/{0}/appRoleAssignments\u0026#34; -f $spOutput.id $ids = @(\u0026#34;df021288-bdef-4463-88db-98f22de89214\u0026#34;, \u0026#34;9e3f62cf-ca93-4989-b6ce-bf83c28f9fe8\u0026#34;) foreach ($id in $ids) { $body = @{ \u0026#34;principalId\u0026#34; = $spOutput.id \u0026#34;resourceId\u0026#34; = $grapshspOutput.id \u0026#34;appRoleId\u0026#34; = $id } | ConvertTo-Json $content = Invoke-WebRequest -Uri $assignUrl -Headers $authHeader -Method POST -Body $body $content.Content | ConvertFrom-Json } After granting app role permissions , the service principal (under enterprise applications), the permissions look like below.\nFor more information check about the permissions and ID, check the API permissions reference documentation.\nClient certificate The first step is creating a client certificate. Several ways are available, I use OpenSSL to create a self-signed certificate with a private key. Creating a certificate consists of the following steps:\nCreate a private key. Create a certificate signing request (CSR). Create a self-signed certificate. Convert the certificate to a PFX file. Thereafter the client certificate is uploaded to the app registration.\nCreate a private key You can use the following command to create a private key.\nopenssl genrsa -des3 -out cert.key 2048 Create a certificate signing request (CSR) You can use the following command to create a CSR with the created private key from above.\nopenssl req -new -key cert.key -out cert.csr Create a self-signed certificate You can use the following command to create a self-signed certificate using the request and private key. The created .crt file is the certificate and will be uploaded to the Azure AD application.\nopenssl x509 -signkey cert.key -in cert.csr -req -days 365 -out cert.crt Convert the certificate to a PFX file You can use the following command to convert the certificate to a PFX file that contains the private key. This is the certificate that is used to log in with the service principal and distributed to a device. Using a private key avoids that the certificate can be shared easily.\nopenssl pkcs12 -inkey cert.key -in cert.crt -export -out cert.pfx For more information about creating a client certificate, see Create a self signed certificate with OpenSSLself-signed.\nUpload the certificate to the application We have created the certificate and the application. Now it is time to upload the certificate to the application. The application accepts the .crt file in converted binary to Base64String.\nThe first step is to get the certificate\u0026rsquo;s raw content and convert to content to a Base64String format. Reading the content using PowerShell is simple by using the Get-Content -File -Raw command\n$certificate = Get-Content .\\cert.crt -Raw The content in the $certficate variable is now:\n-----BEGIN CERTIFICATE----- MIECsDCCAZgCCQDATkHIiZjVhjANBgkqhkiG9w0BAQsFADAaMQswCQYDVQQGEwJO TDELMAkGA1UECAwCT1YwHhcNMjMwNDE5MTAwOTIwWhcNMjQwNDE4MTAwOTIwWjAa MQswCQYDVQQGEwJOTDELMAkGA1UECAwCT1YwggEiMA0GCSqGSIb3DQEBAQUAA4IB DwAwggEKAoIBAQCaBRnQz4kALtBKWllfuVNZ/L444qMcRBBb6D8Dbr79ZtDw1v89 qOvSdVrtBhM0m2ckQgLt/d+eLp6JBvUIU5QFgfpE4UJnxiHY6yBXHEEZeFuOvIZa g40yWiK577kxI7HWrCVD/CA3ygU4bydn/93afvjYE+R2uC0YOt28aANEoJxwqdWB NzD07EKxTw2WWRkwJ45SfJdke3vJLSWY7BjGHiPIfH6Hs5eMA2yyr1SvQYXDy4oe 6yoRj5/j+He7yP5c+HZoIVrhHtIvzrrjne2EkOKvVoR3VRyUq45VRaHEHRDy1h34 iNV5s34K1JnKvt6mQ2RRL1G+9IfdItpnthdtAgMBAAEwDQYJKoZIhvcNAQELBQAD ggEBAD32cPfN3TG4n3Sq+V2y8hmXbLNTkFEugwzq3LxJusf9t8Z7Jc0qORhhaReT a6k+M611QOKH4VD6Pho2H2QSgHms5G5mLDdwR6+dI+wOTKlUvpov5KFrt+E8028f DxZdFI/Y6dLY8n7pouGiQPe97Cac0zaSUgxSLwqj61Dfwspy2RdgsVKZb0F/ONZb OCitVe8nfdBJIVOdmS0rcBm5y2D/UfHq4kLCV7u3QGtF3HPIrIiqS3vdhKOKg2EM +CIDb4kLrq+AFU9EwAPymZSX42kXMvlYnDBjYz9qgelBEbtRvwhZvoLryKS74Deo 1nik+lS5pmFI7d6kdw8JF12DpOc= -----END CERTIFICATE----- After converting to Base64String with the command below the string is formatted.\n$certUploadKey = [System.Convert]::ToBase64String([System.Text.Encoding]::ASCII.GetBytes($certificate)) LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUNzRENDQVpnQ0NRREFUa0hJaVpqVmhqQU5CZ2txaGtpRzl3MEJBUXNGQURBYU1Rc3dDUVlEVlFRR0V3Sk8KVERFTE1Ba0dBMVVFQ0F3Q1QxWXdIaGNOTWpNd05ERTVNVEF3T1RJd1doY05NalF3TkRFNE1UQXdPVEl3V2pBYQpNUXN3Q1FZRFZRUUdFd0pPVERFTE1Ba0dBMVVFQ0F3Q1QxWXdnZ0VpTUEwR0NTcUdTSWIzRFFFQkFRVUFBNElCCkR3QXdnZ0VLQW9JQkFRQ2FCUm5RejRrQUx0QktXbGxmdVZOWi9MNDQ0cU1jUkJCYjZEOERicjc5WnREdzF2ODkKcU92U2RWcnRCaE0wbTJja... With the code below, I update the application\u0026rsquo;s (not the service principal\u0026rsquo;s) key credentials.\n$appUrl = \u0026#34;https://graph.microsoft.com/v1.0/myorganization/applications/{0}\u0026#34; -f $appOutput.id $certUploadBody = @{ keyCredentials = @( @{ displayName = \u0026#34;emergencyaccess_cert\u0026#34; keyId = (New-Guid).Guid type = \u0026#34;AsymmetricX509Cert\u0026#34; usage = \u0026#34;Verify\u0026#34; key = $certUploadKey } ) } | ConvertTo-Json -Depth 10 Invoke-WebRequest -Uri $appUrl -Method PATCH -Headers $authHeader -Body $certUploadBody The return is a 204 No Content. Saving the output into a variable is not needed.\nAzure AD Role assignable group The next step is creating an Azure AD Role assignable group. The reason why specific this group type is because of the following:\nRole-assignable groups are designed to help prevent potential breaches by having the following restrictions:\nThe membership type for role-assignable groups must be Assigned and can\u0026rsquo;t be an Azure AD dynamic group. An automated population of dynamic groups could lead to an unwanted account being added to the group and thus assigned to the role. This is a pro because now you are forced to assign specific users to the group. Instead of assigning all users for example in the case of a faulty query. Another thing is:\nBy default, only Global Administrators and Privileged Role Administrators can manage the membership of a role-assignable group, but you can delegate the management of role-assignable groups by adding group owners. Also in this case I\u0026rsquo;m sure (almost) that only privileged users can assign users to this group. To create an Azure AD Role assignable group automated I used the code below. Make a note about isAssignableToRole = $true value.\n### Create Azure AD Group $groupName = \u0026#34;CAExcludeGroup\u0026#34; $groupBody = @{ displayName = $groupName mailEnabled = $true\tsecurityEnabled = $true groupTypes = @( \u0026#34;Unified\u0026#34; ) mailNickname = $groupName isAssignableToRole = $true visibility = \u0026#34;Private\u0026#34; } | ConvertTo-Json $group = Invoke-WebRequest -Uri \u0026#34;https://graph.microsoft.com/beta/groups\u0026#34; -Headers $authHeader -Method POST -Body $groupBody $groupOutput = $group.Content | ConvertFrom-Json In the case of an emergency, privileged users can\u0026rsquo;t log in. That means someone else needs to assign users to the emergency group. As mentioned above, also a group owner can assign users to this group. The code below adds the created service principal as an owner to the group.\n### Add owner to group $ownerUrl = \u0026#34;https://graph.microsoft.com/beta/groups/{0}/owners/`$ref\u0026#34; -f $groupOutput.id $ownerBody = @{ \u0026#34;@odata.id\u0026#34; = \u0026#34;https://graph.microsoft.com/beta/directoryObjects/{0}\u0026#34; -f $spOutput.id } | ConvertTo-Json Invoke-WebRequest -Uri $ownerUrl -Headers $authHeader -Method POST -Body $ownerBody Besides the user that has created the group now also the service principal is the owner.\nAdd group to MFA Conditional Access policy The next step is to assign the group to the MFA CA policy. In the first step, I request my MFA policy based on the display name. Based on the output, I use the ID to update excludeGroups object with the group ID.\n$caPolicyUrl = \u0026#34;https://graph.microsoft.com/beta/identity/conditionalAccess/policies?`$filter=displayName eq \u0026#39;CA004: Require multifactor authentication for all users\u0026#39;\u0026#34; $caPolicies = Invoke-WebRequest -Uri $caPolicyUrl -Headers $authHeader -Method GET $caPolicy = ($caPolicies.Content | ConvertFrom-Json).value $updateBody = @{ conditions = @{ users = @{ excludeGroups = @( $groupOutput.id ) } } } | ConvertTo-Json -Depth 10 $updateCaUrl = \u0026#34;https://graph.microsoft.com/beta/identity/conditionalAccess/policies/{0}\u0026#34; -f $caPolicy.id $content = Invoke-WebRequest -Uri $updateCaUrl -Headers $authHeader -Method PATCH -Body $updateBody $content.Content | ConvertFrom-Json Log in process Everything is in place now. We have created an application with a certificate login. The application has permission to add users to the MFA exclusion group. The exclusion group is added to the MFA CA Policy excluded groups.\nNow what to do when $%^\u0026amp;* hits the fan?\nWe must log in with the service principal to add a user to the exclusion group. In the upcoming part, I show how to log in using the service principal and add a user to the exclusion group.\nLog in with the service principal using the client certificate To get a Graph API token for further steps, we need to login with a client credentials using a certificate. Using a certificate is a bit different than loggin in using a normal password. I will skip the details, but the main goal is to create a signature from the certificate and private key. The complete signature (which is the JWT token, aka password) is send to the authentication URL.\nConvert certificate in JWT token with PowerShell To log in with a client certificate we have to convert the certificate into a JWT token. That token acts as a password. Converting a certificate into a JWT token has two main steps. Create a certifcate hash and a private key hash. In the end both are combined\nRequirement is you need a certificate with a private key. That is de .pfx file created above.\nThe first step is converting the certificate itself into a Base64String. This is needed because the content is uploaded in later steps.\n# Create base64 hash of certificate $CertificateBase64Hash = [System.Convert]::ToBase64String($certificate.GetCertHash()) The second step is create a signature from the private key and add it to the certifcate hash.\n# Get the private key object of your certificate $PrivateKey = ([System.Security.Cryptography.X509Certificates.RSACertificateExtensions]::GetRSAPrivateKey($certificate)) # Define RSA signature and hashing algorithm $RSAPadding = [Security.Cryptography.RSASignaturePadding]::Pkcs1 $HashAlgorithm = [Security.Cryptography.HashAlgorithmName]::SHA256 # Create a signature of the JWT $Signature = [Convert]::ToBase64String( $PrivateKey.SignData([System.Text.Encoding]::UTF8.GetBytes($JWT), $HashAlgorithm, $RSAPadding) ) -replace \u0026#39;\\+\u0026#39;, \u0026#39;-\u0026#39; -replace \u0026#39;/\u0026#39;, \u0026#39;_\u0026#39; -replace \u0026#39;=\u0026#39; # Join the signature to the JWT with \u0026#34;.\u0026#34; $JWT = $JWT + \u0026#34;.\u0026#34; + $Signature Use JWT token to Microsoft login In the step below I send a client_credentials authentication request based on the application I created earlier. For the scope I use https://graph.microsoft.com//.default which are all the application\u0026rsquo;s configured permissions.\n# Create a hash with body parameters $Body = @{ client_id = $ApplicationId client_assertion = $JWT client_assertion_type = \u0026#34;urn:ietf:params:oauth:client-assertion-type:jwt-bearer\u0026#34; scope = $Scope grant_type = \u0026#34;client_credentials\u0026#34; } # Log in with certificate to change user group membership $Url = \u0026#34;https://login.microsoftonline.com/$TenantName/oauth2/v2.0/token\u0026#34; # Use the self-generated JWT as Authorization $Header = @{ Authorization = \u0026#34;Bearer $JWT\u0026#34; } # Splat the parameters for Invoke-Restmethod for cleaner code $paramters = @{ ContentType = \u0026#39;application/x-www-form-urlencoded\u0026#39; Method = \u0026#39;POST\u0026#39; Body = $Body Uri = $Url Headers = $Header } $Request = Invoke-RestMethod @paramters Check the Access token request with a certificate for more about client credentials login using a certificate.\nI created a script that accepts a tenant name, the PFX certificate and the application ID. When running the script, it asks for the certificate\u0026rsquo;s password. At last it returns the access token that can be use later.\nThe script how to create a JWT token from a certificate is stored at my GitHub\nGet the user object ID of the user you want to add to the exclusion group Using the token received from the certificate to JWT converter script I create a header and send a request to search a user in the AD.\n$token = ./get-jwt-accesstoken.ps1 -TenantName tenant.onmicrosoft.com -CertPath ./cert.pfx -ApplicationId created-breakglass-appID $certGraphHeader = @{ \u0026#39;Content-Type\u0026#39; = \u0026#39;application/json\u0026#39; Authorization = \u0026#34;Bearer {0}\u0026#34; -f $token } $userUrl = \u0026#34;https://graph.microsoft.com/beta/users?`$filter=userPrincipalName eq \u0026#39;user@domain.com\u0026#39;\u0026#34; $users = Invoke-WebRequest -Uri $userUrl -Method GET -Headers $certGraphHeader $user = ($users.Content | ConvertFrom-Json).value The result in the user variable is the ID that will be added to the group.\nAdd the user to the exclusion group. To keep all as secure as possible, the application has no group list permissions. Make sure you write down the correct group ID.\nThe last step is adding the user to the group based on the group ID.\n$groupUrl = \u0026#34;https://graph.microsoft.com//beta/groups/{0}/members/`$ref\u0026#34; -f $groupOutput.id $addMemberBody = @{ \u0026#34;@odata.id\u0026#34; = \u0026#34;https://graph.microsoft.com/v1.0/directoryObjects/{0}\u0026#34; -f $user.id } | ConvertTo-Json Invoke-WebRequest -Uri $groupUrl -Method POST -Headers $certGraphHeader -Body $addMemberBody At last log in with the user account as the emergency user.\nSummary You have created a passwordless, multi-layered break glass account based on a service principal that has permissoins to add a normal user to the MFA exclusion group. To log in you need a password protected certificate.\nObjects to store safely:\nCertifcate PFX file Certificate password Application ID Exclusion group ID The code I used is stored in a script on my GitHub\nThank you for reading my blog passwordless, multi-layered break glass alternative automated. I hope you got a bit inspired. Enjoy your day and happy automating üëã ","date":"April 14, 2023","image":"http://localhost:1313/passwordless-multilayered-break-glass-alternative-automated/image.jpg","permalink":"/passwordless-multilayered-break-glass-alternative-automated/","title":"Passwordless, Multi-layered Break Glass Alternative Automated"},{"categories":["Security","Microsoft Intune","Zero Trust"],"contents":"In earlier steps, I explained how to control identity and devices to connect to your environment. Now people are connected to your environment safely, the next step is to make sure your data won\u0026rsquo;t walk out.\nIn this step of the Zero to Zero Trust journey we will look at the first step of how to keep your data safe. In this blog, I show how to automate the creation of App Protection Policies. Intune App Protection policies help protect corporate data even if a device itself is not managed.\nThis blog post is part of the Zero to Zero Trust series Table Of Contents Introduction Intune App Protection Policies Data transfer override Access requirements override Conditional launch override Create App Protection Policies automated Assign App Protection Policies to groups automated Conditional Access Policies Introduction This blog post focuses on the green box on the right side. Some of the steps are already explained in the previous blogs and are marked in the red box. The orange box shows the steps that are explained in this blog.\nAll blogs related to the Zero to Zero Trust journey are listed in the Zero to Zero Trust overview.\nAll blogs related to level 1 Zero Trust are listed in the Zero Trust Level 1 overview.\nThe full Zero Trust plan is available here.\nIntune App Protection Policies With APP, Intune creates a wall between your organization\u0026rsquo;s data and personal data and has a focus on mobile iOS and Android devices. The app protection policies define which apps are allowed to access your data. After creating an App Protection policy, you enforce data protection with a conditional access policy.\nCurrently, the following App Protection Policy types are available:\niOS/iPadOS Android The Windows Information Protection (WIP) policies are supported but deprecated. Microsoft Purview replaces WIP and will be discussed in a later blog. For more information about WIP, see Get ready for Windows Information Protection in Windows 10/11.\nIn the basics, both types are the same in common. In the first step, you select the apps which you want to protect. In the second step, you can configure the protection settings. The enforcement settings are the same for both platforms.\nFor level 1 protection I selected to secure only the Microsoft Apps.\nData transfer override Unlike the default settings, I choose to block the backup service in the iOS/iPadOS and Android settings. This is because I don\u0026rsquo;t want my data on personal cloud storage like iCloud or Google Drive.\nAnother setting I changed is the printing. I don\u0026rsquo;t want my data to be printed. This is because I don\u0026rsquo;t want my data to be printed from mobile devices since mobile devices are more likely to be lost or stolen. Access requirements override In the access requirements blade, I selected to block simple pins and require a work- or school account. This is because I don\u0026rsquo;t want my data to be accessed by personal accounts. This is also a good way to prevent data leakage.\nConditional launch override In the conditional launch override blade, I added some extra conditions. In the app conditions part, I selected to block access in the case a user has been disabled. If a device has been lost, the first thing you may want to do is disable a user. If a user is disabled, access to the app is stopped immediately.\nCreate App Protection Policies automated As mentioned in the introduction, app protection policies have two types. When automating the creation of the policies, you need to use also two different Graph API endpoints.\nhttps://graph.microsoft.com/beta/deviceAppManagement/iosManagedAppProtections https://graph.microsoft.com/beta/deviceAppManagement/androidManagedAppProtections Creating the policies is done in the same way as the conditional access policies. The only difference is that you need to use the correct endpoint. The JSON file for the iOS/iPadOS policy is stored in the GitHub repository\nIn the example below, I create an iOS app protection policy automated.\n$iosUrl = \u0026#34;https://graph.microsoft.com/beta/deviceAppManagement/iosManagedAppProtections\u0026#34; $iosBody = Get-Content .\\lvl1.appprotection.ios.json $iosPolicy = Invoke-RestMethod -uri $iosUrl -Method POST -Headers $authHeader -ContentType \u0026#39;application/json\u0026#39; -body $iosBody In the example below, I create an Andriod app protection policy automated.\n$androidUrl = \u0026#34;https://graph.microsoft.com/beta/deviceAppManagement/androidManagedAppProtections\u0026#34; $androidBody = Get-Content .\\lvl1.appprotection.android.json $androidPolicy Invoke-RestMethod -uri $androidUrl -Method POST -Headers $authHeader -ContentType \u0026#39;application/json\u0026#39; -body $androidBody Assign App Protection Policies to groups automated The assignment process consists of two steps. Since application protection policies only support AD groups, we first need to find the Azure AD group ID. To find the group ID, I also use the Graph API. In the example below, I search for the All Users group first and then assign the app protection policy to the group.\n$allUsersGroup = Invoke-RestMethod -uri \u0026#34;https://graph.microsoft.com/beta/groups?`$filter=displayName eq \u0026#39;All Users\u0026#39;\u0026#34; -Method GET -Headers $authHeader $body = @{ \u0026#34;assignments\u0026#34; = @( @{ target = @{ groupId = $allUsersGroup.value.id \u0026#34;@odata.type\u0026#34; = \u0026#34;#microsoft.graph.groupAssignmentTarget\u0026#34; } } ) } | ConvertTo-Json -Depth 5 $androidPolicyUrl = \u0026#34;https://graph.microsoft.com/v1.0/deviceAppManagement/androidManagedAppProtections(\u0026#39;{0}\u0026#39;)/assign\u0026#34; -f $androidPolicy.Id Invoke-RestMethod -uri $androidPolicyUrl -Method POST -Headers $authHeader -ContentType \u0026#39;application/json\u0026#39; -body $body Conditional Access Policies To make sure the App Protection Policy is enforced, we return to conditional access policies. In the conditional access policy, you select the App Protection Policy and the users or groups you want to apply the policy.\nHow to automate conditional access policy creation is explained in the previous blog.\nThe needed policies are:\nCA202: Require app protection policy with mobile devices. CA203: Block Exchange ActiveSync on all devices Both policies are also stored as a JSON file in the GitHub repository.\nThank you for reading my blog zero trust data protection using app protection policies automated. I hope you got a bit inspired. Enjoy your day and happy automating üëã ","date":"March 30, 2023","image":"http://localhost:1313/zero-trust-data-protection-using-app-protection-policies-automated/image.png","permalink":"/zero-trust-data-protection-using-app-protection-policies-automated/","title":"Zero Trust Data Protection Using App Protection Policies Automated"},{"categories":null,"contents":" Table Of Contents Thank you for reading my blog 2023 03 30 winget app not supported in preview in intune. I hope you got a bit inspired. Enjoy your day and happy automating üëã ","date":"March 30, 2023","image":"http://localhost:1313/zero-trust-data-protection-using-app-protection-policies-automated/image.png","permalink":"/2023-03-30-winget-app-not-supported-in-preview-in-Intune/","title":"2023 03 30 Winget App Not Supported in Preview in Intune"},{"categories":null,"contents":" Table Of Contents Thank you for reading my blog 2023 03 26 automate simulate risk detections. I hope you got a bit inspired. Enjoy your day and happy automating üëã ","date":"March 26, 2023","image":"http://localhost:1313/zero-trust-data-protection-using-app-protection-policies-automated/image.png","permalink":"/2023-03-26-automate-simulate-risk-detections/","title":"2023 03 26 Automate Simulate Risk Detections"},{"categories":["Security","Intune","Zero Trust"],"contents":"In this blog post, I explain the Endpoint Privileged Management (EPM) feature. I will explain how to implement this feature with Microsoft Intune and Graph API. I will also explain how to find a balance between security and usability.\nhttps://techcommunity.microsoft.com/t5/microsoft-intune-blog/enable-windows-standard-users-with-endpoint-privilege-management/ba-p/3755710 Table Of Contents Thank you for reading my blog zero trust endpoint privileged management. I hope you got a bit inspired. Enjoy your day and happy automating üëã ","date":"March 26, 2023","image":"http://localhost:1313/zero-trust-data-protection-using-app-protection-policies-automated/image.png","permalink":"/zero-trust-endpoint-privileged-management/","title":"Zero Trust Endpoint Privileged Management"},{"categories":["Security","Azure AD","Zero Trust"],"contents":"This is the 3rd step in the series Zero to Zero Trust in an automated way. In the previous steps, we configured an emergency account and protected the account with conditional access policies. The conditional access policies control the log-in process. In this step, we configure access policies for other users and devices.\nThis blog post is part of the Zero to Zero Trust series Table Of Contents Introduction Prerequisites Authentication Add custom banned password list Identity protection Conditional access policies Device protection Filters Configure compliance policies Assign compliance policy to filter Configure configuration profiles OS Update policies Disable default compliancy Wrap Up Introduction This blog post focuses on the red boxes.\nAll blogs related to the Zero to Zero Trust journey are listed in the Zero to Zero Trust overview.\nAll blogs related to level 1 Zero Trust are listed in the Zero Trust Level 1 overview.\nThe full Zero Trust plan is available here.\nPrerequisites Before using the Zero Trust identity and device access policies that are recommended, your tenant needs to meet prerequisites. The full prerequisites list is stored here.\nConsidering the list, the Configure name locations and Multi Factory Authentication are in place already. From the list, I\u0026rsquo;m focussing now on Azure AD password protection and Identity protection.\nAuthentication Some prerequisites are part of the Azure AD that has no Graph API or Azure Management API endpoint. To configure these parts, we need the Azure AD backend API main.iam.ad.ext.azure.com\nEarlier I wrote a post on how to authenticate to the backend API. You can find that post here. I use the header used in the code that I picked from this part\nAdd custom banned password list Microsoft has a global banned password list that is enabled for every user automatically. You don\u0026rsquo;t have to enable a feature or something. However this is a long list that avoids creating weak passwords, there could be a situation where you want to add your passwords as well. In this example, I configure a small additional banned password list.\nNOTE: The banned password needs to have a min length of 4 characters and a max length of 16 characters.\n$passwordPolicyUrl = \u0026#34;https://main.iam.ad.ext.azure.com/api/AuthenticationMethods/PasswordPolicy\u0026#34; $body = @{ lockoutThreshold = 10 lockoutDurationInSeconds = 60 enforceCustomBannedPasswords = $true customBannedPasswords = @( \u0026#34;anotherweakpwd1\u0026#34; \u0026#34;anotherweakpwd2\u0026#34; \u0026#34;anotherweakpwd3\u0026#34; ) enableBannedPasswordCheckOnPremises = $true bannedPasswordCheckOnPremisesMode = 1 } | ConvertTo-Json $params = @{ uri = $passwordPolicyUrl method = \u0026#34;POST\u0026#34; headers = $headers body = $body } Invoke-RestMethod @params For more information about password policies, check the documentation about the custom banned password list.\nIdentity protection Most of the identity protection part is handled by conditional access policies. Check my blog about securing privileged accounts automated. But more settings should be checked under the Azure AD - Security - identity protection blade. Users at risk detected alerts and Weekly digest, which are default enabled by Microsoft could be disabled somehow. The commands below enable risk detection events and weekly digest notifications.\nWe need to authenticate to the Graph API with the correct scope. I almost use the same token request where I changed the resource and scope.\nFirst, I request the current state. This is to get all current users that are on the notification list.\nGlobal Administrators are on the list by default. The first 20 identities on the list get a notification (per role).\nNeeded permissions: IdentityRiskEvent.ReadWrite.All\n$uri = \u0026#34;https://graph.microsoft.com/beta/identityProtection/settings/notifications\u0026#34; $currentState = Invoke-WebRequest -Headers $graphHeader -Uri $uri -Method GET $currentState.notificationRecipients.Foreach({ $_.isRiskyUsersAlertsRecipient = $true $_.isWeeklyDigestRecipient=$true # Remove the properties that are not needed $_.PSObject.Properties.Remove(\u0026#39;DisplayName\u0026#39;) $_.PSObject.Properties.Remove(\u0026#39;email\u0026#39;) $_.PSObject.Properties.Remove(\u0026#39;roles\u0026#39;) }) $currentState.isWeeklyDigestEnabled = $true $currentState.minRiskLevel = \u0026#34;Medium\u0026#34; $body = $currentState | ConvertTo-Json -Depth 5 Invoke-WebRequest -Headers $graphHeader -Uri $uri -Method PATCH -Body $body -ContentType \u0026#34;application/json\u0026#34; More information about configuring notifications can be found here\nConditional access policies As mentioned before, most of the identity protection part is handled by conditional access policies. Check my blog about securing privileged accounts automated. In the blog post, I configure the conditional access policies for the emergency account. In this part, we configure the conditional access policies for all users and make sure they only log in from compliant devices.\nConsidering the image below, we need to configure the red-circled conditional access policies.\nThe purple-circled conditional access policies are already configured in the previous blog.\nThe policies are stored on my GitHub\nTo enroll the normal user conditional access policies please read the import Zero Trust conditional access policies automated part\nRun the following command to import the conditional access policies for normal users. NOTE: The state in the file is set to report only. Remove the state line or enable the policies manually.\n$files = Get-ChildItem -Path \u0026#39;./Identity/ZeroTrust/CommonIdentityDeviceProtection\u0026#39; -Filter *.json foreach ($file in $files) { $caPolUrl = \u0026#34;https://graph.microsoft.com/beta/identity/conditionalAccess/policies\u0026#34; $params = @{ uri = $caPolUrl method = \u0026#34;POST\u0026#34; headers = $graphHeader body = get-content $file } Invoke-RestMethod @params } NOTE: The policies I provided are just a subset of a wide range of policies. I would suggest taking a look at Kenneth van Surksum his website to find out a wide range of conditional access policies. He explains the scenarios where to use them.\nDevice protection This is the part where Mircosoft Intune comes in. In the upcoming steps, we will configure the following settings:\nRequire device compliance Require managed devices Before enabling these settings, we need to make sure that the devices are compliant. To get the compliance status, we need to configure the compliance policies in Intune.\nFilters Because Windows 10 and Windows 11 both are under the Windows 10 and later category, we need to create a filter to find the Windows 11 devices. This is because Windows 10 and Windows 11 have different build numbers to check on later in the compliance policies.\nIt is recommended to use filters above dynamic groups to assign compliance policies in Intune. This way, you can assign the correct compliance policy to the correct device type. In this example, I create a filter for Windows 11. All filters are stored on my GitHub\n{ \u0026#34;displayName\u0026#34; : \u0026#34;Windows 11\u0026#34;, \u0026#34;description\u0026#34; : \u0026#34;This filter finds all Windows 11 machines\u0026#34;, \u0026#34;platform\u0026#34; : \u0026#34;windows10AndLater\u0026#34;, \u0026#34;rule\u0026#34; : \u0026#34;(device.osVersion -contains \\\u0026#34;10.0.22000\\\u0026#34;) or (device.osVersion -contains \\\u0026#34;10.0.22621\\\u0026#34;) and (device.model -ne \\\u0026#34;Virtual Machine\\\u0026#34;)\u0026#34; } The rule is needed to find the Windows 11 devices. In later steps, we will use the filter to assign the compliance policy to the correct devices. For Windows 10 and Windows 11, the configuration profile is different.\nFor Windows 10, the rule is (device.osVersion -contains \\\u0026quot;10.0.19044\\\u0026quot;) or (device.osVersion -contains \\\u0026quot;10.0.19045\\\u0026quot;).\nCreate the filter with the following command:\n$filterUrl = \u0026#34;https://graph.microsoft.com/beta/deviceManagement/assignmentFilters\u0026#34; $filterParams = @{ URI = $filterUrl Method = \u0026#34;POST\u0026#34; Headers = $authHeader Body = Get-Content .\\filters.windows11.json } Invoke-RestMethod @filterParams For more information about creating filters automated, check my blog post about Intune filters explained create automated and assignments\nThe build number overview can be found here\nConfigure compliance policies It is recommended to create a compliance policy for each device type. In this example, I create a compliance policy for Windows 11. The difference between them is the OS version build numbers. The policies are stored on my GitHub\nCreating and assigning compliance policies (with filters) is a two-step process. First, we need to create a compliance policy. Second, we need to assign the compliance policy to the correct filters.\nThe whole process is explained in my blog post about Intune compliance policies explained create automated\nIn basics we create a compliance policy with the following command:\n$compliancyUrl = \u0026#34;https://graph.microsoft.com/beta/deviceManagement/deviceCompliancePolicies/\u0026#34; $filterParams = @{ URI = $compliancyUrl Method = \u0026#34;POST\u0026#34; Headers = $graphHeader Body = Get-Content .\\compliance.windows11.json } $policy = Invoke-RestMethod @filterParams $policy Save the output into a variable. This variable is needed to assign the compliance policy to the correct filters.\nAssign compliance policy to filter In the next step, we request the Windows 11 filter. This filter is needed to assign the compliance policy to the correct devices.\n$filterUrl = \u0026#34;https://graph.microsoft.com/beta/deviceManagement/assignmentFilters\u0026#34; $filterParams = @{ URI = $filterUrl Method = \u0026#34;GET\u0026#34; Headers = $graphHeader } $filters = Invoke-RestMethod @filterParams $win11Filter = $filters.value | Where-Object {$_.DisplayName -eq \u0026#34;Windows 11\u0026#34;} In the last step, we assign the compliance policy to the correct filter. This is done with the following command:\n$assignBody = @{ assignments = @( @{ \u0026#34;target\u0026#34; = @{ \u0026#34;@odata.type\u0026#34; = \u0026#34;#microsoft.graph.allDevicesAssignmentTarget\u0026#34; deviceAndAppManagementAssignmentFilterId = $win11Filter.id deviceAndAppManagementAssignmentFilterType = \u0026#34;include\u0026#34; } } ) } | ConvertTo-Json -Depth 5 $compliancyUrl = \u0026#34;https://graph.microsoft.com/beta/deviceManagement/deviceCompliancePolicies/{0}/assign\u0026#34; -f $policy.id $filterParams = @{ URI = $compliancyUrl Method = \u0026#34;POST\u0026#34; Headers = $graphHeader Body = $assignBody } Invoke-RestMethod @filterParams This step must be repeated for every compliance policy. If no filters are used, then skip the GET filter step and remove the FilterId and FilterType lines in the POST body.\nConfigure configuration profiles Some settings in compliance policies need a configuration profile to configure a specific setting on the device. Take Bitlocker for example. BitLocker must be installed on the device to get compliant. The configuration profile handles the Bitlocker installation and configures common security settings.\nIn the example below, I create a configuration profile for all Windows devices. All configuration profiles are stored on my GitHub\n$compliancyUrl = \u0026#34;https://graph.microsoft.com/beta/deviceManagement/configurationPolicies\u0026#34; $filterParams = @{ URI = $compliancyUrl Method = \u0026#34;POST\u0026#34; Headers = $graphHeader Body = Get-Content .\\windows.endpointprotection.json } $result = Invoke-RestMethod @filterParams After creation, we need to assign the configuration profile to the correct compliance policy. Assigning configuration profiles works the same as assigning compliance profiles. Use the command above to assign the configuration profile to the correct compliance policy.\nOS Update policies In this part of the blog post, I will explain how to configure OS update policies. Planning updates is a complex topic and it is recommended to use several update rings instead of updating all devices at once. I will not go into detail about the planning. I will only explain how to configure the OS update policies.\n$updateParams = @{ URI = \u0026#34;https://graph.microsoft.com/beta/deviceManagement/deviceConfigurations\u0026#34; Method = \u0026#34;POST\u0026#34; Headers = $graphHeader Body = Get-Content .\\update.windows.json } Invoke-RestMethod @updateParams Again we need to assign the OS update policy to the correct devices. In the case of Windows make sure you select the correct filters as discussed in the filters section.\nThe OS update policies are stored on my GitHub\nDisable default compliancy Considering Zero Trust, we need to disable the feature below. This feature is enabled by default and it sets all devices to a compliant state if no policy is assigned. The most ironic part of this is the name of the setting. It is called secureByDefault which is default set to false. In the portal, the false setting represents the compliant state and sets all devices to a compliant state if no policy is assigned.\nTo disable this feature, we need to create a configuration profile with the following command:\n$settingsBody = @{ settings = @{ secureByDefault = $true } } | ConvertTo-Json $secureByDefault = @{ URI = \u0026#34;https://graph.microsoft.com/beta/deviceManagement\u0026#34; Method = \u0026#34;PATCH\u0026#34; Headers = $graphHeader Body = $settingsBody } Invoke-RestMethod @secureByDefault Wrap Up In this blog post, I explained how to add extra security by configuring a banned password list into Azure AD. The next step was configuring risk and health alerts. However these are enabled by default, it is good to be sure. Thereafter the \u0026rsquo;normal\u0026rsquo; user conditional access policies were deployed automated. The next part of the blog focus on device protection in Intune. I showed an example of configuring filters (for assignments) and creating compliance- and configuration policies for Windows 11 devices with Intune.\nAt last, I explained how to disable the default compliance setting in Intune automated. This setting is enabled by default and it sets all devices to a compliant state if no policy is assigned.\nThank you for reading my blog zero trust common identity and device access protection. I hope you got a bit inspired. Enjoy your day and happy automating üëã ","date":"March 14, 2023","image":"http://localhost:1313/zero-trust-common-identity-and-device-access-protection/image.jpg","permalink":"/zero-trust-common-identity-and-device-access-protection/","title":"Zero Trust Common Identity and Device Access Protection"},{"categories":["Security","Azure AD","Zero Trust"],"contents":"Identities are the key to environments and must be configured as securely as possible. To achieve this goal, conditional access policies are indispensable. In this blog post, I show how to secure privileged accounts that need more attention than MFA only. This post shows how to configure Zero Trust conditional access policies in an automated way.\nThis is the next step in our Zero to Zero Trust journey.\nUsing conditional access policies requires one of the following plans:\nAzure Active Directory Premium P1 or P2 Microsoft 365 Business Premium Microsoft 365 E3 or E5 Enterprise Mobility \u0026amp; Security E3 or E5 This blog post is part of the Zero to Zero Trust series Table Of Contents Introcuction Protect your Microsoft 365 privileged accounts Disable legacy per-user MFA Turn off security defaults automated Implement Conditional access policies automated Zero Trust Conditional Access policies scenarios according to privileged accounts Create locations automated Import Zero Trust Conditional Access policies automated Verify Further in this blog multi-factor authentication (MFA) is configured. Imagine what happens when the MFA service is not working for some reason. In that case, you can\u0026rsquo;t log in. To avoid situations like that, creating a highly privileged cloud-based user account is recommended. This user account has global admin permissions, with NO MFA, and is also called a break-glass account.\nI\u0026rsquo;ve created a separate blog post about creating break glass accounts in an automated way, this is the first step.\nIntrocuction This blog post focuses on the red boxes. The first step is to create a break-glass account. This account is used to fix issues when MFA is not working. The second step is to configure conditional access policies to protect privileged accounts.\nAll blogs related to the Zero to Zero Trust journey are listed in the Zero to Zero Trust overview.\nAll blogs related to level 1 Zero Trust are listed in the Zero Trust Level 1 overview.\nThe full Zero Trust plan is available here.\nProtect your Microsoft 365 privileged accounts Now we can\u0026rsquo;t lock out anymore, it is time to protect all other accounts with MFA. To protect these accounts, we use conditional access policies. When heading to the conditional access policies blade you see several categories, like Zero Trust, when clicking create from templates.\nBy clicking the tab, you get an overview of policies that fit the Zero Trust philosophy. However, there are more conditional access policies to implement.\nBefore diving into specific settings, some actions must be done first. Otherwise, the policies won\u0026rsquo;t affect your tenant.\nDisable legacy per-user MFA (From Microsoft) If you\u0026rsquo;ve previously turned on per-user MFA, you must turn it off before enabling Security defaults. Check the docs to turn off legacy per-user MFA\nTurn off security defaults automated Security defaults are a Microsoft-managed basic security level to ensure that all organizations have at least the minimum security settings enabled at no extra cost. Think about forcing MFA and blocking legacy authentication.\nThat sounds great and why should you use a conditional access policy?\nWell, if you have one location and only users that should use MFA anyway, security defaults are fine. But, in the case of excluding trusted locations, excluding specific users (break glass) or applications, conditional access will help you.\n$secDefaultUrl = \u0026#34;https://graph.microsoft.com/beta/policies/identitySecurityDefaultsEnforcementPolicy\u0026#34; $jsonBody = @{ isEnabled = $false } | ConvertTo-Json $params = @{ uri = $secDefaultUrl method = \u0026#34;PATCH\u0026#34; headers = $graphHeader body = $jsonBody } Invoke-RestMethod @params The header contains an access token based on the graph endpoint.\nFor more information about security defaults, check the security defaults overview.\nImplement Conditional access policies automated At this point, you have prepared your tenants to use conditional access policies. A lot is written about conditional access policies. Because of that, I will discuss the basics only. A conditional access policy environment can be very complex, It is a good idea to write down your scenarios and dimensions. Scenario\u0026rsquo;s like blocking legacy authentication, forcing MFA from untrusted locations, or blocking access for a specific user from untrusted locations. For dimensions, you should think about users, locations, devices or apps.\nNow it is time to configure conditional access policies in an automated way. The policies below, are ONLY to protect privileged accounts. Protecting normal user accounts is discussed later in this series.\nZero Trust Conditional Access policies scenarios according to privileged accounts Conditional access scenarios are environment specific. It depends on your situation and internal security policies. The examples below are just some scenarios I think strengthen your Zero Trust policy. The scenarios are:\nNr. Scenario Dimensions 1 An MFA conditional access policy for all admin roles with the strongest authentication strength, the break glass account is excluded. Users 2 A conditional access policy that allows break glass account logins from a trusted location only. Users, Locations 3 A policy that has a 1-hour MFA sign-in frequency for admin roles. Users 4 Force password change when admin roles are at medium risk. Users 5 Block access for admin roles that are at high risk. (the above policy is mandatory to make this work the right way), the break glass account is excluded. Users To keep the break-glass account safe, monitoring of the account risk state is needed. This is discussed in a later post in this series.\nThe files used in the code below are stored in my repository.\nWARNING: The default states in the files are enabledForReportingButNotEnforced. Just to be sure you don\u0026rsquo;t lock yourself out after importing. Change the state to enabled or remove the complete line.\nCreate locations automated Some policies have a location dimension. In a conditional access policy, a location object can have different values: All, Trusted, Untrusted, or a specific location. To make the locations part work correctly, we must create locations first. Because locations are environment-specific, the location objects are empty in my templates. After creating the locations, please update the policies with your location IDs.\nCreating names-locations automated can be done with the commands below.\n$locationUrl = \u0026#34;https://graph.microsoft.com/beta/identity/conditionalAccess/namedLocations\u0026#34; $params = @{ uri = $locationUrl method = \u0026#34;POST\u0026#34; headers = $graphHeader body = get-content \u0026#39;./Identity/ZeroTrust/ProtectPrivilegedAccounts/locations.trusted.home.json\u0026#39; } Invoke-RestMethod @params The returned ID is the location ID that must be used in the location object.\n\u0026#34;locations\u0026#34;: { \u0026#34;includeLocations\u0026#34;: [ \u0026#34;locationID\u0026#34; ], \u0026#34;excludeLocations\u0026#34;: [ \u0026#34;AllTrusted\u0026#34; ] }, Import Zero Trust Conditional Access policies automated With the code below I import a JSON file that holds the conditional access policy configuration into my tenant.\n$caPolUrl = \u0026#34;https://graph.microsoft.com/beta/identity/conditionalAccess/policies\u0026#34; $params = @{ uri = $caPolUrl method = \u0026#34;POST\u0026#34; headers = $graphHeader body = get-content \u0026#39;./Identity/ZeroTrust/ProtectPrivilegedAccounts/CA001: Require biometric multifactor authentication for admins.json\u0026#39; } Invoke-RestMethod @params Let me explain the JSON file a bit. While most settings are empty, the user IncludeRoles object is filled with Azure built-in role IDs. The IDs are documented here. If you like to have more roles in the policy, just add the ID to the list.\n\u0026#34;users\u0026#34;: { \u0026#34;includeUsers\u0026#34;: [], \u0026#34;excludeUsers\u0026#34;: [], \u0026#34;includeGroups\u0026#34;: [], \u0026#34;excludeGroups\u0026#34;: [], \u0026#34;includeRoles\u0026#34;: [ \u0026#34;62e90394-69f5-4237-9190-012177145e10\u0026#34;, \u0026#34;194ae4cb-b126-40b2-bd5b-6091b380977d\u0026#34;, \u0026#34;f28a1f50-f6e7-4571-818b-6a12f2af6b6c\u0026#34;, \u0026#34;29232cdf-9323-42fd-ade2-1d097af3e4de\u0026#34;, \u0026#34;b1be1c3e-b65d-4f19-8427-f6fa0d97feb9\u0026#34;, \u0026#34;729827e3-9c14-49f7-bb1b-9608f156bbb8\u0026#34;, \u0026#34;b0f54661-2d74-4c50-afa3-1ec803f12efe\u0026#34;, \u0026#34;fe930be7-5e62-47db-91af-98c3a49a38b1\u0026#34;, \u0026#34;c4e39bd9-1100-46d3-8c65-fb160da0071f\u0026#34;, \u0026#34;9b895d92-2cd3-44c7-9d02-a6ac2d5ea5c3\u0026#34;, \u0026#34;158c047a-c907-4556-b7ef-446551a6b5f7\u0026#34;, \u0026#34;966707d0-3269-4727-9be2-8c3a10f19b9d\u0026#34;, \u0026#34;7be44c8a-adaf-4e2a-84d6-ab2649e08a13\u0026#34;, \u0026#34;e8611ab8-c189-46e8-94e1-60213ab1f814\u0026#34; ], \u0026#34;excludeRoles\u0026#34;: [], \u0026#34;includeGuestsOrExternalUsers\u0026#34;: null, \u0026#34;excludeGuestsOrExternalUsers\u0026#34;: null } For the biometric part, we need the authenticationStrength block under grantControls. Check my other blog about Deploy and monitor conditional access authentication strength automated for the correct IDs.\n\u0026#34;authenticationStrength\u0026#34;: { \u0026#34;id\u0026#34; : \u0026#34;00000000-0000-0000-0000-000000000004\u0026#34; } The other scenarios work all the same. Read the JSON content and run the PowerShell code.\nI created a small loop to import all policies at once.\n$files = Get-ChildItem -Path \u0026#39;./Identity/ZeroTrust/ProtectPrivilegedAccounts\u0026#39; -Filter *.json foreach ($file in $files) { $caPolUrl = \u0026#34;https://graph.microsoft.com/beta/identity/conditionalAccess/policies\u0026#34; $params = @{ uri = $caPolUrl method = \u0026#34;POST\u0026#34; headers = $graphHeader body = get-content $file } Invoke-RestMethod @params } Verify The error below is one of the results, after importing, when using a break-glass account from an untrusted location.\nThe files are stored on my GitHub Thank you for reading my blog protect privileged accounts the zero trust way automated. I hope you got a bit inspired. Enjoy your day and happy automating üëã ","date":"March 5, 2023","image":"http://localhost:1313/protect-privileged-accounts-the-zero-trust-way-automated/image.jpg","permalink":"/protect-privileged-accounts-the-zero-trust-way-automated/","title":"Protect Privileged Accounts the Zero Trust Way Automated"},{"categories":null,"contents":" Table Of Contents Thank you for reading my blog 2023 02 20 zero trust infrastructure azure virtual desktop automated. I hope you got a bit inspired. Enjoy your day and happy automating üëã ","date":"February 20, 2023","image":"http://localhost:1313/protect-privileged-accounts-the-zero-trust-way-automated/image.jpg","permalink":"/2023-02-20-zero-trust-infrastructure-azure-virtual-desktop-automated/","title":"2023 02 20 Zero Trust Infrastructure Azure Virtual Desktop Automated"},{"categories":["Azure","Security"],"contents":"We all have been there, we created an Azure VM with RDP (port 3389) enabled on the outside. When using the Azure portal and enabling RDP all sources are allowed by default. Yes you will get a notice but often is just for test and you forgot limiting RDP connections to some IPs. This is also happend to me. I deployed an Azure VM for test, and did not add my own IP address in the firewall. After a few days I got a message from Defender for Endpoint that tells my VM had some bruteforce attacks. Luckilly, I got a strong password so they didn\u0026rsquo;t come in. But you don\u0026rsquo;t want that kind of traffic hammering to your environment.\nIn this blog I show a way how to trigger an automation sequence that blocks all RDP traffic to an Azure VM based on an Defender for Endpoint alert.\nTable Of Contents Defender for Endpoint Alert Defender for Endpoint Alert Thank you for reading my blog block all traffic to azure vm in case of bruteforce attack automated. I hope you got a bit inspired. Enjoy your day and happy automating üëã ","date":"February 14, 2023","image":"http://localhost:1313/block-all-traffic-to-azure-vm-in-case-of-bruteforce-attack-automated/image.jpg","permalink":"/block-all-traffic-to-azure-vm-in-case-of-bruteforce-attack-automated/","title":"Block all traffic to Azure VM in case of bruteforce attack automated"},{"categories":["Security","Azure AD","Zero Trust"],"contents":"Nowadays a good cloud environment has strict security policies. Well-known policies are conditional access policies. Policies that require a compliant device before login for example or forcing MFA. Configuring conditional access is good but there is always a risk of lockout yourself. To not lock yourself out, you must create an emergency account.\nThis post is the start of my Zero to Zero Trust series. Creating emergency accounts is the first step to a Zero Trust environment. The post explains how to create an emergency, aka break glass, user account including basic monitoring in an automated way.\nThis blog post is part of the Zero to Zero Trust series Table Of Contents Account creation and permissions Create a Break Glass user account Assign permanent Global Admin role with PIM Monitor Account Usage Diagnostic settings Azure AD Create Action Group Create monitor rule Edit MFA Conditional Access policies Results Script Notes from the field Account creation and permissions Emergency access accounts are highly privileged, and they are not assigned to specific individuals. Emergency access accounts are limited to emergency or \u0026ldquo;break glass\u0026rdquo;\u0026rsquo; scenarios where normal administrative accounts can\u0026rsquo;t be used. Think about a scenario where MFA is broken. It is highly recommended that you maintain a goal of restricting emergency account use to only the times when it is absolutely necessary.\nCreate a Break Glass user account The first step is creating a break glass account. With the code below I use the Graph API. I generate a strong password based on random characters. The Get-RandomPassword function can be found on my GitHub page.\n$username = Get-RandomString 8 $password = Get-RandomString 24 $userInfo = @{ \u0026#34;accountEnabled\u0026#34; = $true \u0026#34;displayName\u0026#34; = \u0026#34;Break Glass\u0026#34; \u0026#34;mailNickname\u0026#34; = $username \u0026#34;userPrincipalName\u0026#34; = \u0026#34;{0}{1}@contoso.onmicrosoft.com\u0026#34; -f \u0026#34;bkr\u0026#34;, $username \u0026#34;passwordProfile\u0026#34; = @{ \u0026#34;forceChangePasswordNextSignIn\u0026#34; = $false \u0026#34;password\u0026#34; = $password } } | ConvertTo-Json -Depth 5 $userUrl = \u0026#34;https://graph.microsoft.com/beta//users\u0026#34; $value = Invoke-RestMethod -Uri $userUrl -Method POST -Headers $authHeader -Body $userInfo -ContentType \u0026#34;application/json\u0026#34; $value Keep the account ID ready for the next steps. Because I stored the output into a variable, I can request the ID any time.\nFor more information check the emergency access documentation.\nAssign permanent Global Admin role with PIM In the next step, we make sure the break glass account has global admin permissions assigned permanently. To assign permissions I use Azure AD Privileged Identity Management (PIM). PIM is a service in Azure Active Directory (Azure AD) that enables you to manage, control, and monitor access to important resources in your organization.\nThe reason why I give permanent permissions is because of I can\u0026rsquo;t assign permissions in the case of an emergency. If I\u0026rsquo;m not able to log in anymore, I can\u0026rsquo;t elevate permissions ;).\nIn the code below, I assign the global admin role to the user I created above. The role definition ID \u0026quot;62e90394-69f5-4237-9190-012177145e10\u0026quot; is a built-in role ID that is available in every tenant and has the same ID. (See BuiltIn Roles)\nThe Get-Date makes sure the start date is NOW. The expiration date is set to noExpiration. The principalId is the user ID from above.\n$pimUrl = \u0026#34;https://graph.microsoft.com/beta/roleManagement/directory/roleAssignmentScheduleRequests\u0026#34; $pimBody = @{ action = \u0026#34;adminAssign\u0026#34; justification = \u0026#34;Assign permanent break glass global admin permissions\u0026#34; reason = \u0026#34;Permanent global admin permissions needed in case of emergency\u0026#34; roleDefinitionId = \u0026#34;62e90394-69f5-4237-9190-012177145e10\u0026#34; directoryScopeId = \u0026#34;/\u0026#34; principalId = $value.id scheduleInfo = @{ startDateTime = Get-Date expiration = @{ type = \u0026#34;noExpiration\u0026#34; } } } | ConvertTo-Json -Depth 5 $pimValue = Invoke-RestMethod -Uri $pimUrl -Method POST -Headers $authHeader -Body $pimBody -ContentType \u0026#34;application/json\u0026#34; $pimValue More info is available in the PIM overview.\nFor more information about assigning permissions in PIM using Graph API, check the Graph API documentation\nMonitor Account Usage As discussed above, the break glass account is a highly privileged user account that must be handled with care. Because of that, it is recommended to monitor account usage. This is the reason why I create a monitor rule to check for account usage.\nThe idea is to create an alert rule in Azure Monitor that checks the sign-in logs in the Log Analytics. Every log-in step is logged even when the login is not succesful. If the user account is logged, the IT admin gets an email.\nDiagnostic settings Azure AD To get information into Log Analytics, diagnostic settings must be set on the AAD IAM resource. The needed logs are the SignInLogs. In the code below, I create a testDiagSetting value with the log settings SignInLogs in it.\nAssuming there is a Log Analytics workspace already I use the code below to configure diagnostics settings for Azure AD using the Azure Management API.\n$logsUrl = \u0026#34;https://management.azure.com/providers/microsoft.aadiam/diagnosticSettings/{0}?api-version=2017-04-01\u0026#34; -f \u0026#34;testDiagSetting\u0026#34; $logsBody = @{ properties = @{ logs = @( @{ \u0026#34;category\u0026#34; = \u0026#34;SignInLogs\u0026#34; \u0026#34;categoryGroup\u0026#34; = $null \u0026#34;enabled\u0026#34; = $true \u0026#34;retentionPolicy\u0026#34; = @{ \u0026#34;days\u0026#34; = 90 \u0026#34;enabled\u0026#34;= $true } } ) workspaceID = \u0026#34;/subscriptions/xxx/resourcegroups/rg-mon/providers/microsoft.operationalinsights/workspaces/lawork\u0026#34; } } | ConvertTo-Json -Depth 5 $logsValue = Invoke-RestMethod -Uri $logsUrl -Method PUT -Headers $authHeader -Body $logsBody -ContentType \u0026#34;application/json\u0026#34; $logsValue Create Action Group The next step is creating an action group. An action group is the next step in line after an alert has occurred. An alert group contains \u0026rsquo;endpoints\u0026rsquo; where alerts are sent too. This can be a webhook, logic app or an e-mail for example. In this case, I use the e-mail and SMS endpoint. If this alert group receives an alert, the message is sent to my e-mail address and telephone number.\nThe body below is quite simple. I create an alert group in a resource group and call it SentMail.\nThe reason why I create an action group first is because I can directly provide the correct action group while creating an alert rule.\n$actionGroupUrl = \u0026#34;https://management.azure.com/subscriptions/xxx/resourceGroups/rg-roz-avd-mon/providers/Microsoft.Insights/actionGroups/{0}?api-version=2021-09-01\u0026#34; -f \u0026#34;SentMail\u0026#34; $actionGroupBody = @{ location = \u0026#34;Global\u0026#34; properties = @{ groupShortName = \u0026#34;mail\u0026#34; emailReceivers = @( @{ name = \u0026#34;Sander email\u0026#34; emailAddress = \u0026#34;email@me.com\u0026#34; useCommonAlertSchema = $true } ) smsReceivers = @( @{ name = \u0026#34;Sander SMS\u0026#34; countryCode = \u0026#34;31\u0026#34; phoneNumber = \u0026#34;0612345678\u0026#34; status = \u0026#34;Enabled\u0026#34; } ) } } | ConvertTo-Json -Depth 5 $actionGroupValue = Invoke-RestMethod -Uri $actionGroupUrl -Method PUT -Headers $authHeader -Body $actionGroupBody -ContentType \u0026#34;application/json\u0026#34; $actionGroupValue For more information about creating alert groups with Azure API, check the docs.\nCreate monitor rule The last step is creating the monitor rule itself. Several alert rule types are available. Think about metrics, activity logs and custom logs. Every type has its own API endpoint. For this type (custom logs), we need the Scheduled Query Rules endpoint. This sounds a bit weird maybe, but in fact the name fits quite well. This is because this rule type is actually a scheduled task. A task that runs every n-minutes.\nIn the body below, all above-created components are coming together.\nThe first resource is the workspaceID. This is the rule scope and the first thing you select when creating a rule manually.\nThe next resource is stored in the query object. This is the next step, when selecting the custom log type.\nThe query is in Kusto Query language (KQL) format. This query searches for logs in SignInLogs for a user principal name equal to the created break glass account above.\nKQL is case sensitive, because an UPN is lowercase, I convert the value to lower case.\nSigninLogs | where UserPrincipalName == tolower(\u0026#39;$($value.userPrincipalName)\u0026#39;) After filling in the rest of the conditions the next step is what the alert rule needs to do. This is the place where the action group comes up.\nYou can have more action groups configured. The body then look like the following.\nactions = @{ actionGroups = @( $actionGroupValue.id $actiongroup2 ) } The complete action rule body is stored below. In the end, we have a rule that searches every 5 minutes for a sign-in log where the user principal name equals the break glass account.\nI configured also a dimension based on the user\u0026rsquo;s SPN. This is to receive a split overview when more break glass accounts are monitored and used.\nIf there is a result, a severity 0 (highest) alert will be created and I will be notified at my email address.\n$queryUrl = \u0026#34;https://management.azure.com/subscriptions/xxx/resourceGroups/rg-roz-avd-mon/providers/Microsoft.Insights/scheduledQueryRules/{0}?api-version=2021-08-01\u0026#34; -f \u0026#34;BreakGlassUsed\u0026#34; $queryBody = @{ location = \u0026#34;westeurope\u0026#34; properties = @{ description = \u0026#34;Break glass account used!\u0026#34; severity = 0 evaluationFrequency = \u0026#34;PT5M\u0026#34; scopes = @( $workspaceId ) windowSize = \u0026#34;PT5M\u0026#34; criteria = @{ allOf = @( @{ query = \u0026#34;SigninLogs | where UserPrincipalName == tolower(\u0026#39;$($value.userPrincipalName)\u0026#39;)\u0026#34; timeAggregation = \u0026#34;count\u0026#34; dimensions = @( @{ name = \u0026#34;ServicePrincipalName\u0026#34; operator = \u0026#34;Include\u0026#34; values = @( \u0026#34;*\u0026#34; ) } ) operator = \u0026#34;GreaterThan\u0026#34; threshold = 0 } ) } actions = @{ actionGroups = @( $actionGroupValue.id ) } autoMitigate = $false } } | ConvertTo-Json -Depth 10 $queryValue = Invoke-RestMethod -Uri $queryUrl -Method PUT -Headers $authHeader -Body $queryBody -ContentType \u0026#34;application/json\u0026#34; $queryValue For more information about creating Custom Logs monitor rules automated, check the API Documentation\nEdit MFA Conditional Access policies The last step is to make sure the break-glass accounts are excluded from your MFA policies. This is where it all goes about :). With the code below, I search for all conditional access policies with MFA configuration. Based on the results, the break glass account will be added, to every policy, as an excluded user.\n$caUrl = \u0026#34;https://graph.microsoft.com/beta/identity/conditionalAccess/policies\u0026#34; $caPolicies = Invoke-RestMethod -Uri $caUrl -Method GET -Headers $authHeader $caBody = @{ conditions = @{ users = @{ excludeUsers = @( $value.id ) } } } | ConvertTo-Json -Depth 5 $caPolicies.value | Where-Object {$_.grantControls.builtInControls -match \u0026#34;mfa\u0026#34;} | ForEach-Object { $caUrl = \u0026#34;https://graph.microsoft.com/beta/identity/conditionalAccess/policies/{0}\u0026#34; -f $_.id $caValue = Invoke-RestMethod -Uri $caUrl -Method PATCH -Headers $authHeader -Body $caBody -ContentType \u0026#34;application/json\u0026#34; $caValue } Results I did a simple test by trying to log in with invalid credentials. That results in an alert in my mailbox.\nIn Log Analtics, the results look like below.\nScript I created a small script that is stored at my ZeroTrust Repo To use the script change the variables into your own and run the code in a PowerShell environment.\nNotes from the field Break Glass accounts are highly privileged accounts, that\u0026rsquo;s no surprise anymore (I hope). So, we need to handle these accounts with care. Microsoft\u0026rsquo;s documentation says we need to create a Global Admin. But, to be honest I (and more MVPs) do have their concerns about creating an account with the highest permission level with no MFA configured. I also spoke with some fellow MVPs (Jan Bakker and Niels Kok) and they have the same.\nBecause of that, an extra blog post comes with content on how to get more control over your break-glass accounts.\nFor now, these are the first baby steps to a Zero Trust environment. Thank you for reading my blog configure break glass accounts infrastructure automated. I hope you got a bit inspired. Enjoy your day and happy automating üëã ","date":"February 13, 2023","image":"http://localhost:1313/configure-break-glass-accounts-infrastructure-automated/image.jpg","permalink":"/configure-break-glass-accounts-infrastructure-automated/","title":"Configure Break Glass Accounts Infrastructure Automated"},{"categories":["Security","Zero Trust"],"contents":"Welcome to the Zero to Zero Trust series automated and to this automation index. In this series, I will show the process from a zero security configuration to Zero Trust in the Microsoft public cloud. The series passes all the important items and show how to configure the components automated if applicable.\nTable Of Contents What is Zero Trust? Used platforms and products Zero Trust Automation plan Zero Trust model for Microsoft 365 Zero Trust automation blog overview Zero Trust model for Azure (PAAS \u0026amp; IAAS) Preparation Authentication Client ID overview Licensing Code, scripts and templates What is Zero Trust? From Microsoft: Zero Trust is a security strategy. It is not a product or a service, but an approach to designing and implementing the following set of security core principles:\nVerify explicitly: Always authenticate and authorize based on all available data points. Use least privilege access: Limit user access with Just-In-Time and Just-Enough-Access (JIT/JEA), risk-based adaptive policies, and data protection. Assume breach: Minimize blast radius and segment access. Verify end-to-end encryption and use analytics to get visibility, drive threat detection, and improve defenses. Instead of believing everything behind the corporate firewall is safe, the Zero Trust model assumes breach and verifies each request as though it originated from an uncontrolled network. Regardless of where the request originates or what resource it accesses, the Zero Trust model teaches us to \u0026quot;never trust, always verify.\u0026quot;\nBelow, the key technology pillars are shown. Used platforms and products The following main platforms and products are used in this series:\nAzure Active Directory (Identity) Microsoft Intune Microsoft Defender Microsoft Purview Azure Every component has a set of prerequisites that need to be configured before you can start with the Zero Trust configuration.\nZero Trust Automation plan Before we start, it is good to have a plan. In this series, I will discuss the key concepts and object from there enrolling the configurations. Microsoft has published two Zero Trust models. The first one is the Zero Trust model for Microsoft 365. The second one focus on Azure (PAAS) and infrastructure (IAAS). In this series, I will focus on both, starting with the Microsoft 365 Zero Trust model.\nZero Trust model for Microsoft 365 The Zero Trust model for Microsoft 365 focuses on the following areas:\nIdentity Endpoints Data Applications The complete Zero Trust model for Microsoft 365 is shown below. The full overview is located here.\nBased on Zero Trust guidance I created a list of blogs that will be published and are listed below. The model above returns in every blog and shows where we are at that time. Every blog helps you to deploy a Zero Trust infrastructure in an automated way. Every needed component is deployed and configured.\nZero Trust automation blog overview In the following table, the blogs are listed and the corresponding area is shown. The blogs are published in the order of the Zero Trust model. Some of the blogs are not published yet and more blogs are planned.\nArea Blog \u0026mdash;\u0026mdash;\u0026mdash; \u0026mdash;\u0026mdash;\u0026mdash; Identity Configure Break Glass Accounts Infrastructure Automated Identiy Passwordless, Multi-layered Break Glass Alternative Automated Identity Protect Privileged Accounts Automated Identity Common Identity and Device Access Protection Identity Monitor privilegded account usage Identity Check Condtitional Access policy effects automated Identity Monitor conditional access changes Identity Configure Privileged Identity Management automated Application \u0026amp; Data Zero Trust Data Protection Using App Protection Policies Automated Endpoints Monitor device risk and compliance to security baselines Endpoints Configure Windows Hello for Business automated Data Implement data loss prevention (DLP) automated Data \u0026amp; Collaboration Deploy Zero Trust for Microsoft Teams automated Zero Trust model for Azure (PAAS \u0026amp; IAAS) Area Blog \u0026mdash;\u0026mdash;\u0026mdash; \u0026mdash;\u0026mdash;\u0026mdash; Azure PAAS Deploy Zero Trust for AVD automated Preparation Before starting with the Zero Trust configuration automated, some preparation is needed. In this section, I will show the needed preparation steps.\nAuthentication In this series, several configuration layers pass. Think about Graph, hidden Azure AD API, Security \u0026amp; Compliance and Azure environments. Every layer (aka scope), has its permissions and way of authentication. For deployment, I use the user impersonation device code flow. For managing the Zero Trust infrastructure I use a service principal.\nThe needed permissions are listed in every article.\nFor the device code flow authentication, I created a PowerShell function that can be used in every article. The function is located in the GitHub repository.\nIn the example below, I request an access token for the Microsoft Graph PowerShell application. The scope is DeviceManagementApps.ReadWrite.All. The function returns the access token in the variable $AuthHeader.\n$AuthHeader = .\\graph.authentication.interactive.ps1 -TenantName \u0026#34;contoso.onmicrosoft.com\u0026#34; -ClientId \u0026#34;14d82eec-204b-4c2f-b7e8-296a70dab67e\u0026#34; -Scope \u0026#34;DeviceManagementApps.ReadWrite.All\u0026#34; After running the command, you will get a device code. Open a browser and navigate to https://microsoft.com/devicelogin.\nEnter the code and authenticate. After authentication, you will get a token.\nThe clientId I used is the Microsoft Graph PowerShell application ID under Azure AD Enterprise Applications. This authentication part is used for automating the Zero Trust component deployment. The managing part is be added later.\nClient ID overview The following table shows the client IDs that are used in this series.\nScope ClientID Microsoft Graph PowerShell 14d82eec-204b-4c2f-b7e8-296a70dab67e Microsoft Intune PowerShell d1ddf0e4-d672-4dae-b554-9d5bdfd93547 Azure API Management 8602e328-9b72-4f2d-a4ae-1387d013a2b3 Licensing During the series, I use some premium features that require licenses. The licenses are listed below.\nAzure AD Premium P2 Mircosoft 365 E3 with E5 Security Add-On Mircosoft 365 E5 Code, scripts and templates All the code, scripts and templates are available on GitHub\nI would suggest cloning the repository to your local machine.\nThis blog post is growing and will be updated. For now, I keep my focus on the M365 Zero Trust model. The Azure Zero Trust model will be added later for example.\nLike to collaborate? Feel free to contact me using the social links at the top of this website.\nThank you for reading my blog zero to zero trust automation index. I hope you got a bit inspired. Enjoy your day and happy automating üëã ","date":"February 11, 2023","image":"http://localhost:1313/zero-to-zero-trust-automation-index/image.png","permalink":"/zero-to-zero-trust-automation-index/","title":"Zero to Zero Trust Automation Index"},{"categories":["Microsoft Intune","Device Management","Security"],"contents":"Device management scripts in Intune are custom scripts that can run on Windows 10 and higher clients and MacOS. The ability to run scripts on local devices gives you an alternative to configuring devices with settings that are not available under the configuration policies or in the apps part of Intune.\nUnfortunately, it is not possible to read, or even download, the file after the script is uploaded and configured in Intune. This is very inconvenient. In this blog, I show a way how to get the content from an uploaded script in Intune. Table Of Contents Scripts Get script content Keep in mind Scripts The scripts component can be found under Devices -\u0026gt; Scripts in the portal. In the screenshot below a script is deployed to set some registry keys. Keys that are not allowed to be set through configuration profiles. As you can see, there is no option available to see what is in the script or you can click it to download the file. Get script content To get the script content, we must go back to the first step, upload. During the upload, the script content is read by the upload process and converted to a BASE64 string. In the example below, I request my script that writes some registry settings to a group of devices.\nAt first, I request all the scripts to find out the script ID.\n$appUrl = \u0026#34;https://graph.microsoft.com/beta//deviceManagement/deviceManagementScripts\u0026#34; $value = Invoke-RestMethod -Uri $appUrl -Method GET -Headers $authHeader $value.value Based on the ID, I look further into the configuration itself by using the code below.\n$appUrl = \u0026#34;https://graph.microsoft.com/beta//deviceManagement/deviceManagementScripts/87c7c543-3dac-49dd-b438-1eb44ea2da0b\u0026#34; $value = Invoke-RestMethod -Uri $appUrl -Method GET -Headers $authHeader $value Now the script content is shown, which is a BASE64 string. So, we need to convert it back to a human-readable string Converting the Intune script content back to normal text is very simple using PowerShell. I paste the $value.scriptContent value into [System.Convert]::FromBase64String() object.\n[System.Text.Encoding]::ASCII.GetString([System.Convert]::FromBase64String($($value.scriptContent))) If you want to write it down into a file, pipeline the content to the Out-File PowerShell command. $value.fileName represents the original filename in Intune.\n[System.Text.Encoding]::ASCII.GetString([System.Convert]::FromBase64String($($value.scriptContent))) | out-file $value.fileName Keep in mind Using scripts can be a lifesaver but I prefer using configuration profiles. This is because of the following reasons:\nSECURITY is the biggest thing in my opinion. You don\u0026rsquo;t have any clue about what is going on in the script after uploading. There is no sight in the script content. Anything can be written in the script without any notice. Hard to manage: Besides you being blind to the content, it is also not possible to simply modify some settings without walking through the whole process. The script timeouts after 30 minutes, hopefully, when using scripts, you only use them to tweak some small settings. Error handling, not that I\u0026rsquo;m very experienced using scripts in Intune, but error handling must be done at the client level in the Intune logs. Not very convenient. The script has a max of 200 KB. That\u0026rsquo;s a very big script. As said before, hopefully, you just use a script to set small configurations For more information about scripts check the PowerShell or MacOS shell documentation.\nThank you for reading my blog get intune device management scripts content using graph. I hope you got a bit inspired. Enjoy your day and happy automating üëã ","date":"February 9, 2023","image":"http://localhost:1313/get-intune-device-management-scripts-content-using-graph/image.jpeg","permalink":"/get-intune-device-management-scripts-content-using-graph/","title":"Get Intune Device Management Scripts Content using Graph"},{"categories":["Azure AD","Identity"],"contents":"Recently Microsoft redesigned the company branding functionality. That means you have more flexibility in the main layout, full-screen background image for example. But also the ability to change the header, footer and even link relating to self-service password reset, privacy \u0026amp; cookies or Terms of Use. Also, there is an option to upload a CSS style sheet to change colors, buttons, and more.\nIn this blog, I show how to configure company branding in an automated way. When automated, you can do some cool things like changing style to light or dark on specific hours.\nYou can read the official article here: https://techcommunity.microsoft.com/t5/microsoft-entra-azure-ad-blog/introducing-enhanced-company-branding-for-sign-in-experiences-in/bc-p/3701590/highlight/true\nTable Of Contents Company Branding To CSS or not to CSS Not to CSS Configure Company Branding automated Authentication Configure predefined settings Upload images Upload CSS Add languages Benefits Keep in mind Summary Company Branding When users sign in to https://login.microsoft.com, https://portal.azure.com, or own build applications with an Azure AD identity provider, the default Microsoft sign-in screen shows up like below.\nThe company branding feature allows you to give users a nice sign-in experience that matches the corporate identity of your company. I created a simple light theme and did a small modification on the button which is shown in the screenshot below.\nIn the upcoming chapters, I show how to modify the company branding configuration in an automated way.\nTo CSS or not to CSS The company branding feature consists of two main parts, in my opinion, CSS and not CSS. At least, that\u0026rsquo;s what I thought in the beginning. During my tests, I stored all configurable items in a CSS style sheet. Items like colors, font sizes, shadows, link colors, the background image, etc in the CSS file.\nThe main idea was to create two style sheets, a light and a dark mode style. I stored the backgrounds on a public location (GitHub) and the background is loaded with the CSS code below in the .ext-background-image object.\nbackground: url(\u0026#39;https://github.com/urltotheimage.jpg\u0026#39;); After some tries, I noticed the color settings, font sizes etc are working as expected. But setting a background also from the CSS file isn\u0026rsquo;t working quite well. The background image isn\u0026rsquo;t responsive for example.\nIn the end, I noticed I need both components to get the full benefit.\nAs described above I stored items like link colors, font sizes and button preferences in the CSS file.\na:link { /* Styles for links */ color: grey; } .ext-sign-in-box { /* Styles for the sign in box container */ opacity: 90%; } .ext-button.ext-primary { /* Styles for primary buttons */ display: flex; flex-direction: column; align-items: center; padding: 6px 14px; border-radius: 0px; border: none; background: #ccc; box-shadow: 0px 0.5px 1px rgba(0, 0, 0, 0.1), inset 0px 0.5px 0.5px rgba(255, 255, 255, 0.5), 0px 0px 0px 0.5px rgba(0, 0, 0, 0.12); color: #222; user-select: none; -webkit-user-select: none; touch-action: manipulation; } Mircosoft provides a predefined CSS stylesheet with all configurable items. This file can be downloaded from this page: https://learn.microsoft.com/en-us/azure/active-directory/fundamentals/how-to-customize-branding#layout (See custom CSS item).\nNot to CSS So, I removed the background image from the CSS file and used the background image placeholder under the basis tab.\nCurrently, only the predefined items in the CSS file are configurable with CSS. All other needs to be configured using the placeholders. (Layout for example)\nConfigure Company Branding automated To configure company branding in an automated way, I split the process into three parts.\nConfigure settings like layout type or link configuration; These are predefined items on Microsoft\u0026rsquo;s site Image upload; CSS upload; The reason why I separated the upload process has to do with the content and the request header. (Will explain below).\nAuthentication Several options are available to authenticate. You can use the client-secret method with Organization.ReadWrite.All permission. I use the Connect-AzAccount command with a user account that has Contributor permissions. Because Azure AD also works with Azure Roles I don\u0026rsquo;t need a client/secret with Graph API permissions.\n$context = [Microsoft.Azure.Commands.Common.Authentication.Abstractions.AzureRmProfileProvider]::Instance.Profile.DefaultContext $token = [Microsoft.Azure.Commands.Common.Authentication.AzureSession]::Instance.AuthenticationFactory.Authenticate($context.Account, $context.Environment, $context.Tenant.Id.ToString(), $null, [Microsoft.Azure.Commands.Common.Authentication.ShowDialog]::Never, $null, $resource).AccessToken $authHeader = @{ Authorization = \u0026#39;Bearer \u0026#39; + $token } $tenantId = $context.tenant.id Configure predefined settings These predefined settings have to do with all that is shown (hide/unhide) or specific texts. These items are all behind the /organization/{tenantId}/branding Graph API endpoint. I sent the body below to this endpoint.\n$brandingBody = @{ \u0026#34;backgroundColor\u0026#34; = $null \u0026#34;customAccountResetCredentialsUrl\u0026#34; = $null \u0026#34;customCannotAccessYourAccountText\u0026#34; = $null \u0026#34;customCannotAccessYourAccountUrl\u0026#34; = $null \u0026#34;customForgotMyPasswordText\u0026#34; = $null \u0026#34;customPrivacyAndCookiesText\u0026#34; = $null \u0026#34;customPrivacyAndCookiesUrl\u0026#34; = $null \u0026#34;customResetItNowText\u0026#34; = $null \u0026#34;customTermsOfUseText\u0026#34; = $null \u0026#34;customTermsOfUseUrl\u0026#34; = \u0026#34;https://rozemuller.com\u0026#34; \u0026#34;headerBackgroundColor\u0026#34; = $null \u0026#34;signInPageText\u0026#34; = \u0026#34;Hello!\u0026#34; \u0026#34;usernameHintText\u0026#34; = $null \u0026#34;loginPageTextVisibilitySettings\u0026#34; = @{ \u0026#34;hideCannotAccessYourAccount\u0026#34; = $null \u0026#34;hideAccountResetCredentials\u0026#34; = $false \u0026#34;hideTermsOfUse\u0026#34; = $false \u0026#34;hidePrivacyAndCookies\u0026#34; = $false \u0026#34;hideForgotMyPassword\u0026#34; = $null \u0026#34;hideResetItNow\u0026#34; = $null } \u0026#34;contentCustomization\u0026#34; = @{ \u0026#34;adminConsentRelativeUrl\u0026#34; = $null \u0026#34;attributeCollectionRelativeUrl\u0026#34; = $null \u0026#34;registrationCampaignRelativeUrl\u0026#34; = $null \u0026#34;conditionalAccessRelativeUrl\u0026#34; = $null \u0026#34;adminConsent\u0026#34; = @() \u0026#34;attributeCollection\u0026#34; = @() \u0026#34;registrationCampaign\u0026#34; = @() \u0026#34;conditionalAccess\u0026#34; = @() } \u0026#34;loginPageLayoutConfiguration\u0026#34; = @{ \u0026#34;layoutTemplateType\u0026#34; = \u0026#34;default\u0026#34; \u0026#34;isHeaderShown\u0026#34; = $false \u0026#34;isFooterShown\u0026#34; = $true } } | ConvertTo-Json $brandingUrl = \u0026#34;https://graph.microsoft.com/beta/organization/{0}/branding/\u0026#34; -f $tenantId Invoke-RestMethod -uri $brandingUrl -Headers $authHeader -Method PATCH -body $brandingBody -ContentType \u0026#39;application/json\u0026#39; Make a note about the Content-Type application/json. The Content-Type will change in later steps.\nThe layout template types have two options: default and verticalSplit. The basics are set.\nUpload images For the image part, we need several Graph API endpoints. Every image has a specific endpoint but works all the same in the end. In an earlier post, I explained how the intunewinfile upload process works in automation.\nIn this post, the file upload process works almost the same.\nI read the file and file size. Then I read the file and encode all bytes. The encoded bytes are sent to the Graph API endpoint. Encoding image files with PowerShell to send to the Graph API can be done with the code below.\n$imagePath = Get-Item \u0026#39;pathToImageFile.jpeg/png\u0026#39; $fileSize = (Get-Item -Path $imagePath).Length $BinaryReader = New-Object -TypeName System.IO.BinaryReader([System.IO.File]::Open($imagePath, [System.IO.FileMode]::Open, [System.IO.FileAccess]::Read, [System.IO.FileShare]::ReadWrite)) $bytes = $BinaryReader.ReadBytes($fileSize) $isoEncoding = [System.Text.Encoding]::GetEncoding(\u0026#34;iso-8859-1\u0026#34;) $encodedBytes = $isoEncoding.GetString($bytes) I repeat this process for every image.\nIn the next step, I sent the encoded bytes to the Graph API.\nThis time I use the image/* content type. This tells the request I\u0026rsquo;m sending an image. (In the example below I sent the background image)\n$imageUrl = \u0026#34;https://graph.microsoft.com/beta/organization/{0}/branding/localizations/0/backgroundImage\u0026#34; -f $tenantId Invoke-RestMethod -uri $imageUrl -Headers $authHeader -Method PUT -body $encodedBytes -ContentType \u0026#39;image/*\u0026#39; The following Graph API company branding image endpoints are available:\nPurpose Graph API Endpoint Favicon https://graph.microsoft.com/beta/organization/{0}/branding/localizations/0/favicon Banner Logo https://graph.microsoft.com/beta/organization/{0}/branding/localizations/0/bannerLogo Header Logo https://graph.microsoft.com/beta/organization/{0}/branding/localizations/0/headerLogo Square Logo https://graph.microsoft.com/beta/organization/{0}/branding/localizations/0/squareLogo Square Logo (dark) https://graph.microsoft.com/beta/organization/{0}/branding/localizations/0/squareLogoDark Background Image https://graph.microsoft.com/beta/organization/{0}/branding/localizations/0/backgroundImage Upload CSS The last step is uploading the CSS file. This step is quite easy. I read the CSS file content and send the content to the Graph API CustomCss endpoint. To send CSS files, I use the text/css content type.\n$cssBody = Get-Content ./style.css $cssUrl = \u0026#34;https://graph.microsoft.com/beta/organization/{0}/branding/localizations/0/customCSS\u0026#34; -f $tenantID Invoke-RestMethod -uri $cssUrl -Headers $authHeader -Method PUT -body $cssBody -ContentType \u0026#39;text/css\u0026#39; Good to know is that all these endpoints do not return content. When using Invoke-WebRequest a status code 204 NO CONTENT is returned.\nAdd languages The whole above process is the default login. It is also possible to configure different languages with their own branding. To create languages automated, use the code below.\nFirst, create the language object itself. I changed the header a bit where I tell also to accept the Dutch language with the nl-NL code.\n$language = \u0026#34;nl-NL\u0026#34; $authHeader = @{ \u0026#39;Accept-Language\u0026#39; = $language Authorization = \u0026#39;Bearer \u0026#39; + $Token } $locBody = @{ id = $language backgroundColor = \u0026#39;#0072c6\u0026#39; } | ConvertTo-Json $locUrl = \u0026#34;https://graph.microsoft.com/beta/organization/{0}/branding/localizations\u0026#34; -f $tenantId Invoke-RestMethod -uri $locUrl -Headers $authHeader -Method POST -body $locBody -ContentType \u0026#39;application/json\u0026#39; I create an object with the language code ID and provide just an object because it is mandatory.\nSecondly, I configure the rest of the branding with the code (CSS example) below. Only this time I also provide the language ID in the URL.\n$cssBody = Get-Content ./style.css $cssUrl = \u0026#34;https://graph.microsoft.com/beta/organization/{0}/branding/localizations/{1}/customCSS\u0026#34; -f $tenantId, $language Invoke-RestMethod -uri $cssUrl -Headers $authHeader -Method PUT -body $cssBody The correct language is shown based on the browser\u0026rsquo;s default language.\nBenefits Besides automation saving time, there is more. Let\u0026rsquo;s take a look at the background image upload process. In the portal, we are restricted by sending a file with a max image size of 1920x1080px and a max file size of 300KB.\nHowever, it looks like the image is there you are not able to save the configuration\nWhen using the automated way, I was able to send an image over the 2MB. It is recommended to store images as small as possible of course. But I can be a lifesaver.\nAnother thing is you can play with multiple themes. It is not possible to configure more themes and schedule themes at a specific time. A light theme during the day and a dark theme during the night for example.\nIf you like to use multiple themes, automation is key. Create different configurations and run an automation task to configure the specific config.\nKeep in mind To use the company branding feature you must have one of the following licenses\nAzure AD Premium 1 Azure AD Premium 2 Office 365 (for Office apps) Summary In this blog post, I showed how to configure company branding in an automated way. Automating the process gives you the possibility to configure specific themes at specific times. I also showed how to send different content to the Graph API by providing the correct content type.\nFor more information, I would suggest reading the articles below.\nConfigure company branding: https://learn.microsoft.com/en-us/azure/active-directory/fundamentals/customize-branding Company branding preview: https://learn.microsoft.com/en-us/azure/active-directory/fundamentals/how-to-customize-branding Thank you for reading my blog automate user sign-in experience with company branding. I hope you got a bit inspired. Enjoy your day and happy automating üëã ","date":"January 10, 2023","image":"http://localhost:1313/automated-user-sign-in-experience-with-company-branding/light-dark.jpg","permalink":"/automated-user-sign-in-experience-with-company-branding/","title":"Automate user sign-in experience with company branding"},{"categories":["Azure Virtual Desktop","Az.Avd"],"contents":"Azure Virtual Desktop (AVD) Insights is a way to get sight into your AVD environment. In short (from Microsoft), Azure Virtual Desktop Insights is a dashboard built on Azure Monitor Workbooks that helps IT professionals understand their Azure Virtual Desktop environments.\nIn this blog post, I show how to configure AVD Insights in an automated way using the Az.Avd PowerShell module.\nThe commands used in the blog post are available in the v2.5.0-beta.32 and above.\nTable Of Contents AVD and Insights Hostpool Application Group Workspace Session hosts Performance Counters and Events Verify Data captured AVD and Insights The Insights part is based on a Log Analytics workspace. To make sure all AVD parts store logs into Log Analytics, we need to store all needed information in a workspace. It is not necessary to store all logs in the same workspace. But it is recommended to store session host information in a separate workspace. But there is more, we also need insight information from the AVD session hosts. Insight information like Windows Events and Performance Counters.\nIn the end, AVD Insights shows log information from different AVD components combined in a workbook.\nTo make AVD Insights work, diagnostics settings must be set with mandatory logs. The Az.Avd module provides commands that help configure AVD Insights with all mandatory settings.\nThe commands have an insights part in them. To check all the commands. use the command below.\nGet-Command -module Az.Avd *insights* In the commands below, I refer to @avdParams parameter.\n$avdParams = @{ hostpoolName = \u0026#34;Rozemuller-AAD\u0026#34; resourceGroupName = \u0026#39;rg-roz-avd-01\u0026#39; } Hostpool The AVD toplevel component is a hostpool. This is the first component where to configure diagnostics settings in. AVD Insights at host pool level.\nAssuming there is a Log Analytics Workspace already, I use the command below.\nEnable-AvdInsightsHostpool @avdParams -LAWorkspace \u0026#39;log-analytics-avd-89608\u0026#39; -LaResourceGroupName \u0026#39;rg-roz-avd-mon\u0026#39; -DiagnosticsName \u0026#39;avdInsights\u0026#39; -Verbose If you don\u0026rsquo;t have a Log Analytics workspace yet, use the -autocreate flag and provide also the LaLocation and RetentionInDays parameters. The default SKU is standard. If you like to change it, then also provide the LaSku parameter.\nApplication Group The next level is the application group. Other categories apply to this setting. The command configures every application group in a host pool. Just provide the host pool details.\nEnable-AvdInsightsApplicationGroup @avdParams -LAWorkspace \u0026#39;log-analytics-avd-89608\u0026#39; -LaResourceGroupName \u0026#39;rg-roz-avd-mon\u0026#39; -DiagnosticsName \u0026#39;avdInsights\u0026#39; -Verbose If you just want a specific application group enabled, then provide the ApplicationGroupName and ApplicationResourceGroup instead of the host pool details.\nWorkspace The last main AVD component is the workspace. To configure workspace insights use the code below. Also, this command is based on a host pool.\nEnable-AvdInsightsWorkspace @avdParams -LAWorkspace \u0026#39;log-analytics-avd-89608\u0026#39; -LaResourceGroupName \u0026#39;rg-roz-avd-mon\u0026#39; -DiagnosticsName \u0026#39;AVDInsights\u0026#39; -Verbose If you want to enable Insights for a specific workspace, replace the host pool values with WorkspaceName and WorkspaceResourceGroup.\nSession hosts AVD session hosts are Windows Clients. To get all the needed information from these clients we need to configure the session hosts to send logs to a Log Analytics workspace.\nTo send logs, the OMSExtenstion extension is needed. The command below installs the extension.\nAdd-AvdInsightsSessionHost -id $id -WorkSpaceId $workspaceResourceId The $id is the session host\u0026rsquo;s resource ID. You can get the ID using the Get-AvdSessionHost or Get-AvdSessionHostResources.\nIf hosts are missing you will see a screen like below.\nPerformance Counters and Events In the last step, we configure the needed Windows Performance Counters and Events as sources in the Log Analytics workspace to capture.\nEnable-AvdInsightsCounters -Id $resourceId -Verbose Verify When all is configured correctly, you don\u0026rsquo;t see errors like the error below in the main Insights blade. When looking into Insights (under Configuration Workbook) you will notice all checkmarks are green.\nUnder the session host tab, no session hosts are listed. In the screen below, I \u0026lsquo;forgot\u0026rsquo; one.\nAlso, no counters and events are missing.\nData captured At last, after some time, also there is data captured. I shot some of the graphs to show.\nFor more information about AVD Insights, check the documentation at Microsoft Docs: https://learn.microsoft.com/en-us/azure/virtual-desktop/insights\nThank you for reading my blog enable azure virtual desktop insights automated. I hope you got a bit inspired. Enjoy your day and happy automating üëã ","date":"December 23, 2022","image":"http://localhost:1313/enable-avd-insights-automated/insights.jpeg","permalink":"/enable-avd-insights-automated/","title":"Enable Azure Virtual Desktop Insights automated"},{"categories":["Microsoft Intune","WinGet"],"contents":" Table Of Contents Create Win32LOB App portal process Create Win32LOB App in Intune Request intunewin file version Upload and commit intunewin file to Azure Storage Blob Create Win32LOB App in automation Create Win32LOB App in Intune automated Request intunewin file version Create place holder for intunewin file version automated Upload file to Win32 LOB App automated Extract Intunewin file Calculate and create chunks Upload chunks Commit upload Update file version Summary A few days ago a question reached me about creating Win32Lob applications including the intunewin file upload using the REST API. The create Win32Lob Graph API has no object available to store a file in, and the documentation doesn\u0026rsquo;t explain how to upload files while creating Win32Lob applications automated.\nThat decided me to look into the file upload process and write this blog.\nCheck the question and responses below \u0026#x1f447;\nHow does someone create a #Win32 app through the #msgraph API? I found https://t.co/KLhk6lWaBb bt it doesn\u0026#39;t show a way to attach the intunewin file, like you do in the endpoint manager portal. #developer #MVPBuzz got a cool project idea.\n\u0026mdash; Stephan van Rooij üïô (@svrooij) December 8, 2022 Create Win32LOB App portal process The reason why there is no file placeholder available in the Graph API is because of the application creation process has multiple phases.\nThe file upload process is a separate process for example. There is no direct connection to the application creation process. During the process, there is a breakout to an Azure Storage blob container.\nCreate Win32LOB App in Intune In the screenshot below, I created a test Win32 app in the portal. I filled in the required fields where the screenshot shows the last page just before deployment.\nAfter hitting the create button the process starts. At the backend, when looking into the browser\u0026rsquo;s development, you will see the Win32 app is created without a file upload.\nRequest intunewin file version When the application deployment is finished, there is a response with the application ID. Based on the ID, the backend searches for the application you have just created.\nFrom that point, The first step is requesting a version number. The response is a version ID.\nIn the case of a new application, the version ID is 1.\nWith the version, a file placeholder is created with file metadata.\nMeta data like the file\u0026rsquo;s name and size. In the placeholder creation background, a place is created on Azure storage.\nWhen the placeholder is created, a response is returned with an ID that represents the placeholder on the Azure storage location. Creating the Azure storage location can take a while. While waiting the browser requests the ID till the moment the storage location URL is returned.\nAfter a small minute, the storage location is present.\nUpload and commit intunewin file to Azure Storage Blob In the next step, the process takes the file and reads all file bytes in memory, and uses the blob placeholder information to stage the data. This is where the actual upload process starts.\nWhen the file is read, the last step is to commit the staged file to the blob storage. In the commit phase, you see the request URL has the application ID (ID in Intune), the file version (which is 1 at the initial application creation), and the response ID (ID on the Azure Blob Storage).\nIn the next few requests, the browser waits till the commit status has been completed successfully by sending a GET request. Create Win32LOB App in automation Ok, now we know what phases we have, and multiple tasks are executed to create Win32LOB applications in Intune. In the next part of this blog, let\u0026rsquo;s take a look at how these steps look in the automation world.\nTo keep the post readable and as short as possible, I only show the happy flow\nCreate Win32LOB App in Intune automated The first step is creating the Win32LOB App in Intune. In this step, only the application is created with no file attached. To create a Win32LOB file in Intune automated with the REST API, I used the code below. The body is a JSON-formatted body with the application details.\n$win32LobBody = @\u0026#34; { \u0026#34;@odata.type\u0026#34;: \u0026#34;#microsoft.graph.win32LobApp\u0026#34;, \u0026#34;applicableArchitectures\u0026#34;: \u0026#34;x64\u0026#34;, \u0026#34;allowAvailableUninstall\u0026#34;: false, \u0026#34;categories\u0026#34;: [], \u0026#34;description\u0026#34;: \u0026#34;Install-WinGetApplication.exe\u0026#34;, \u0026#34;developer\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;displayName\u0026#34;: \u0026#34;Install-WinGetApplication.exe\u0026#34;, \u0026#34;displayVersion\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;fileName\u0026#34;: \u0026#34;Install-WinGetApplication.intunewin\u0026#34;, \u0026#34;installCommandLine\u0026#34;: \u0026#34;test.exe\u0026#34;, \u0026#34;installExperience\u0026#34;: { \u0026#34;deviceRestartBehavior\u0026#34;: \u0026#34;allow\u0026#34;, \u0026#34;runAsAccount\u0026#34;: \u0026#34;system\u0026#34; }, \u0026#34;informationUrl\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;isFeatured\u0026#34;: false, \u0026#34;roleScopeTagIds\u0026#34;: [], \u0026#34;notes\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;minimumSupportedWindowsRelease\u0026#34;: \u0026#34;21H1\u0026#34;, \u0026#34;msiInformation\u0026#34;: null, \u0026#34;owner\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;privacyInformationUrl\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;publisher\u0026#34;: \u0026#34;test\u0026#34;, \u0026#34;returnCodes\u0026#34;: [ { \u0026#34;returnCode\u0026#34;: 0, \u0026#34;type\u0026#34;: \u0026#34;success\u0026#34; }, { \u0026#34;returnCode\u0026#34;: 1707, \u0026#34;type\u0026#34;: \u0026#34;success\u0026#34; }, { \u0026#34;returnCode\u0026#34;: 3010, \u0026#34;type\u0026#34;: \u0026#34;softReboot\u0026#34; }, { \u0026#34;returnCode\u0026#34;: 1641, \u0026#34;type\u0026#34;: \u0026#34;hardReboot\u0026#34; }, { \u0026#34;returnCode\u0026#34;: 1618, \u0026#34;type\u0026#34;: \u0026#34;retry\u0026#34; } ], \u0026#34;rules\u0026#34;: [ { \u0026#34;@odata.type\u0026#34;: \u0026#34;#microsoft.graph.win32LobAppFileSystemRule\u0026#34;, \u0026#34;ruleType\u0026#34;: \u0026#34;detection\u0026#34;, \u0026#34;operator\u0026#34;: \u0026#34;notConfigured\u0026#34;, \u0026#34;check32BitOn64System\u0026#34;: false, \u0026#34;operationType\u0026#34;: \u0026#34;exists\u0026#34;, \u0026#34;comparisonValue\u0026#34;: null, \u0026#34;fileOrFolderName\u0026#34;: \u0026#34;test.exe\u0026#34;, \u0026#34;path\u0026#34;: \u0026#34;c:\\\\\u0026#34; } ], \u0026#34;runAs32Bit\u0026#34;: false, \u0026#34;setupFilePath\u0026#34;: \u0026#34;Install-WinGetApplication.exe\u0026#34;, \u0026#34;uninstallCommandLine\u0026#34;: \u0026#34;test.exe\u0026#34; } \u0026#34;@ $win32LobUrl = \u0026#34;https://graph.microsoft.com/beta/deviceAppManagement/mobileApps\u0026#34; $win32LobApp = Invoke-RestMethod -Uri $win32LobUrl -Body $win32LobBody -Headers $authHeader -Method Post -ContentType \u0026#39;application/json\u0026#39; I stored the deployment output in the $win32LobApp variable. In the variable the Intune application ID is stored which is needed in the next phase.\nRequest intunewin file version The application is created and we have an Intune application ID. Now it is time to create a file placeholder on the Azure Storage.\n$Win32LobVersionUrl = \u0026#34;https://graph.microsoft.com/beta/deviceAppManagement/mobileApps/{0}/microsoft.graph.win32LobApp/contentVersions\u0026#34; -f $win32LobApp.id $win32LobAppVersionRequest = Invoke-RestMethod -Uri $Win32LobVersionUrl -Method \u0026#34;POST\u0026#34; -Body \u0026#34;{}\u0026#34; -Headers $authHeader $win32LobAppVersionRequest It doesn\u0026rsquo;t matter how often you request a version, the version ID remains the same till the commit phase was successful.\nCreate place holder for intunewin file version automated This is the part where it becomes more complex and the first time we pick up the intunewin file. To create a placeholder, we need file information like the name and the size. This is called the metadata. To get the file information I used the code below.\nI search the intunewin file and open it. An intunewin file has a detection.xml inside that holds the needed information. After reading the application information, we close the file to save memory.\n$filePath = (Get-childItem \u0026#39;./Install-WinGetApplication.intunewin\u0026#39;).FullName $IntuneWin32AppFile = [System.IO.Compression.ZipFile]::OpenRead($filePath) $DetectionXMLFile = $IntuneWin32AppFile.Entries | Where-Object { $_.Name -like \u0026#34;detection.xml\u0026#34; } $FileStream = $DetectionXMLFile.Open() $StreamReader = New-Object -TypeName \u0026#34;System.IO.StreamReader\u0026#34; -ArgumentList $FileStream -ErrorAction Stop $DetectionXMLContent = [xml]($StreamReader.ReadToEnd()) $FileStream.Close() $StreamReader.Close() $IntuneWin32AppFile.Dispose() With the above-gathered information, I create a body with the needed information and send the request.\n$Win32LobFileBody = [ordered]@{ \u0026#34;@odata.type\u0026#34; = \u0026#34;#microsoft.graph.mobileAppContentFile\u0026#34; \u0026#34;name\u0026#34; = $DetectionXMLContent.ApplicationInfo.FileName \u0026#34;size\u0026#34; = [int64]$DetectionXMLContent.ApplicationInfo.UnencryptedContentSize \u0026#34;sizeEncrypted\u0026#34; = (Get-Item -Path $filePath).Length \u0026#34;manifest\u0026#34; = $null \u0026#34;isDependency\u0026#34; = $false } | ConvertTo-Json $Win32LobFileUrl = \u0026#34;https://graph.microsoft.com/beta/deviceAppManagement/mobileApps/{0}/microsoft.graph.win32LobApp/contentVersions/{1}/files\u0026#34; -f $win32LobApp.id, $win32LobAppVersionRequest.id $Win32LobPlaceHolder = Invoke-RestMethod -Uri $Win32LobFileUrl -Method \u0026#34;POST\u0026#34; -Body $Win32LobFileBody -Headers $authHeader The screenshot above shows the first response. As you can see the request is pending under the uploadState. Also, the isCommited object is False which means there is no file uploaded under the id yet. The last thing is the azureStorageUri which is still empty.\nThe code below checks, based on the placeholder ID, if the request is handled.\n$storageCheckUrl = \u0026#34;https://graph.microsoft.com/beta/deviceAppManagement/mobileApps/{0}/microsoft.graph.win32LobApp/contentVersions/{1}/files/{2}\u0026#34; -f $win32LobApp.id, $win32LobAppVersionRequest.id, $Win32LobPlaceHolder.id $storageCheck = Invoke-RestMethod -Uri $storageCheckUrl -Method \u0026#34;GET\u0026#34; -Headers $authHeader $storageCheck Now we have a placeholder to store the intunewin file at and start the real upload.\nAttention: The storage URL location is temporary and has an expiration time, make sure the upload is done within the expiration time\nUpload file to Win32 LOB App automated Uploading files through the HTTP protocol needs some attention. I\u0026rsquo;m not going into deep but explaining the basics. To upload files through HTTP we need to slice the packages in pieces, this is called chunking.\nChunked encoding is cutting data into smaller \u0026ldquo;blocks.\u0026rdquo; Chunks are sent independently of one another, usually through a single persistent connection. The type is specified in the Transfer-Encoding header (in the first block).\nThe receiver never sees the entire file (as it might not have been completely available in the first place ‚Äî some examples being a server reading and sending a large file to a client, or generating a table of results from a database).\nFor more information about chunking check: https://bunny.net/academy/http/what-is-chunked-encoding/\nThe whole upload process has the following steps in basics:\nExtract intunewin file to an unencrypted file; Chunk extracted file; Upload the chunks; Commit the upload; Update the file version in the Intune application; Extract Intunewin file Before we can create chunks the file needs to be extracted in an unencrypted file. To extract the file the code below is needed.\n$Base64Key = $DetectionXMLContent.ApplicationInfo.EncryptionInfo.EncryptionKey $Base64IV = $DetectionXMLContent.ApplicationInfo.EncryptionInfo.InitializationVector $ExtractedIntuneWinFile = $FilePath + \u0026#34;.extracted\u0026#34; $ZipFile = [System.IO.Compression.ZipFile]::OpenRead($FilePath) #$IntuneWinFileName = Split-Path -Path $FilePath -Leaf $IntuneWinFileName = $DetectionXMLContent.ApplicationInfo.FileName $ZipFile.Entries | Where-Object { $_.Name -like $IntuneWinFileName } | ForEach-Object { [System.IO.Compression.ZipFileExtensions]::ExtractToFile($_, $ExtractedIntuneWinFile, $true) } $ZipFile.Dispose() $Key = [System.Convert]::FromBase64String($Base64Key) $IV = [System.Convert]::FromBase64String($Base64IV) $TargetFilePath = $FilePath + \u0026#34;.decoded\u0026#34; $TargetFilePathName = Split-Path -Path $TargetFilePath -Leaf [System.IO.FileStream]$FileStreamTarget = [System.IO.File]::Open($TargetFilePath, [System.IO.FileMode]::Create, [System.IO.FileAccess]::ReadWrite, [System.IO.FileShare]::None) $AES = [System.Security.Cryptography.Aes]::Create() [System.Security.Cryptography.ICryptoTransform]$Decryptor = $AES.CreateDecryptor($Key, $IV) [System.IO.FileStream]$FileStreamSource = [System.IO.File]::Open($ExtractedIntuneWinFile, [System.IO.FileMode]::Open, [System.IO.FileAccess]::Read, [System.IO.FileShare]::None) $FileStreamSourceSeek = $FileStreamSource.Seek(48l, [System.IO.SeekOrigin]::Begin) [System.Security.Cryptography.CryptoStream]$CryptoStream = New-Object -TypeName System.Security.Cryptography.CryptoStream -ArgumentList @($FileStreamTarget, $Decryptor, [System.Security.Cryptography.CryptoStreamMode]::Write) -ErrorAction Stop $buffer = New-Object byte[](2097152) while ($BytesRead = $FileStreamSource.Read($buffer, 0, 2097152)) { $CryptoStream.Write($buffer, 0, $BytesRead) $CryptoStream.Flush() } Calculate and create chunks With the basics in mind, I use the code block below to create chunks and send every chunk to the Azure Storage Blob. In the first part, I create 6MB chunks and calculate, based on the expanded file size, how many chunks there are.\n$ChunkSizeInBytes = 1024l * 1024l * 6l; $SASRenewalTimer = [System.Diagnostics.Stopwatch]::StartNew() $FileSize = (Get-Item -Path $ExtractedIntuneWinFile).Length $ChunkCount = [System.Math]::Ceiling($FileSize / $ChunkSizeInBytes) $BinaryReader = New-Object -TypeName System.IO.BinaryReader([System.IO.File]::Open($FilePath, [System.IO.FileMode]::Open, [System.IO.FileAccess]::Read, [System.IO.FileShare]::ReadWrite)) In my case, the intunewin file is that small I only have one chunk.\nUpload chunks In the loop below, I create chunks and load the bytes into it. Thereafter, I send every chunk and corresponding bytes to the storage location.\n$ChunkIDs = @() for ($Chunk = 0; $Chunk -lt $ChunkCount; $Chunk++) { $ChunkID = [System.Convert]::ToBase64String([System.Text.Encoding]::ASCII.GetBytes($Chunk.ToString(\u0026#34;0000\u0026#34;))) $ChunkIDs += $ChunkID $Start = $Chunk * $ChunkSizeInBytes $Length = [System.Math]::Min($ChunkSizeInBytes, $FileSize - $Start) $Bytes = $BinaryReader.ReadBytes($Length) $CurrentChunk = $Chunk + 1 $Uri = \u0026#34;{0}\u0026amp;comp=block\u0026amp;blockid={1}\u0026#34; -f $storageCheck.azureStorageUri, $ChunkID $ISOEncoding = [System.Text.Encoding]::GetEncoding(\u0026#34;iso-8859-1\u0026#34;) $EncodedBytes = $ISOEncoding.GetString($Bytes) $Headers = @{ \u0026#34;x-ms-blob-type\u0026#34; = \u0026#34;BlockBlob\u0026#34; } $UploadResponse = Invoke-WebRequest $Uri -Method \u0026#34;Put\u0026#34; -Headers $Headers -Body $EncodedBytes -UseBasicParsing -ErrorAction Stop } In the last step, I finalize the chunk list and send an XML list to the storage location.\n$finalChunkUri = \u0026#34;{0}\u0026amp;comp=blocklist\u0026#34; -f $storageCheck.azureStorageUri $XML = \u0026#39;\u0026lt;?xml version=\u0026#34;1.0\u0026#34; encoding=\u0026#34;utf-8\u0026#34;?\u0026gt;\u0026lt;BlockList\u0026gt;\u0026#39; foreach ($Chunk in $ChunkID) { $XML += \u0026#34;\u0026lt;Latest\u0026gt;$($Chunk)\u0026lt;/Latest\u0026gt;\u0026#34; } $XML += \u0026#39;\u0026lt;/BlockList\u0026gt;\u0026#39; Invoke-RestMethod -Uri $finalChunkUri -Method \u0026#34;Put\u0026#34; -Body $XML -ErrorAction Stop $BinaryReader.Close() $BinaryReader.Dispose() Commit upload The last step is to commit the chunks into the file at the storage location. To commit we head back to the XML content. In the XML content, we grab the encryption info. The encryption info is used to decrypt the file in the Intune environment.\nAfter creating, the encryption body is sent to the commit URL. This is the Azure Storage URL including the placeholder ID. The API request does not give a response.\n$Win32FileEncryptionInfo = @{ \u0026#34;fileEncryptionInfo\u0026#34; = [ordered]@{ \u0026#34;encryptionKey\u0026#34; = $DetectionXMLContent.ApplicationInfo.EncryptionInfo.EncryptionKey \u0026#34;macKey\u0026#34; = $DetectionXMLContent.ApplicationInfo.EncryptionInfo.macKey \u0026#34;initializationVector\u0026#34; = $DetectionXMLContent.ApplicationInfo.EncryptionInfo.initializationVector \u0026#34;mac\u0026#34; = $DetectionXMLContent.ApplicationInfo.EncryptionInfo.mac \u0026#34;profileIdentifier\u0026#34; = \u0026#34;ProfileVersion1\u0026#34; \u0026#34;fileDigest\u0026#34; = $DetectionXMLContent.ApplicationInfo.EncryptionInfo.fileDigest \u0026#34;fileDigestAlgorithm\u0026#34; = $DetectionXMLContent.ApplicationInfo.EncryptionInfo.fileDigestAlgorithm } } | ConvertTo-Json $CommitResourceUri = \u0026#34;{0}/commit\u0026#34; -f $storageCheckUrl, $Win32LobPlaceHolder.id Invoke-RestMethod -uri $CommitResourceUri -Method \u0026#34;POST\u0026#34; -Body $Win32FileEncryptionInfo -Headers $authHeader After committing the file, I check the upload state.\n$CommitStatus = Invoke-RestMethod -uri $storageCheckUrl -Method GET -Headers $authHeader $CommitStatus Update file version At last, the code below tells the application which version is committed.\n$Win32AppCommitBody = [ordered]@{ \u0026#34;@odata.type\u0026#34; = \u0026#34;#microsoft.graph.win32LobApp\u0026#34; \u0026#34;committedContentVersion\u0026#34; = $win32LobAppVersionRequest.id } | ConvertTo-Json $Win32AppUrl = \u0026#34;{0}/{1}\u0026#34; -f $win32LobUrl, $win32LobApp.id Invoke-RestMethod -uri $Win32AppUrl -Method \u0026#34;PATCH\u0026#34; -Body $Win32AppCommitBody -Headers $authHeader Summary In this blog post, I explained in basics how to update (intunewin) files with the Graph API. The whole process is quite complex and I showed the happy flow with small code blocks. There are no checks and the code is not very efficient. Luckily there is a good PowerShell module that has all the logic and intelligence to upload intunewin files with the Graph API. I would recommend taking a look at Nicolaj Andersen\u0026rsquo;s PowerShell module IntuneWin32App. See the GitHub project at: https://github.com/MSEndpointMgr/IntuneWin32App or in the PowerShell gallery at: https://www.powershellgallery.com/packages/IntuneWin32App/1.3.6\nThank you for reading my blog win32lob intunewin file upload process explained for automation. I hope you got a bit inspired. Enjoy your day and happy automating üëã ","date":"December 9, 2022","image":"http://localhost:1313/win32lob-intunewin-file-upload-process-explained-for-automation/upload.jpeg","permalink":"/win32lob-intunewin-file-upload-process-explained-for-automation/","title":"Win32LOB intunewin file upload process explained for automation"},{"categories":["Microsoft Intune","WinGet"],"contents":"In the week of November 28, 2022, Microsoft released the new Microsoft Store which is implemented in Intune using the Windows Packager Manager. This is because the Microsoft Store for Business will be deprecated in the first quarter of 2023.\nIt is a great feature that helps distribute applications in a very simple way.\nYou can find the announcement here.\nIn this blog post, I show how to create these store apps in Intune in an automated way. I show how to get the correct application from the store and configure the Windows Store App into Intune.\nTable Of Contents Search applications in Microsoft Store Get the application details from Microsoft Store Get Windows Store application icon URL Deploy Windows Store application in Intune automated with icon Assign Windows Store application Search applications in Microsoft Store The first step is searching for the application in the Microsoft Store. When using the portal start typing the application name. Every result that contains your search show up in the results.\nThere is no exact match search option available. In the case of automation, we need an exact match to make sure the specified application will be installed.\nIn the example below, I install WhatsApp on every device. To search in the Microsoft store, I use the API below. Microsoft Store for Endpoints API URL: https://storeedgefd.dsx.mp.microsoft.com\n$appName = \u0026#34;WhatsApp\u0026#34; $storeSearchUrl = \u0026#34;https://storeedgefd.dsx.mp.microsoft.com/v9.0/manifestSearch\u0026#34; $body = @{ Query = @{ KeyWord = $appName MatchType = \u0026#34;Substring\u0026#34; } } | ConvertTo-Json $appSearch = Invoke-RestMethod -Uri $storeSearchUrl -Method POST -ContentType \u0026#39;application/json\u0026#39; -body $body After searching, I got the results below. WhatsApp and WhatsApp Beta, in my case I want to install the WhatsApp application. In the code, we see the same. To make sure the correct application is installed, I search for the exact match based on the application name I provided.\n$exactApp = $appSearch.Data | Where-Object { $_.PackageName -eq $appName } Get the application details from Microsoft Store In the Microsoft Store, every application has a unique ID. The identifier is the ID we need in the next steps. Before adding the application to Intune, we need more information. Based on the identifier a next search is needed. The next search is searching for the application details.\nTo get the application details from the Microsoft Store with code I used the PowerShell code below. To get package details, I use this Microsoft Store API URL: https://storeedgefd.dsx.mp.microsoft.com/v9.0/packageManifests/{id}.\n$appUrl = \u0026#34;https://storeedgefd.dsx.mp.microsoft.com/v9.0/packageManifests/{0}\u0026#34; -f $exactApp.PackageIdentifier $app = Invoke-RestMethod -Uri $appUrl -Method GET $appId = $app.Data.PackageIdentifier $appInfo = $app.Data.Versions[-1].DefaultLocale $appInstaller = $app.Data.Versions[-1].Installers To make sure I get the latest version, I use the [-1] selector behind the versions-object.\nGet Windows Store application icon URL A second request is needed to the Microsoft store to get an icon URL. This time I use the \u0026rsquo;normal\u0026rsquo; store URL. Unfortunately, the endpoint store does not return an icon URL. In the \u0026rsquo;normal\u0026rsquo; store the installation scope is not returned, so I need both.\nIn the code below I request the specific application, with the ID again. Thereafter, I download the image to a temporary location, convert it to a hashed string and sent the output in the deploy body.\n$imageUrl = \u0026#34;https://apps.microsoft.com/store/api/ProductsDetails/GetProductDetailsById/{0}?hl=en-US\u0026amp;gl=US\u0026#34; -f $exactApp.PackageIdentifier $image = Invoke-RestMethod -Uri $imageUrl -Method GET $wc = New-Object System.Net.WebClient $wc.DownloadFile($image.IconUrl, \u0026#34;./temp.jpg\u0026#34;) $base64string = [Convert]::ToBase64String([IO.File]::ReadAllBytes(\u0026#39;./temp.jpg\u0026#39;)) Deploy Windows Store application in Intune automated with icon We have all the needed details to create a valid request. In the request below I deploy the application in Intune first.\nAs you can see there is a new type available called #microsoft.graph.winGetApp. Using this type gives options to provide Microsoft Store application details in the deployment.\nTo authenticate please check this part of my other blog: https://www.rozemuller.com/intune-filters-explained-and-create-automated/#authentication\nGive your app the correct permissions: DeviceManagementApps.ReadWrite.All\nThe body isn\u0026rsquo;t very hard. The only thing I noticed is the installExperience.\n$deployUrl = \u0026#34;https://graph.microsoft.com/beta/deviceAppManagement/mobileApps\u0026#34; $appBody = @{ \u0026#39;@odata.type\u0026#39; = \u0026#34;#microsoft.graph.winGetApp\u0026#34; description = $appInfo.ShortDescription developer = $appInfo.Publisher displayName = $appInfo.packageName informationUrl = $appInfo.PublisherSupportUrl largeIcon = @{ \u0026#34;@odata.type\u0026#34;= \u0026#34;#microsoft.graph.mimeContent\u0026#34; type = \u0026#34;String\u0026#34; value = $base64string } installExperience = @{ runAsAccount = $appInstaller.scope } isFeatured = $false packageIdentifier = $appId privacyInformationUrl = $appInfo.PrivacyUrl publisher = $appInfo.publisher repositoryType = \u0026#34;microsoftStore\u0026#34; roleScopeTagIds = @() } | ConvertTo-Json $appDeploy = Invoke-RestMethod -uri $deployUrl -Method POST -Headers $authHeader -Body $appBody Make sure you use the correct scope (user / system). Using the wrong scope results in an error like below.\nYour app is not ready yet. If app content is uploading, wait for it to finish. If app content is not uploading, try creating the app again.\nAssign Windows Store application The last step assigning the application to users or devices. To assign the Windows Store application to all devices, for example, I used the code below. Also in this body, there is a new type called microsoft.graph.winGetAppAssignmentSettings\n$assignUrl = \u0026#34;https://graph.microsoft.com/beta/deviceAppManagement/mobileApps/{0}/assign\u0026#34; -f $appDeploy.Id $assignBody = @{ mobileAppAssignments = @( @{ \u0026#34;@odata.type\u0026#34; = \u0026#34;#microsoft.graph.mobileAppAssignment\u0026#34; target = @{ \u0026#34;@odata.type\u0026#34; = \u0026#34;#microsoft.graph.allDevicesAssignmentTarget\u0026#34; } intent = \u0026#34;Required\u0026#34; settings = @{ \u0026#34;@odata.type\u0026#34; = \u0026#34;#microsoft.graph.winGetAppAssignmentSettings\u0026#34; notifications = \u0026#34;showAll\u0026#34; installTimeSettings = $null restartSettings = $null } } ) } | ConvertTo-Json -Depth 8 Invoke-RestMethod -uri $assignUrl -Method POST -Headers $authHeader -ContentType \u0026#39;application/json\u0026#39; -body $assignBody In the end, the Windows Store application is deployed including assignment and an icon in an automated way. If you want more underlying information in detail you should check Peter Klapwijk his post. You can check it here: https://www.inthecloud247.com/new-microsoft-store-integrated-in-intune/\nYou can find the script in my GitHub repository: https://github.com/srozemuller/MicrosoftEndpointManager/blob/main/Deployment/Applications/deploy-win-store-app.ps1 Thank you for reading my blog add microsoft store (winget) app with icon into intune automated. I hope you got a bit inspired. Enjoy your day and happy automating üëã ","date":"December 4, 2022","image":"http://localhost:1313/add-microsoft-store-app-with-icon-into-intune-automated/store-trolley.jpeg","permalink":"/add-microsoft-store-app-with-icon-into-intune-automated/","title":"Add Microsoft Store (WinGet) app with icon into Intune automated"},{"categories":["Log Analytics","Microsoft Intune"],"contents":"I often write blogs about getting data with the Graph API and using it for monitoring. A while ago I was thinking about how to get my own data into Log Analytics.\nIn this blog post, I show how to ingest custom MEM data into a Log Analytics table. I also show how to configure all components in an automated way.\nI knew it was possible to create custom tables but it was only possible to ingest data from an extension, performance counters, Syslog, or Windows events as a source. Now I have REST API output. After some search in Microsoft Docs, I found an ARM template that allows me to create another type of data collection rule. That allows me to ingest data that come from any other resource.\nTable Of Contents Situation Architecture Authentication Before we start Get the data Create a custom Log Analytics table Create a Data Collection Endpoint (Preview) Data Collection Rule Data collection structure in ARM Data Collection Rule deployment Ingest your data Set permissions on DCR Push the data Summary Keep in mind Situation What is the situation? I have data grabbed with the Graph API. In my case, Microsoft Secure Score recommendation action information. I want to create workbooks that show that information. The data is not available out of the box and not with diagnostics settings.\nI need this custom data in Log Analytics.\nArchitecture The schema below shows what components I have and what is configured at the end of this blog. On the left, I have a task that grabs the information with the Graph API. A service principal is used for that. On the right side, we have Log Analytics and the data collection environment.\nThe service principal from the grab script has also been permitted to use the Data Collection Rule.\nAuthentication In this blog, different authentication layers will pass. To clarify, I divided these layers into three themes. Deployment, Grab and Deliver. In the deployment part, I use an Azure account with proper permissions to create resources. I connect with Connect-AzAccount and request a token. (See the code below)\nConnect-Azaccount $tokenInfo = Get-AzAccessToken -ResourceUrl \u0026#34;https://management.azure.com\u0026#34; $authHeader = @{ Authorization = \u0026#34;{0} {1}\u0026#34; -f $tokenInfo.Type, $tokenInfo.Token ContentType = \u0026#34;application/json\u0026#34; } $subscriptionId = (Get-AzContext).Subscription $resourceGroupName = \u0026#39;rg-mem-monitoring\u0026#39; In the Grab and Deliver layers I use a service principal with Graph API permissions to grab the data. In this blog, I use the same account to grab and deliver the data to Log Analytics.\nBefore we start I‚Äôm always excited when it comes to using the technique. But for now, hold your horses. Before we dive into the awesome techniques we first need to know what data we want in the table. However, data ingestion is the last step we need to think about the data first. Because the output decides how we have to build the environment.\nGet the data In the example below I used data from one of my earlier blogposts. With the Graph API, I search for security improvement action statuses. This information is not available out of the box to monitor. In the blog post, I search for actions with an ignored state. For this example, I skip the ignore-state part and search for all statuses.\nAs you see, I want to output with a time, the ID, Title, Service, State, UpdatedBy, and, UpdatedDateTime. We have to create columns for that in our new custom table.\nThis is the Grab layer, I used a service principal with the correct Graph API permissions.\n$policies = Invoke-RestMethod @parameters $allobjects = [System.Collections.ArrayList]@() foreach ($pol in $policies.value) { $pol.controlStateUpdates | ForEach-Object { $object = [PSCustomObject]@{ TimeGenerated = $currentTime Id = $pol.Id Title = $pol.Title Service = $pol.Service State = $_.State UpdatedBy = $_.UpdatedBy UpdatedDateTime = $_.UpdatedDateTime } $allobjects.Add($object) | Out-Null } } Create a custom Log Analytics table To store data, we use a Log Analytics Workspace. In LAWS we must create a new custom table. To create a custom table I use the REST API. Based on the data above, we create a table schema. Below I created a table schema with seven columns. The columns are based on the output above.\n$tableSchema = @( @{ \u0026#34;name\u0026#34; = \u0026#34;TimeGenerated\u0026#34; \u0026#34;type\u0026#34; = \u0026#34;datetime\u0026#34; \u0026#34;description\u0026#34; = \u0026#34;The time at which the data was generated\u0026#34; } @{ \u0026#34;name\u0026#34; = \u0026#34;Id\u0026#34; \u0026#34;type\u0026#34; = \u0026#34;string\u0026#34; \u0026#34;description\u0026#34; = \u0026#34;The policys name\u0026#34; } @{ \u0026#34;name\u0026#34; = \u0026#34;Title\u0026#34; \u0026#34;type\u0026#34; = \u0026#34;string\u0026#34; \u0026#34;description\u0026#34; = \u0026#34;The policys title\u0026#34; } @{ \u0026#34;name\u0026#34; = \u0026#34;Service\u0026#34; \u0026#34;type\u0026#34; = \u0026#34;string\u0026#34; \u0026#34;description\u0026#34; = \u0026#34;The which service belongs the policy\u0026#34; } @{ \u0026#34;name\u0026#34; = \u0026#34;State\u0026#34; \u0026#34;type\u0026#34; = \u0026#34;string\u0026#34; \u0026#34;description\u0026#34; = \u0026#34;The new state\u0026#34; } @{ \u0026#34;name\u0026#34; = \u0026#34;UpdatedBy\u0026#34; \u0026#34;type\u0026#34; = \u0026#34;string\u0026#34; \u0026#34;description\u0026#34; = \u0026#34;Who did the update\u0026#34; } @{ \u0026#34;name\u0026#34; = \u0026#34;UpdatedDateTime\u0026#34; \u0026#34;type\u0026#34; = \u0026#34;datetime\u0026#34; \u0026#34;description\u0026#34; = \u0026#34;When did the updated happend\u0026#34; } ) After we have exactly know what data to ingest we create a table with the code below. As you can see the $tableSchema is stored in the columns object. During the deployment, I store the output into the $table variable. We need some table information in later steps.\nThis is the deployment layer. Use the Azure account to deploy.\n$subscriptionId = (Get-AzContext).Subscription $resourceGroupName = \u0026#34;rg-mem-monitoring\u0026#34; $lawsName = \u0026#34;la-mem-workspace78433\u0026#34; $tableName = \u0026#34;SecImpAction_CL\u0026#34; $tableBody = @{ properties = @{ schema = @{ name = $tableName columns = $tableSchema } } } | ConvertTo-Json -Depth 10 $createTableParameters = @{ uri = \u0026#34;https://management.azure.com/subscriptions/{0}/resourceGroups/{1}/providers/Microsoft.OperationalInsights/workspaces/{2}/tables/{3}?api-version=2021-12-01-preview\u0026#34; -f $subscriptionId, $resourceGroupName, $lawsName, $tableName method = \u0026#34;PUT\u0026#34; headers = $authHeader body = $tableBody } $table = Invoke-RestMethod @createTableParameters As you can see the table is created with the provided columns. Besides that, during deployment, Microsoft also put some extra columns into the table. These are standard columns. The table can be found under the Custom Logs component.\nCreate a Data Collection Endpoint (Preview) At the moment of writing, this feature is in preview. A DCE is a public endpoint to deliver logs. Every DCE has its own private endpoint URL and ID (immutableId). In the code below I create an endpoint and store the output in the $dce variable. Also from this variable we information in later steps.\nAlso, this is a part of the deployment layer.\n$dceName = \u0026#34;custom-dce\u0026#34; $dceUrl = \u0026#34;https://management.azure.com/subscriptions/{0}/resourceGroups/{1}/providers/Microsoft.Insights/dataCollectionEndpoints/{2}?api-version=2021-04-01\u0026#34; -f $subscriptionId, $resourceGroupName, $dceName $dceBody = @{ Location = \u0026#34;WestEurope\u0026#34; properties = @{ networkAcls = @{ publicNetworkAccess = \u0026#34;Enabled\u0026#34; } } } | ConvertTo-Json -Depth 5 $dce = Invoke-RestMethod -Uri $dceUrl -Method PUT -Headers $authHeader -body $dceBody $dce For more information about Data Collection Endpoints, check the documentation.\nData Collection Rule A data collection rule (DCR) is the most intelligent part and the data flow‚Äôs brain. A DCR can be compared with a network route table. It tells how to route your data to the next hop.\nWhen creating a DCR through the portal we are a bit limited in resources. We only can select Azure resources. But there are options. We have the option to create a DCR with an ARM template. The DCR structure is quite simple. We have three components, a stream, a destination, and a data flow.\nData collection structure in ARM A stream has a name and a data structure called columns in ARM. The stream name is the stream identification. In the column part, you provide the custom table‚Äôs data structure. The column names must exactly match the table.\nIn the destination part, the rule must send the data to. Also, a destination has a name. In addition, it also has a Log Analytics part. You can send data to more than one workspace.\nAt last, we have the data flow. In a data flow, we configure the stream- and destination name (from previous steps). In the next part, we have a transform KQL part. We need to provide a Kusto Query on how the incoming data must be converted to the custom table. In my example, we have straight data. I try to provide clean data as much as possible (rubbish in is rubbish out). Withing the DCR the query converts the data and sends it to the output stream.\nFor more information about transformation, check the transformation docs.\nThe stream name must begin with Custom- followed by the custom table name. To achieve that, I created a variable in the ARM template.\nData Collection Rule deployment I modified the template, to make the template suitable for automation. The template accepts extra parameters with data out of the variables in earlier steps. The template is stored on my GitHub. If you prefer the Azure portal, then use a custom deployment.\nTo deploy the data collection rule with an ARM template I used PowerShell. Download the template from my GitHub and store it in a good location. From that location run the code below.\n(Assuming you did the deployments at the first steps and stored the output in a variable)\nFirst I grab the created columns from the table to use in the Kusto query. This is the query that converts the data to the output stream, which is the custom table. This is fully supported Kusto Query language. In the $columns variable I create a part of the Kusto query based on the columns in the custom table. Using the table output helps me create a Kusto query fully automated.\nAgain, this is the deployment layer.\n$columns = $([String]::Join(\u0026#39;, \u0026#39;, ($table.properties.schema.columns | Select-Object name).Name)) $kusto = \u0026#34;source | project {0}\u0026#34; -f $columns $dcrName = \u0026#34;secImprove-dcr\u0026#34; $streamName = \u0026#34;Custom-new-stream\u0026#34; $parameterObject = @{ dataCollectionRuleName = $dcrName workspaceResourceId = \u0026#34;/subscriptions/{0}/resourceGroups/{1}/providers/Microsoft.OperationalInsights/workspaces/{2}\u0026#34; -f $subscriptionId, $resourceGroupName, $lawsName endpointResourceId = $dce.id tableColumns = $tableSchema streamName = $streamName tableName = $tableName kustoQuery = $kusto } $dcr = New-AzResourceGroupDeployment -resourcegroupname $resourceGroupName -Name $dcrName -TemplateParameterObject $parameterObject -TemplateFile ./data-collection-rule.json I also looked to create a DCR with the REST API and custom data but got some errors that not every component is available at the REST API endpoint.\nIngest your data Till now we have created a custom table on a Log Analytics Workspace, created a public data endpoint to accept data, and, a data rule to map data from the outside on the correct table. At this point, we only have just one more thing to do before we are able to ingest data into the table. Setting permissions on the collection rule to the account which is ingesting data.\nSet permissions on DCR To ingest data over the Data Collection Rule we need permissions. In the code below, a service principal is added with the Monitoring Metrics Publisher role. The ID in the code below is a fixed ID provided by Microsoft. To get a list of all build-in roles, check the Azure build-in roles overview. The principal ID is the service principal‚Äôs object ID.\nThis is also the deployment layer\n$guid = (new-guid).guid $monitorMetricsPublisherRoleId = \u0026#34;3913510d-42f4-4e42-8a64-420c390055eb\u0026#34; $roleDefinitionId = \u0026#34;/subscriptions/{0}/providers/Microsoft.Authorization/roleDefinitions/{1}\u0026#34; -f $subscriptionId, $monitorMetricsPublisherRoleId $roleUrl = \u0026#34;https://management.azure.com/{0}/providers/Microsoft.Authorization/roleAssignments/{1}?api-version=2018-07-01\u0026#34; -f $dcr.outputs.datacollectionRuleId.value, $guid $roleBody = @{ properties = @{ roleDefinitionId = $roleDefinitionId principalId = \u0026#34;xxxxx-xxxx-xxxx-xxxx-xxxxxxx\u0026#34; # Service Principal object ID scope = $dcr.outputs.datacollectionRuleId.value } } $jsonRoleBody = $roleBody | ConvertTo-Json -Depth 6 Invoke-RestMethod -Uri $roleUrl -Method PUT -Body $jsonRoleBody -headers $authHeader Push the data Now it is time to push the data. We‚Äôve got the data from the first chapter in this blog and sent it to the Data Collection Endpoint. Because the DCE is behind the monitor.azure.com endpoint, we need a new token. In the code block below I request a new token based on the same service principal as used in the ‚Äòget-the-data‚Äô-part.\nThis is the deliver layer, get a new token based on the service principal with Monitoring Metrics Publisher permissions.\n$tenantId = \u0026#34;xxx\u0026#34; $secMonAppId = \u0026#34;xxx\u0026#34; $secMonAppSecret = \u0026#34;xxx\u0026#34; $scope = [System.Web.HttpUtility]::UrlEncode(\u0026#34;https://monitor.azure.com//.default\u0026#34;) $body = \u0026#34;client_id=$secMonAppId\u0026amp;scope=$scope\u0026amp;client_secret=$secMonAppSecret\u0026amp;grant_type=client_credentials\u0026#34;; $headers = @{\u0026#34;Content-Type\u0026#34; = \u0026#34;application/x-www-form-urlencoded\u0026#34; }; $loginUri = \u0026#34;https://login.microsoftonline.com/$tenantId/oauth2/v2.0/token\u0026#34; $bearerToken = (Invoke-RestMethod -Uri $loginUri -Method \u0026#34;Post\u0026#34; -Body $body -Headers $headers).access_token After acquiring a token we pick up the body from the ‚Äòget-the-data‚Äô-part. The output is stored in the $allobjects variable, which must be converted to a JSON format. In the URI, first, we pick up the output from the Data Collector Endpoint and Data Collector Rule. Secondly, we reuse the $streamName also from the Data Collector Rule deployment. This makes an URL like:\nhttps://new-custom-dce-xxx.westeurope-1.ingest.monitor.azure.com/dataCollectionRules/dcr-02cd4b7c9e5844e3b2xxxxxxx/streams/Custom-new-stream?api-version=2021-11-01-preview\nThen, we sent the data with REST API to the endpoint. Good to know that the REST API does not give a response. I mentioned that to the Microsoft team to investigate.\n$uploadBody = $allobjects | ConvertTo-Json $uri = \u0026#34;{0}/dataCollectionRules/{1}/streams/{2}?api-version=2021-11-01-preview\u0026#34; -f $dce.properties.logsIngestion.endpoint, $dcr.Outputs.immutableId.Value, $streamName $monitorHeaders = @{\u0026#34;Authorization\u0026#34; = \u0026#34;Bearer $bearerToken\u0026#34;; \u0026#34;Content-Type\u0026#34; = \u0026#34;application/json\u0026#34; }; $uploadResponse = Invoke-RestMethod -Uri $uri -Method POST -Body $uploadBody -Headers $monitorHeaders $uploadResponse In the end, the data has arrived at the Log Analytics Workspace.\nSummary In this blog post about how to ingest custom MEM data into Log Analytics based on Custom logs, I showed which components we need and how to implement an automated way. First, we looked at our data to create a custom Log Analytics table. After we created a table, we deployed a Data Collector Endpoint. The DCE is the \u0026lsquo;outside\u0026rsquo; of Azure Monitor to send data to.\nAfter the DCE deployment, we created a custom Data Collector Rule which routes data to the custom table. A DCR has a stream, destination, and a dataflow. We also set the Monitor Metrics Publisher role on the DCR.\nAt last, we grabbed the data and sent it to the DCE.\nKeep in mind There are a few things to keep in mind that I like to share.\nThe custom Data Collector Rule is in preview; The data may take some time to be ingested, especially if this is the first time data is being sent to a particular table. It shouldn‚Äôt take longer than 15 minutes; The data ingestion input schema must exactly match the database schema. Otherwise, the data will not import; The table name must end with _CL; The table must contain a column name TimeGenerated; https://docs.microsoft.com/en-us/azure/azure-monitor/logs/tutorial-custom-logs\nThank you for reading my blog route own intune data to log analytics using custom logs. I hope you got a bit inspired. Enjoy your day and happy automating üëã ","date":"November 30, 2022","image":"http://localhost:1313/route-own-intune-data-to-log-analytics-using-custom-logs/wired.jpeg","permalink":"/route-own-intune-data-to-log-analytics-using-custom-logs/","title":"Route own Intune data to Log Analytics using custom logs"},{"categories":["Azure AD","Security"],"contents":"It looks like Microsoft has added a new option that allows users to create tenants. Some people have seen the Tenant Creation option in the regular Azure portal but was removed fast. For now, the option is still available in the Preview Azure Portal (https://preview.portal.azure.com/)\nThe nasty thing about it is that the option default allows users to create new tenants. Also without a global admin or tenant creator role.\nIn this short blog, I show how to disable this option for users.\nTable Of Contents Disable Tenant Creation option Get authorization policy Update authorization policy Disable Tenant Creation option What does tenant creation allow:\n\u0026lsquo;Yes\u0026rsquo; allows default users to create Azure AD tenants. \u0026lsquo;No\u0026rsquo; allows only users with the global administrator or tenant creator roles to create Azure AD tenants. Anyone who creates a tenant will become the global administrator for that tenant.\nThe tenant creation option is part of the authorization policy and is available in the Graph API BETA version. See: https://learn.microsoft.com/en-us/graph/api/authorizationpolicy-get?view=graph-rest-beta\nThe documentation, however, is not updated (yet).\nGet authorization policy Use the command below to update the authorization policy with the Graph API. First, I request the authorization policy to get the name.\nConnect-AzAccount $resource = \u0026#34;https://graph.microsoft.com\u0026#34; $context = [Microsoft.Azure.Commands.Common.Authentication.Abstractions.AzureRmProfileProvider]::Instance.Profile.DefaultContext $Token = [Microsoft.Azure.Commands.Common.Authentication.AzureSession]::Instance.AuthenticationFactory.Authenticate($context.Account, $context.Environment, $context.Tenant.Id.ToString(), $null, [Microsoft.Azure.Commands.Common.Authentication.ShowDialog]::Never, $null, $resource).AccessToken $authHeader = @{ \u0026#39;Content-Type\u0026#39; = \u0026#39;application/json\u0026#39; Authorization = \u0026#39;Bearer \u0026#39; + $Token } $policyUrl = \u0026#34;https://graph.microsoft.com/beta/policies/authorizationPolicy\u0026#34; $request = Invoke-RestMethod -Uri $policyUrl -Method GET -Headers $authHeader $request.value Update authorization policy Based on the policy name the command below updates the role permission where the new option is in.\n$body = @{ defaultUserRolePermissions = @{ allowedToCreateTenants = $false } } | ConvertTo-Json $updateUrl = \u0026#34;https://graph.microsoft.com/beta/policies/authorizationPolicy/{0}\u0026#34; -f $req.value.id Invoke-RestMethod -Uri $updateUrl -Method PATCH -Headers $authHeader -Body $body The command does not return any content.\nCheck the discussion below.\nThat preview toggle is to restrict member users in THAT tenant from creating other Azure AD tenants. This setting enables admins to constrain that to only the admin role holders in the tenant. This gives admins greater control over the existing functionality to disable it.\n\u0026mdash; Jef Kazimer (@JefTek) November 17, 2022 Thank you for reading my blog disable user tenant creation. I hope you got a bit inspired. Enjoy your day and happy automating üëã ","date":"November 17, 2022","image":"http://localhost:1313/disable-user-tenant-creation/user-create-tenant.png","permalink":"/disable-user-tenant-creation/","title":"Disable user tenant creation"},{"categories":["Azure Virtual Desktop","Network"],"contents":"Recently I had a situation where a customer asked me how to make sure the AVD environment always has the same WAN IP. To give Azure Virtual Desktop a fixed external IP, some options are available. Think about a NAT gateway or an Azure Firewall. In this blog post, I show how to configure a static WAN IP for AVD with the use of the Azure Firewall in an automated way.\nWhen creating an Azure Firewall we need additional resources. A public IP (becomes the static WAN IP for AVD), a Firewall Policy, and a virtual network. In this blog, I assume that you already have a fully operating Azure Virtual Desktop environment. From that point, we pick up the deployment.\nTable Of Contents Static WAN IP for AVD Architecture Deployment strategy Authentication Azure Firewall setup Add AzureFirewallSubnet subnet to existing AVD VNET Get AVD network information with Az.Avd Create public IP Create Azure Firewall policies Deploy network policy rule collection and assign it to the policy Deploy Azure Firewall with REST API AVD Network routing Routing source and destination Next hop, get Azure Firewall‚Äôs private IP Create route table Assign route table to subnet Summary Static WAN IP for AVD Architecture To clarify configuring a static WAN IP for AVD, I have drawn the schema below. The main idea is to send all the traffic from an existing AVD subnet helped by a routing table through the Azure Firewall. The Azure Firewall has a public IP that represents the network.\nDeployment strategy Before we start to deploy, I assume you already have a fully operating AVD environment. (If not, please check my AVD Automation Cocktail series. In this series, I show different AVD deployment methods).\nThe first step is searching for the current VNET used in your AVD environment. In the existing VNET, I create a new subnet called AzureFirewallSubnet (must be this name).\nSecondly, I create the public IP. The public IP is the static external WAN IP that represents the AVD network on the outside.\nNext, I create a firewall policy. The policy has settings from the protect AVD documentation.\nBased on the best practices I create a rule collection and add it to the just-created policy. In the firewall, I add the policy.\nAt last, I bring the AVD and firewall parts together by creating a routing table and assigning the table to the existing AVD subnet. The routing table sends all the outside traffic (0-route) to the Azure Firewall.\nThe reason why I created a routing table, at last, is because of not having downtime. Routing traffic to non-existing addresses results in sending traffic to a dead end.\nAuthentication To authenticate I use an Azure login account with enough permissions at the subscription or resource group level. In this situation, I do not use an app registration.\nConnect-AzAccount $token = Get-AzAccessToken -ResourceUrl \u0026#34;https://management.azure.com\u0026#34; $authHeader = @{ \u0026#39;Content-Type\u0026#39; = \u0026#39;application/json\u0026#39; Authorization = \u0026#39;Bearer \u0026#39; + $token.Token } Azure Firewall setup I hear people thinking Azure Firewall, Why?!? A NAT gateway is cheaper. And yes, you are right. Within the AVD community, we experienced a lot of disconnects when using a NAT gateway. An Azure Firewall is more stable.\nAdd AzureFirewallSubnet subnet to existing AVD VNET As mentioned we are going to update an existing Azure Virtual Desktop environment with no Azure Firewall already placed. During an Azure Firewall deployment, you must create a subnet named AzureFirewallSubnet. This is the firewall‚Äôs own subnet.\nFirst I search for the current VNET using the Get-AvdNetworkInfo command. This command is a part of the Az.Avd PowerShell module. Based on the provided AVD host pool, this command searches for the session host`s virtual network and subnet information. During the subnet creation, I use the VNET information.\nLogin in with the Connect-AzAccount command.\nInstall-Module Az.Avd Import-Module Az.Avd Get AVD network information with Az.Avd To get the current AVD subnet information, I use the Az.Avd PowerShell module. This module searches network information based on the session hosts in a host pool. Because we need the VNET information in the next step, I stored the command output in a variable.\n$Parameters = @{ hostpoolName = \u0026#34;Rozemuller-hostpool\u0026#34; resourceGroupName = \u0026#34;rg-roz-avd-01\u0026#34; } $network = Get-AvdNetworkInfo @Parameters The $network variable has the network information of all of the session hosts. The first step in creating a new subnet with the REST API is getting a token that is provided in the header information.\nSecondly, I create a body with the address prefix, and the network segment, in it. The body is in JSON format. In the next step, I create a splatting table with all the parameters in it. Parameters provided to the API request.\nThe URI is formatted with the base REST API URL, the VNET resource ID, and the new subnet name. Because I only need one VNET ID, I picked the first record in the output variable by adding the [0] behind.\n$body = @{ properties = @{ addressPrefix = \u0026#34;10.1.0.0/26\u0026#34; } } | ConvertTo-Json $vnetParameters = @{ uri = \u0026#34;https://management.azure.com/{0}/subnets/{1}?api-version=2021-05-01\u0026#34; -f $network.VnetId[0], \u0026#34;AzureFirewallSubnet\u0026#34; method = \u0026#34;PUT\u0026#34; body = $body headers = $authHeader } $subnet = Invoke-RestMethod @vnetParameters Take a note about the address prefix, which must be at least a /26 network.\nCreate public IP In the next part, we create a public IP with the Azure REST API. At first, I use the variables to create the REST API URL. Secondly, I create a body and convert it to JSON format again. At last, I create a splatting table with all the parameters. I re-use the $authHeader from the subnet part.\n$resourceGroup = \u0026#34;rg-roz-avd-01\u0026#34; $subscription = (Get-AzContext).Subscription $publicIpName = \u0026#34;pip-external-avd\u0026#34; $uri = \u0026#34;https://management.azure.com/subscriptions/{0}/resourceGroups/{1}/providers/Microsoft.Network/publicIPAddresses/{2}?api-version=2021-05-01\u0026#34; -f $subscription, $resourceGroup, $publicIpName $pipBody = @{ properties = @{ publicIPAllocationMethod = \u0026#34;Static\u0026#34; idleTimeoutInMinutes = 10 publicIPAddressVersion = \u0026#34;IPv4\u0026#34; } sku = @{ name = \u0026#34;Standard\u0026#34; tier = \u0026#34;Regional\u0026#34; } location = \u0026#34;westeurope\u0026#34; } | ConvertTo-Json $pipParameters = @{ uri = $uri method = \u0026#34;PUT\u0026#34; body = $pipBody headers = $authHeader } $pip = Invoke-RestMethod @pipParameters $pip For more information about creating public IPs through the REST API, check the public IP documentation.\nCreate Azure Firewall policies Firewall policies are the firewall‚Äôs brains. With policies, you tell how the firewall acts and what to do with network traffic. For this situation, routing AVD traffic to a fixed external IP, we also create a policy. In the example below, I create a policy that protects your AVD environment based on Microsoft‚Äôs best practices. This will not mean this policy fits your own security policy.\nCheck the documentation about how to secure your AVD environment in detail here.\nI created a policy in West Europe named fw-pol-avd. Because I need the policy in later steps, I store the output in the $policy variable.\n$resourceGroup = \u0026#34;rg-roz-avd-01\u0026#34; $subscription = (Get-AzContext).Subscription $policyName = \u0026#34;fw-pol-avd\u0026#34; $policyUrl = \u0026#34;https://management.azure.com/subscriptions/{0}/resourceGroups/{1}/providers/Microsoft.Network/firewallPolicies/{2}?api-version=2021-08-01\u0026#34; -f $subscription, $resourceGroup, $policyName $body = @{ location = \u0026#34;WestEurope\u0026#34; properties = @{ sku = @{ tier = \u0026#34;Standard\u0026#34; } threatIntelMode = \u0026#34;Alert\u0026#34; } } | ConvertTo-Json -Depth 10 $parameters = @{ uri = $policyUrl method = \u0026#34;PUT\u0026#34; headers = $authHeader body = $body } $policy = Invoke-RestMethod @parameters For more information about creating a firewall policy with the REST API, please check the docs.\nDeploy network policy rule collection and assign it to the policy The next step is creating network rules which allow and deny traffic to our AVD subnet. In this step, I need the powershell $network variable again from the first step. As mentioned above I use Microsoft‚Äôs secure AVD best practices in this example. The collection is committed to the policy. I use the $policy variable to assign the collection rule to the policy.\n$collectionUrl = \u0026#34;https://management.azure.com/{0}/ruleCollectionGroups/DefaultNetworkRuleCollectionGroup?api-version=2021-08-01\u0026#34; -f $policy.id $body = @{ properties = @{ priority = 100 ruleCollections = @( @{ ruleCollectionType = \u0026#34;FirewallPolicyFilterRuleCollection\u0026#34; name = \u0026#34;AVD-Outbound-Collection\u0026#34; priority = 100 action = @{ type = \u0026#34;Allow\u0026#34; } rules = @( @{ ruleType = \u0026#34;NetworkRule\u0026#34; name = \u0026#34;http-80-outbound\u0026#34; ipProtocols = @( \u0026#34;TCP\u0026#34; ) destinationPorts = @( \u0026#34;80\u0026#34; ) sourceAddresses = @( $network[0].SubnetInfo.properties.addressPrefix ) destinationAddresses = @( \u0026#34;169.254.169.254\u0026#34;,\u0026#34;168.63.129.16\u0026#34; ) } @{ ruleType = \u0026#34;NetworkRule\u0026#34; name = \u0026#34;servicetags-outbound\u0026#34; ipProtocols = @( \u0026#34;TCP\u0026#34; ) destinationPorts = @( \u0026#34;443\u0026#34; ) sourceAddresses = @( $network[0].SubnetInfo.properties.addressPrefix ) destinationAddresses = @( \u0026#34;AzureCloud\u0026#34;, \u0026#34;WindowsVirtualDesktop\u0026#34; ) } @{ ruleType = \u0026#34;NetworkRule\u0026#34; name = \u0026#34;dns-53-outbound\u0026#34; ipProtocols = @( \u0026#34;TCP\u0026#34; ) destinationPorts = @( \u0026#34;53\u0026#34; ) sourceAddresses = @( $network[0].SubnetInfo.properties.addressPrefix ) destinationAddresses = @( \u0026#34;*\u0026#34; ) }, @{ ruleType = \u0026#34;NetworkRule\u0026#34; name = \u0026#34;kms-1688-outbound\u0026#34; ipProtocols = @( \u0026#34;TCP\u0026#34; ) destinationPorts = @( \u0026#34;1688\u0026#34; ) sourceAddresses = @( $network[0].SubnetInfo.properties.addressPrefix ) destinationAddresses = @( \u0026#34;23.102.135.246\u0026#34; ) } ) } ) } } | ConvertTo-Json -Depth 10 $parameters = @{ uri = $collectionUrl method = \u0026#34;PUT\u0026#34; headers = $authHeader body = $body } $ruleCollection = Invoke-RestMethod @parameters In addition to creating a firewall rule collection check the API documentation.\nDeploy Azure Firewall with REST API Creating a policy and rules completes the list with mandatory components to create an Azure Firewall. To clarify, let‚Äôs summarize what we have so far. We have searched for the current AVD subnet (network variable). This is the subnet that needs a fixed public IP. We have created a public IP (pip variable) and, create a firewall policy, with rules. (policy variable)\nBecause I stored every deployment into variables, I have all the needed information present.\nNow it is time to create the Azure Firewall with the REST API. In the body below all the components are coming together. The policy, the AVD VNET, and public IP. By providing the AzureFirewallSubnet subnet, the firewall is assigned to the correct VNET automatically.\n$resourceGroup = \u0026#34;rg-roz-avd-01\u0026#34; $subscription = (Get-AzContext).Subscription $fwName = \u0026#34;fw-avd\u0026#34; $fwUrl = \u0026#34;https://management.azure.com/subscriptions/{0}/resourceGroups/{1}/providers/Microsoft.Network/azureFirewalls/{2}?api-version=2021-08-01\u0026#34; -f $subscription, $resourceGroup, $fwName $fwBody = @{ location = \u0026#34;WestEurope\u0026#34; properties = @{ firewallPolicy = @{ id = $policy.id } sku = @{ name = \u0026#34;AZFW_VNet\u0026#34; tier = \u0026#34;Standard\u0026#34; } threatIntelMode = \u0026#34;Alert\u0026#34; ipConfigurations = @( @{ name = \u0026#34;azureFirewallIpConfiguration\u0026#34; properties = @{ subnet = @{ id = $subnet.id } publicIPAddress = @{ id = $pip.id } } } ) } } | ConvertTo-Json -Depth 99 $fwParameters = @{ uri = $fwUrl method = \u0026#34;PUT\u0026#34; body = $fwBody headers = $authHeader } Invoke-RestMethod @fwParameters Another great advantage of saving the output into variables is that the deployment bodies are way smaller than putting all the information into one single request. This makes your code more comfortable to read and troubleshoot. And fewer mistakes.\nCheck the Azure Firewall REST API documentation for more information.\nAVD Network routing In this last part, we need to route the traffic through the Azure Firewall. To route traffic in Azure use a Route Table. In a routing, table routes are defined. A route consists of a source, destination, and next-hop. In this chapter, I create a routing table, and a route and assign the routing table to the AVD subnet.\nRouting source and destination As mentioned we need to route AVD traffic to Internet via the Azure Firewall. Let‚Äôs find the source network first. To find the source AVD network, I use the Az.Avd PowerShell module. I like to refer back to the top of this blog post about how to get the AVD network information.\nAbout the destination, I could be very clear. In network language, the route to the Internet is also called the 0-route. This is because the Internet destination address is 0.0.0.0\nNext hop, get Azure Firewall‚Äôs private IP The next-hop represents the next network location where the traffic is sent. In this situation, we need the Azure Firewall‚Äôs private IP. With the code below, I search for the private IP address. The private IP address is available in the ipConfigurations properties. I store the whole output into the $fw variable. This is because I use the private IP address in the route creation.\n$fwParameters = @{ uri = $fwUrl method = \u0026#34;GET\u0026#34; headers = $authHeader } $fw = Invoke-RestMethod @fwParameters $fw.properties.ipConfigurations.properties.privateIPAddress The $fwUrl parameter is a variable from the firewall creation paragraph. In relation to that paragraph I know to use the GET method instead of PUT.\nCreating an Azure Firewall takes time. During deployment, the private is not available directly.\nUse a loop to check the deploy status. The deployment status is available in the general properties. ($fw.properties)\nCreate route table After creation, it is time to bring all the parts together with a route in a route table. With the code below I create a route table and a route at once with the REST API. Make sure you provide VirtualApplicance as the next hop type. The next-hop IP address is the firewall‚Äôs private IP.\n$resourceGroup = \u0026#34;rg-roz-avd-01\u0026#34; $subscription = (Get-AzContext).Subscription $rtName = \u0026#34;rt-avd\u0026#34; $routeTableUrl = \u0026#34;https://management.azure.com/subscriptions/{0}/resourceGroups/{1}/providers/Microsoft.Network/routeTables/{2}?api-version=2021-08-01\u0026#34; -f $subscription, $resourceGroup, $rtName $rtBody = @{ location = \u0026#34;WestEurope\u0026#34; properties = @{ routes = @( @{ name = \u0026#34;AVD-0-Route\u0026#34; properties = @{ nextHopType = \u0026#34;VirtualAppliance\u0026#34; nextHopIpAddress = $firewall.properties.ipConfigurations.properties.privateIPAddress addressPrefix = 0.0.0.0/0 } } ) } } | ConvertTo-Json -Depth 5 $rtParameters = @{ uri = $routeTableUrl method = \u0026#34;PUT\u0026#34; body = $rtBody headers = $authHeader } $routeTable = Invoke-RestMethod @rtParameters $routeTable In the last step, we assign the routing table to the AVD subnet. To associate the routing table with a network subnet I use the REST API code below. This time we go back to the subnet REST API endpoint.\nFor more information about creating route tables with REST API, check the documentation.\nAssign route table to subnet At last, we create the routing table and associate it with the AVD subnet. I use the $rt variable and the $network variable again. In the body, I provide the routing table and the address prefix (is mandatory)\n$avdSubnetUrl = \u0026#34;https://management.azure.com/{0}?api-version=2021-08-01\u0026#34; -f $network[0].subnetInfo.id $resourceGroup = \u0026#34;rg-roz-avd-01\u0026#34; $subscription = (Get-AzContext).Subscription $subnetBody = @{ properties = @{ addressPrefix = $network[0].subnetInfo.properties.addressPrefix routeTable = @{ id = $routeTable.id } } } | ConvertTo-Json $subnetParameters = @{ uri = $avdSubnetUrl method = \u0026#34;PUT\u0026#34; body = $subnetBody headers = $authHeader } $avdSubnet = Invoke-RestMethod @subnetParameters $avdSubnet For updating subnets with REST API check this document.\nSummary Thank you for reading my blog about configuring a static WAN IP for AVD in an automated way.\nThank you for reading my blog route avd traffic through static wan ip with azure firewall automated. I hope you got a bit inspired. Enjoy your day and happy automating üëã ","date":"November 4, 2022","image":"http://localhost:1313/route-avd-traffic-static-wan-ip-with-azure-firewall-automated/traffic-above.png","permalink":"/route-avd-traffic-static-wan-ip-with-azure-firewall-automated/","title":"Route AVD traffic through static WAN IP with Azure Firewall automated"},{"categories":["Azure AD","Graph API"],"contents":"This blog post is a follow-up to my post about using the hidden API in automation. In that post, I explained how to authenticate to the https://main.iam.ad.ext.azure.com/api/ endpoint and, how to get information from it. If you haven\u0026rsquo;t already, I would suggest reading that post first. I used Azure Functions to run the automation tasks.\nIn this blog post, I show how to get the same license information using Logic Apps. I will skip the authentication, Key Vault setup and store the initial refresh token parts.\nTable Of Contents Create Logic App Assign the correct role to the system identity Create Logic App steps automated Logic App explained Trigger (HTTP) Trigger (Recurrence) Actions Summary Create Logic App At first, we need a Logic App. After the Logic App is created the Key Vault Secrets Officer role is assigned to the system identity. To assign the correct role please check the part assign the correct roles from my other blog post.\nAfter authentication, I create an empty Logic App workflow with the code below.\n$workFlowName = \u0026#34;getLicenses\u0026#34; $uri = \u0026#34;{0}/subscriptions/{1}/resourceGroups/membeermonitor_group/providers/Microsoft.Logic/workflows/{2}?api-version=2016-06-01\u0026#34; -f $mainUrl, $subscriptionId, $workFlowName $logicAppWorkflowBody = @{ location = \u0026#34;westeurope\u0026#34; properties = @{ definition = @{ \u0026#34;`$schema\u0026#34; = \u0026#34;https://schema.management.azure.com/providers/Microsoft.Logic/schemas/2016-06-01/workflowdefinition.json\u0026#34; } } identity = @{ type = \u0026#34;SystemAssigned\u0026#34; } } | ConvertTo-Json -Depth 5 $logicApp = Invoke-RestMethod -Headers $headers -Method PUT -uri $uri -Body $logicAppWorkflowBody $logicApp The reason why I created an empty Logic App first is because of the managed identity ID. This ID is needed to give the identity permissions to the Azure Key Vault.\nAssign the correct role to the system identity To assign the correct role for the Azure Key Vault usage, I will suggest reading the assign role part of my previous blog.\nReplace the \u0026quot;system identity ID\u0026quot; in the code with $logicApp.id from the deployment above.\nCreate Logic App steps automated In this part, the different Logic App steps are created in an automated way. The steps are defined under the properties in the definition object. The definition (that is the Logic App) consists of parameters, triggers, actions and outputs. The Logic App now has only a trigger (manual) and actions.\nI use the create-block from above and appended the trigger and actions in the body. The $uri variable is the same as above. With the code below I only update the body.\n$logicAppContentBody = @{ location = \u0026#34;westeurope\u0026#34; properties = @{ definition = @{ \u0026#34;`$schema\u0026#34; = \u0026#34;https://schema.management.azure.com/providers/Microsoft.Logic/schemas/2016-06-01/workflowdefinition.json\u0026#34; triggers = @{ manual = @{ type = \u0026#34;Request\u0026#34; kind = \u0026#34;HTTP\u0026#34; } } actions = $jsonBody | ConvertFrom-Json -Depth 99 } } } | ConvertTo-Json -Depth 99 $logicAppContent = Invoke-RestMethod -Headers $headers -Method PUT -uri $uri -Body $logicAppContentBody $logicAppContent After the request, the Logic App has a trigger and all actions. The trigger is created in the request body above. The actions are filled from a JSON file I created. The file is stored on my GitHub. If you want to use the file\u0026rsquo;s content directly in the request, change the PowerShell variables $($kvRefeshTokenSecret) and $($tenantId) in the file. In that case, skip the step below.\nIf you want to use the current PowerShell variables then do the following:\nCopy the JSON file content between de @ lines and save the content into the variable.\n$kvRefeshTokenSecret = \u0026#34;https://[keyVaultName].vault.azure.net/secrets/[secretName]?api-version=7.3\u0026#34; $tenantId = \u0026#34;TenantID\u0026#34; $jsonBody = @\u0026#34; ---- JSON CONTENT FROM FILE ---- \u0026#34;@ Logic App explained We have created a Logic App automated with PowerShell to grab license information using the Azure hidden API. Let me explain in short how the Logic App works.\nTrigger (HTTP) The HTTP trigger is created with this part of the code. The HTTP trigger is used to start the Logic APP.\ntriggers = @{ manual = @{ type = \u0026#34;Request\u0026#34; kind = \u0026#34;HTTP\u0026#34; } } Trigger (Recurrence) If you want to run the Logic App at scheduled times, then use the recurrence trigger. Run the code below to create a recurrence trigger instead of a HTTP trigger.\ntrigger = @{ Recurrence = @{ recurrence = @{ frequency = \u0026#34;Week\u0026#34; interval = 1 }, evaluatedRecurrence = @{ frequency = \u0026#34;Week\u0026#34; interval = 1 }, type = \u0026#34;Recurrence\u0026#34; } } Actions The actions are straightforward. In the first block, the variables are initialized for later use. In the next step, the refresh token secret is requested from the Key Vault. In the block, I use the RefreshTokenLocationUrl variable. Also, I added the authentication parameter to use a system-assigned managed identity to authenticate. In this case, I use the system-managed identity of the Logic App itself. For the audience I use https://vault.azure.net. This is the API endpoint for Azure Key Vaults. The response is in a JSON body that is parsed to objects. I used the response content as a sample to create the schema. The value from the parse action is the refresh token content that is used in the login request. The tenantId is the variable from above. After login, a new refresh token is generated. I store the new token back in the Azure Key Vault based on the RefreshTokenLocation URL and the PUT method. Also this time I use the system-assigned managed identity.\nFrom the ParseJsonFromLogin action, I used the access token object to create an authorization header. The two x-ms-*** objects MUST exist in the header for using the main.iam.ad.ext.azure.com header. The GUIDs are just random created id\u0026rsquo;s.\nIn the URI https://main.iam.ad.ext.azure.com/api/AccountSkus?backfillTenants=true I request all available licenses in my tenant with nicely created display names. Summary I want to thank you for reading my post about how to use the hidden https://main.iam.ad.ext.azure.com Azure API with Logic Apps, refresh tokens, and the Azure Key Vault. I showed how to create all components, including the Logic App, are created in an automated way.\nThank you for reading my blog use internal main.iam.ad api with logic apps. I hope you got a bit inspired. Enjoy your day and happy automating üëã ","date":"October 24, 2022","image":"http://localhost:1313/use-internal-azure-api-with-logic-apps/logic-app-design-thumbnail.png","permalink":"/use-internal-azure-api-with-logic-apps/","title":"Use internal main.iam.ad API with Logic Apps"},{"categories":["Azure AD","Graph API"],"contents":"We all know the Graph API, graph.microsoft.com, and the Azure management API, management.azure.com. Both APIs are used very often and lots of automation tasks depend on those APIs. Also, most PowerShell modules rely on them and, the Graph and Azure management APIs are well documented.\nBut, there are situations where those APIs are not enough. (see my blog about configuring authentication strength automated). When looking in the Azure portal a lot of information is present and interesting to monitor or create reports. Or, to create resources. But when mapping the browser over the well-documented APIs you will notice not every item is available in the API.\nSo, how do we get ALL information or, how do we automate if there is none of the API endpoints available?\nIf it\u0026rsquo;s in the browser, you can get it out of it.\nTable Of Contents The Azure steam machine How to use the hidden internal Azure API? Authenticate to main.iam.ad.ext.azure.com How to use main.iam.ad.ext.azure.com in automation? Authentication process Log in with the refresh token Store refresh token in Azure Key Vault Create automation task Create and assign the system identity to Key Vaults Secrets Officer Get and set the refresh token from Key Vault Request main.iam.ad hiddden API Summary The Azure steam machine The answer is in the Azure basement where a steam machine is running called https://main.iam.ad.ext.azure.com/api/. This API isn\u0026rsquo;t documented but is very useful. A lot of Azure actions still depend on this API and it holds useful information. Use the developer panel in a browser while walking through the Azure portal and you will notice.\nHow to use the hidden internal Azure API? Using the https://main.iam.ad.ext.azure.com/api/ endpoint differs from the regular APIs. The biggest difference is authentication. The API does NOT support the use of a service principal and a client_credentials login method like the Graph authentication example below.\n$body = @{ grant_Type = \u0026#34;client_credentials\u0026#34; scope = \u0026#34;https://graph.microsoft.com/.default\u0026#34; client_Id = \u0026#34;appId\u0026#34; client_Secret = \u0026#34;appSecret\u0026#34; } $connectParams = @{ uri = \u0026#34;https://login.microsoftonline.com/{0}/oauth2/v2.0/token\u0026#34; -f \u0026#34;tenantId\u0026#34; method = \u0026#34;POST\u0026#34; body = $body } $connect = Invoke-RestMethod @connectParams $authHeader = @{ \u0026#39;Content-Type\u0026#39; = \u0026#39;application/json\u0026#39; Authorization = \u0026#39;Bearer \u0026#39; + $connect.access_token } Authenticate to main.iam.ad.ext.azure.com To use the hidden API you MUST log in interactively as a user for the first time. After login and user_impersonation you receive a refresh token. The permissions are based on the user\u0026rsquo;s role. The refresh token can be used to reauthenticate later interactively without providing credentials. Make sure you put the token in a safe place!! (Azure KeyVault for example).\nA refresh token has a 90-day lifetime. If the token is used it refreshes (as the name says) to a new token that needs to be stored again.\nHow to use main.iam.ad.ext.azure.com in automation? In this part, I will show the workflow on how to use the access_token and update the refresh tokens in automation tasks. The main components are the script (from above) and, an Azure Key Vault. The idea is to log in as a user for the first time and then write the refresh token to the Azure Key Vault.\nAuthentication process The first step is running the initial authentication process. The process needs to run once to get a refresh token. You also get an access token but is not needed in this step.\nInspired by Jos Lieben`s script, I created a script that creates an access token and the refresh token based on a device login and, writes the refresh token back to the Key Vault. The script is stored on my GitHub. https://github.com/srozemuller/Identity\nTo log in interactively as a user, I use the code below.\n$clientId= \u0026#34;1950a258-227b-4e31-a9cf-717495945fc2\u0026#34; # This is de Microsoft Azure Powershell application $tenantId = \u0026#34;tenantId\u0026#34; $resource = \u0026#34;https://main.iam.ad.ext.azure.com/\u0026#34; # Send the request to receive a device authentication URL $codeRequest = Invoke-RestMethod -Method POST -UseBasicParsing -Uri \u0026#34;https://login.microsoftonline.com/$tenantId/oauth2/devicecode\u0026#34; -Body \u0026#34;resource=$resource\u0026amp;client_id=$clientId\u0026#34; Write-Output \u0026#34;`n$($codeRequest.message)\u0026#34; When running the code, on the screen a message returns like below. The response is returned to the $codeRequest variable.\nFollow the link and the wizard in the browser. Select the correct user, and fill in the code from the screen.\nNext, run the code below where the response device code is used to log in and receive a refresh token. The code waits till the response is received. Thereafter, the refresh token is stored in the $refreshToken variable.\n# Create the body for the token request, where the device code from the previous request will be used in the call $tokenBody = @{ grant_type = \u0026#34;urn:ietf:params:oauth:grant-type:device_code\u0026#34; code = $codeRequest.device_code client_id = $clientId } # Get OAuth Token while ([string]::IsNullOrEmpty($tokenRequest.access_token)) { $tokenRequest = try { Invoke-RestMethod -Method POST -Uri \u0026#34;https://login.microsoftonline.com/$tenantId/oauth2/token\u0026#34; -Body $tokenBody } catch { $errorMessage = $_.ErrorDetails.Message | ConvertFrom-Json # If not waiting for auth, throw error if ($errorMessage.error -ne \u0026#34;authorization_pending\u0026#34;) { throw \u0026#34;Authorization is pending.\u0026#34; } } } # Printing the relevant information for tracability of the token and code Write-Output $($tokenRequest | Select-Object -Property token_type, scope, resource, access_token, refresh_token, id_token) $refreshToken = $tokenRequest.refresh_token Considering the response the access_token and refresh_token are important. The access_token is used to get access. The \u0026rsquo;normal\u0026rsquo; token as we know in the Graph and Azure management APIs uses the client_credentials login method.\nThe refresh_token is important in automation tasks. This is the token that must be used to log in without a user prompt and to receive an access_token.\nLog in with the refresh token To log in with the refresh token I request the login.windows.net page and provide the refresh token. With the response, I create a header with an access_token. I use the $tenant from above. The $clientId is the Microsoft Azure PowerShell id that was used in the initial script. In the response also the new refresh token is provided. Use the code above to set the new refresh token in the key vault for later use.\n$response = (Invoke-RestMethod \u0026#34;https://login.windows.net/$tenantId/oauth2/token\u0026#34; -Method POST -Body \u0026#34;resource=74658136-14ec-4630-ad9b-26e160ff0fc6\u0026amp;grant_type=refresh_token\u0026amp;refresh_token=$refreshToken\u0026amp;client_id=$clientId\u0026amp;scope=openid\u0026#34; -ErrorAction Stop) $resourceToken = $response.access_token $headers = @{ \u0026#34;Authorization\u0026#34; = \u0026#34;Bearer \u0026#34; + $resourceToken \u0026#34;Content-type\u0026#34; = \u0026#34;application/json\u0026#34; \u0026#34;X-Requested-With\u0026#34; = \u0026#34;XMLHttpRequest\u0026#34; \u0026#34;x-ms-client-request-id\u0026#34; = [guid]::NewGuid() \u0026#34;x-ms-correlation-id\u0026#34; = [guid]::NewGuid() } Store refresh token in Azure Key Vault To store the refresh tokens for later use, I create an Azure Key Vault and write back the refresh token.\nIn the initial step, I copied the refresh token manual into the secret.\nAs mentioned, the refresh token is the user\u0026rsquo;s password.\nThink twice about which user to use for impersonation. If you just need information, use a user with a READER role. That said, the Azure Key Vault becomes an important component. Make sure to restrict access as much as possible.\nI created an Azure Key Vault with the code below. Before I did, I logged in with Connect-AzAccount in PowerShell and created a header.\nThe full script is on my GitHub.\nI enabled public network access. This is because otherwise, Azure Functions can not connect to the key vault. Azure Functions is not marked as a trusted Azure Service. Also, the Azure Functions endpoints have a wide range of IP addresses which is hard to enable all IPs in the key vault firewall. I enabled the RBAC authorization instead of access policies. In later steps, a system identity is the Key Vault Contributor role.\n$vaultName = \u0026#34;kv-hidden-api\u0026#34; $uri = \u0026#34;{0}/subscriptions/{1}/resourceGroups/{2}/providers/Microsoft.KeyVault/vaults/{3}?api-version=2021-10-01\u0026#34; -f $mainUrl, $subscriptionId, $resourceGroupName, $vaultName $kvBody = @{ location = \u0026#34;WestEurope\u0026#34; properties = @{ enablePurgeProtection = $true enableRbacAuthorization = $true publicNetworkAccess = \u0026#34;Enabled\u0026#34; tenantId = $tenantId sku = @{ family = \u0026#34;A\u0026#34; name = \u0026#34;standard\u0026#34; } } } | ConvertTo-Json -Depth 5 $keyVaultParameters = @{ uri = $uri method = $method headers = $headers body = $kvBody } $kv = Invoke-RestMethod @keyVaultParameters $kv Create automation task There are several ways to run tasks in Azure. Think about Azure Functions, Azure Automation and Logic Apps. In this part, I create an Azure Function that sends API requests to get readable license information.\nCreate and assign the system identity to Key Vaults Secrets Officer At first, we need a resource with a system identity. The system identity is assigned to the Key Vaults Secrets Officer role. Due to the length of the blog post, I skip the resource creation. I used the code below to assign the resource\u0026rsquo;s system identity to the Key Vault Secrets Officer role (id: b86a8fe4-44ce-4948-aee5-eccb2c155cd7). See https://learn.microsoft.com/en-us/azure/role-based-access-control/built-in-roles#key-vault-contributor\nThe scope is set to the key vault only. I used the $kv.id from the earlier key vault creation.\n$roleGuid = (New-Guid).Guid $roleUri = \u0026#34;{0}/{1}/providers/Microsoft.Authorization/roleAssignments/{2}?api-version=2018-01-01-preview\u0026#34; -f $mainUrl, $kv.id, $roleGuid $roleBody = @{ properties = @{ roleDefinitionId = \u0026#34;/subscriptions/{0}/providers/Microsoft.Authorization/roleDefinitions/b86a8fe4-44ce-4948-aee5-eccb2c155cd7\u0026#34; -f $subscriptionId ## b86a8fe4... guid is the buildin role ID Key Vault Secrets Officer principalId = \u0026#34;system identity ID\u0026#34; ## This is the system identity id or the resource (I used the .id object from the ) } } | ConvertTo-Json -Depth 5 $roleParameters = @{ uri = $roleUri method = $method headers = $headers body = $roleBody } $role = Invoke-RestMethod @roleParameters $role For more information about Key Vaults roles please check: https://learn.microsoft.com/en-us/azure/key-vault/general/rbac-guide?tabs=azure-cli#azure-built-in-roles-for-key-vault-data-plane-operations\nThe principal ID in the body properties is the resource\u0026rsquo;s system identity ID. After the assignment, check the Azure role assignments Get and set the refresh token from Key Vault Now the resource has proper permissions to the Key Vault we can update secrets. In the example below I created a PowerShell function Within the Azure Functions resource. At first, the function is logging in as a system identity and creating an authentication header for requesting the Key Vault\n$azureAccount = Connect-AzAccount -Identity $accessToken = Get-AzAccessToken -ResourceUrl $resource -DefaultProfile $azureAccount $authHeader = @{ \u0026#39;Content-Type\u0026#39; = \u0026#39;application/json\u0026#39; Authorization = \u0026#39;Bearer {0}\u0026#39; -f $accessToken.Token } To get and set the secret I use the code below. I set the audience to vault.azure.net. Which was graph.microsoft.com earlier for example. To get the value I use the GET method, in the case of setting a new value you need the PUT method.\n$vaultUrl = \u0026#34;https://vault.azure.net\u0026#34; $getMethod = \u0026#34;GET\u0026#34; $token = [Microsoft.Azure.Commands.Common.Authentication.AzureSession]::Instance.AuthenticationFactory.Authenticate($context.Account, $context.Environment, $context.Tenant.Id.ToString(), $null, [Microsoft.Azure.Commands.Common.Authentication.ShowDialog]::Never, $null, $vaultUrl).AccessToken $headers = @{ \u0026#39;Content-Type\u0026#39; = \u0026#39;application/json\u0026#39; Authorization = \u0026#39;Bearer \u0026#39; + $token } $kvSecretName = \u0026#34;RefreshToken\u0026#34; $secretUri = \u0026#34;{0}/secrets/{1}?api-version=7.3\u0026#34; -f $kv.properties.vaulturi, $kvSecretName $secretBody = @{ value = $resourceToken } | ConvertTo-Json -Depth 5 $secretParameters = @{ uri = $secretUri method = $getMethod headers = $headers body = $secretBody } $secret = Invoke-RestMethod @secretParameters $secret.value For more information about how to set keys, check: https://learn.microsoft.com/en-us/rest/api/keyvault/secrets/set-secret\nRequest main.iam.ad hiddden API With the header, we can request https://main.iam.ad.ext.azure.com. In the example below, I request all assigned account SKU\u0026rsquo;s. These are assigned licenses with also a readable display name. Something that isn\u0026rsquo;t available in the Graph API.\n$uri = \u0026#34;https://main.iam.ad.ext.azure.com/api/AccountSkus?backfillTenants=true\u0026#34; Invoke-RestMethod -Method get -uri $uri -header $headers The full Azure function is stored here: https://github.com/srozemuller/Identity/tree/main/LicenseInformation\nThe script to deploy a resource group, key vault, and set the correct RBAC role is stored here: https://github.com/srozemuller/Identity/blob/main/HiddenAPI/create-hidden-api-environment.ps1\nSummary In the blog post, I showed how to use the hidden https://main.iam.ad.ext.azure.com in automation with the use of Azure Functions and PowerShell. The main \u0026rsquo;trick\u0026rsquo; is to log in with the refresh token to get an access token returned to log in. The new refresh token is saved in the Key Vault.\nAs mentioned earlier, the refresh token is the new password. Keep it safe and make sure to use an account with as less permissions as possible. Due to the length of the blog post, I\u0026rsquo;m planning a new post on how to use this method in Logic Apps.\nThank you for reading my blog use internal main.iam.ad api in automation. I hope you got a bit inspired. Enjoy your day and happy automating üëã ","date":"October 17, 2022","image":"http://localhost:1313/use-internal-azure-api-in-automation/machine.png","permalink":"/use-internal-azure-api-in-automation/","title":"Use internal main.iam.ad API in automation"},{"categories":["Azure AD","Security"],"contents":"Conditional access is an indispensable configuration setting in Azure AD. Conditional Access policies are in basic if-then statements, if a user wants to access a resource, then they must complete an action. Example: A payroll manager wants to access the payroll application and is required to do multi-factor authentication to access it. Or, it requires everyone to use multifactor authentication. Multifactor authentication can be a password in combination with an SMS or the use of the Microsoft Authenticator app for example. In the past, a user can choose to use SMS or the authenticator or what the admins provide you to use.\nNow it is also possible to configure a setting called Require authentication strength. This new setting is in preview now. In this blog post, I show how to configure and monitor this setting in a conditional access policy.\nTable Of Contents Require Authentication Strength Conditional Access policy Deploy conditional access policy automated Authenticate to main.iam.ad.ext.azure.com Request conditional access policies Authentication strength values Required authentication strength ID`s Create a conditional access policy with authentication strength Monitor conditional access policy required authentication strength Final words Require Authentication Strength Conditional Access policy The required authentication strength setting can be used to force users to use a specific set of authentication methods. For example, all admins MUST use a biometric device and end-users must use the Microsoft Authenticator. In this case, you have to create two conditional access policies and assign them to the correct user group. For the admin group, you have to select the Phishing-resistant multifactor authentication. For all end-users select the Passwordless multifactor authentication.\nDeploy conditional access policy automated To deploy a conditional access policy automated with the new authentication strength option, the process is a bit different from the \u0026rsquo;normal\u0026rsquo; Graph API deployments. After screening what is happening in the background I noticed there is no Graph API endpoint available (yet) for conditional access policies and access controls.\nTo deploy a conditional access policy automated with controls we need the https://main.iam.ad.ext.azure.com/api/ URL.\nThis is because the Graph API endpoint has no control object available. (https://learn.microsoft.com/en-us/graph/api/conditionalaccesspolicy-get?view=graph-rest-beta\u0026amp;tabs=http)\nIt has the grantControls object but the authentication strength is missing.\nAuthenticate to main.iam.ad.ext.azure.com To use this hidden API you MUST log in with a user for the first time. Based on the login and user_impersonation make sure you receive a refresh token. This token can be used to reauthenticate later interactively without providing credentials.\nMake sure you put the token in a safe place!! (Azure KeyVault for example).\nA refresh token has a 90-day lifetime. If the token is used it refreshes (as the name says) to a new token that needs to be stored again.\nInspired by Jos Lieben`s script, I created a script that creates an access token and the refresh token based on a device login. The script is stored on my GitHub. https://github.com/srozemuller/Identity\nRequest conditional access policies The Graph API has an endpoint for conditional access policies but I use the main.iam.ad.ext.azure.com API for this task as well because I only have to authenticate once. To get the conditional access policies I used the API URL below.\n$mainIamUrl = \u0026#34;https://main.iam.ad.ext.azure.com/api/Policies/Policies?top=10\u0026amp;nextLink=null\u0026amp;appId=\u0026amp;includeBaseline=true\u0026#34; (Invoke-RestMethod -Uri $mainIamUrl -Method GET -Headers $Headers ).items The $headers variable is filled by the script.\nWhen having the correct policy ID use the API URL below.\n$resource = \u0026#34;https://main.iam.ad.ext.azure.com\u0026#34; $graphURL = \u0026#34;{0}/api/Policies/5a766b5b-fbcf-4480-bb6f-8bee83b73c83\u0026#34; -f $resource $policy = Invoke-RestMethod -Uri $graphURL -Method GET -Headers $Headers $policy.controls Authentication strength values The values returned are GUID`s. Every GUID represents a setting. Although there is no option to get the required authentication strength from the Graph API, there is a resource type called authenticationStrength. https://learn.microsoft.com/en-us/graph/api/resources/authenticationstrength?view=graph-rest-beta\nI requested the Graph API endpoint https://graph.microsoft.com/beta/identity/conditionalAccess/authenticationStrengths/policies to get all available values. The $authHeader variable is filled based on\n$url = \u0026#34;https://graph.microsoft.com/beta/identity/conditionalAccess/authenticationStrengths/policies\u0026#34; $request = Invoke-RestMethod -Uri $url -Method Get -Headers $authHeader $request.value | Select-Object id, displayName Required authentication strength ID`s id displayname description 00000000-0000-0000-0000-000000000002 Multi-factor authentication Combinations of methods that satisfy strong authentication, such as a password + SMS 00000000-0000-0000-0000-000000000003 Passwordless MFA Passwordless methods that satisfy strong authentication, such as Passwordless sign-in with the Microsoft Authenticator 00000000-0000-0000-0000-000000000004 Phishing resistant MFA Phishing-resistant, Passwordless methods for the strongest authentication, such as a FIDO2 security key Create a conditional access policy with authentication strength Let\u0026rsquo;s say we have a situation where administrators MUST use a biometric device to authenticate. In that case, I create a conditional access policy with authentication strength with Phishing resistant MFA (ID: 00000000-0000-0000-0000-000000000004).\n$resource = \u0026#34;https://main.iam.ad.ext.azure.com\u0026#34; $mainIamURL = \u0026#34;{0}/api/Policies/\u0026#34; -f $resource $body = Get-Content ./Identity/ConditionalAccess/CA004-Require-multifactor-authentication-for-all-admins.json $caPolicy = Invoke-RestMethod -Uri $mainIamURL -Method POST -Headers $Headers -Body $body $caPolicy The JSON file is stored in my GitHub repository. In the policy, all Azure Admin Roles are embedded. Make sure you also add an identity (break-glass account) to the exclude-part. This is to avoid lockout yourself.\nhttps://github.com/srozemuller/Identity/tree/main/ConditionalAccess\nMonitor conditional access policy required authentication strength I would suggest using code like PowerShell to monitor resources behind the https://main.iam.ad.ext.azure.com endpoint. This is easier than using Logic Apps for example because of the authentication flow that does not support service principals. Azure Functions could be a good option for monitoring.\nA good monitoring option could be monitoring ADMIN policies that do not have a biometric strength configured. In that case, use the GET request to get all the policies and their information.\nMake sure you have the refresh token stored in the Azure Functions environment using the app service configuration. Point the configuration item to an Azure Keyvault secret. Every time the refresh token has been renewed, you should update the secret.\nHow to configure a monitoring environment using the hidden Azure API, will be discussed later.\nFinal words This feature is still in PREVIEW. It is good to test with it but make sure you have a breakglass account.\nAnother thing, make sure you have enabled the authentication methods as well. For example, when using phishing-resistant (FIDO2), make sure you have enabled the FIDO2 authentication method.\nThe conditional access policy does not check this for you!\n","date":"October 6, 2022","image":"http://localhost:1313/deploy-monitor-conditional-access-require-authentication-strength/fingerprint-thumbnail.png","permalink":"/deploy-monitor-conditional-access-require-authentication-strength/","title":"Deploy and monitor conditional access authentication strength automated"},{"categories":["Microsoft Intune","Monitoring"],"contents":"Intune and the rest of Microsoft Intune are growing. To keep your environment nice, clean and secure it is a good idea to monitor your MEM environment. In this blog, I explain how to monitor Intune based on Graph API and PowerShell with Azure Functions.\nTable Of Contents Monitor Intune with Diagnostics Settings or Graph API Why Azure Functions Source control Security Control Efficiency Azure Functions architecture Set Graph API permissions on Managed Identity PowerShell Functions Authentication using Managed Identity in Azure Functions Request the Graph API Send alerts Monitor Intune with Diagnostics Settings or Graph API With growing environments in mind, I enabled the diagnostics settings that send the logs to Log Analytics. When digging into the logs I noticed a lot of interesting information is not available. Think about a security profile that is out of date or unassigned baselines. Also, some form of notification is missing. Luckily, we have the Graph API that has lots of valuable information to dig into.\nGraph API is a treasure chest that has lots of valuable information to dig into.\nWhy Azure Functions More ways are leading to Rome so why do I use Azure Functions instead of Logic App for example? The answer is simple, CONTROL. I do have the infrastructure-as-a-code philosophy and try to have all components in code as much as possible. But there is more.\nSource control The main reason for using Azure Functions is source control. With source control I do have full control about what is changing, by who and when the change will be live in the functions.\nConfigure source control under the deployment center at the Azure Function level. By enabling source control editing code in the browser has been disabled. You MUST change the code in the repository. Source control does not support subfolders. To make functions work put a run.ps1 (which is the PowerShell script) and function.json (which is the function configuration file) in this folder. The name of the folder is the name of the function. In the end, the whole repository is uploaded to the Azure Function but only the folders with a correct run.ps1 and function.json will work as functions.\nCheck the repository content for more information: https://github.com/srozemuller/Monitoring/tree/main/Security-BaselineVersionCheck\nSecurity Control Another topic is security. A little forward to the next paragraph, Azure Functions can use managed identities (system or user). During creation, I consciously choose to create a System Assigned identity. This makes the identity only available for this Azure Resource. Every inside function is using this identity. By knowing the inside functions, I also know what is using the identity and when. Using a User identity means that other resources can use that identity with a change overcommitting permissions.\nEfficiency DRY, aka, Don\u0026rsquo;t Repeat Yourself. Using Azure Functions means having freedom and control to create functions based on different coding languages like Python, .NET or PowerShell. In this case, I use PowerShell which can use functions or import modules.\nConcerning a non-code solution (which is fine of course!) every purpose is a full configuration. If something general changes, you have to change it in all your solutions. As mentioned, the whole repository is synced to Azure Functions. Check the module file in my repository https://github.com/srozemuller/Monitoring/tree/main/Modules. Use import-module ./functions-file.psm1 to import a module file in your scripts.\ntry { import-module .\\Modules\\mem-monitor-functions.psm1 } catch { Write-Error \u0026#34;Functions module not found!\u0026#34; exit; } Azure Functions architecture At the top, we have the Azure Function with a System Assigned Managed Identity. Considering a User Assigned this identity is managed by the Azure AD. After an Azure Function removal, the managed identity is also removed. A User Assigned Identity needs to be removed by the admin.\nWhy not use a User Assigned managed identity?\nThis is because I think the system must be running sovereign. And, from a security perspective, a System Assigned identity is safer than a shared User identity. This is because the identity is assigned to the resource only and I know exactly what is using the System Identity.\nSet Graph API permissions on Managed Identity A managed identity is an enterprise application in the background just like an app registration. Configuring API permissions under an app registration is simple by going to the API Permissions page in the specific app.\nIn the case of a managed identity, there is no app registration and no API permission page. To configure API permissions for a managed identity we need to code. In the example below, I also use Graph API. At first, I generate a token to use in the header.\n$tokenInfo = Get-AzAccessToken -ResourceUrl \u0026#34;https://graph.microsoft.com\u0026#34; # \u0026#34;https://graph.microsoft.com\u0026#34; $header = @{ Authorization = \u0026#34;{0} {1}\u0026#34; -f $tokenInfo.Type, $tokenInfo.Token \u0026#39;Content-Type\u0026#39; = \u0026#34;application/json\u0026#34; } I search for the Graph API service principal for the Graph API permissions. From the result, I search for the DeviceManagementManagedDevices.Read.All permission. To get the correct permissions I suggest reading this part: https://www.rozemuller.com/deploy-intune-settings-catalog-automated-from-scratch-with-graph-api/#how-do-i-get-settings-before-automation\n$graphAppId = \u0026#34;00000003-0000-0000-c000-000000000000\u0026#34; $graphAppUrl = \u0026#34;https://graph.microsoft.com/beta/servicePrincipals?`$filter=appId eq \u0026#39;$GraphAppId\u0026#39;\u0026#34; $getGraphPermissions = Invoke-RestMethod -uri $graphAppUrl -Method GET -Headers $header | Select-Object value $PermissionName = \u0026#34;DeviceManagementManagedDevices.Read.All\u0026#34; $appRoleId = $getGraphPermissions.value.appRoles | Where-Object {$_.value -eq $PermissionName -and $_.AllowedMemberTypes -contains \u0026#34;Application\u0026#34;} I search for the created System Assigned managed identity. This is de created Azure Function name. Thereafter the result is used to assign app roles based on the Graph API Service Principal.\n$url = \u0026#34;https://graph.microsoft.com/beta/servicePrincipals?`$filter=displayname eq \u0026#39;membeermonitor\u0026#39;\u0026#34; $servicePrincipal = Invoke-RestMethod -uri $url -Method GET -Headers $header | Select-Object value $spUrl = \u0026#34;https://graph.microsoft.com/beta/servicePrincipals/{0}/appRoleAssignedTo\u0026#34; -f $getGraphPermissions.value.id $body = @{ principalId = $servicePrincipal.value.id resourceId = $getGraphPermissions.value.id appRoleId = $appRoleId.id } | ConvertTo-Json $assignPermissions = Invoke-RestMethod -uri $spUrl -Method POST -Headers $header -Body $body $assignPermissions Check this URL for a detailed overview about assign permissions to a managed identity. https://gotoguy.blog/2022/03/15/add-graph-application-permissions-to-managed-identity-using-graph-explorer/\nPowerShell Functions Within Azure Functions there are the (PowerShell) functions themselves. I created a few functions with a security purpose that executes different Graph API requests. In the basics, the functions are the same and divided into three parts: authentication, request, and send.\nAuthentication using Managed Identity in Azure Functions Authentication using the managed identity in Azure Functions is simple. Use the command Connect-AzAccount -Identity\nSee: https://learn.microsoft.com/en-us/powershell/module/az.accounts/connect-azaccount?view=azps-8.3.0#example-5-connect-using-a-managed-service-identity\nAfter creating a managed identity in the Azure Function environment the secret is available in the environment variables. The connect-azaccount -identity uses the System Assigned secret to login. A simple IF statement helps you to define if a function can use a managed identity or not.\nif ($env:MSI_SECRET) { Connect-AzAccount -Identity } Considering efficiency, I created a function called Get-AuthApiToken that is imported at the beginning.\ntry { import-module .\\Modules\\mem-monitor-functions.psm1 } catch { Write-Error \u0026#34;Functions module not found!\u0026#34; exit; } try { Get-AuthApiToken -resource $env:graphApiUrl } catch { Throw \u0026#34;No token received, $_\u0026#34; } The function file is stored in my GitHub repository.\nRequest the Graph API In the next block, the request is sent to a Graph API URL. The variable $results holds the results. In the URL below I request every template that has an advancedThreatProtectionSecurityBaseline, microsoftEdgeSecurityBaseline or, cloudPC type.\ntry { Write-Information \u0026#34;Searching for security baselines\u0026#34; $getUrl = \u0026#34;{0}/beta/deviceManagement/templates?$filter=(templateType%20eq%20\u0026#39;securityBaseline\u0026#39;)%20or%20(templateType%20eq%20\u0026#39;advancedThreatProtectionSecurityBaseline\u0026#39;)%20or%20(templateType%20eq%20\u0026#39;microsoftEdgeSecurityBaseline\u0026#39;)%20or%20(templateType%20eq%20\u0026#39;cloudPC\u0026#39;)\u0026#34; -f $env:graphApiUrl $results = Invoke-RestMethod -URI $getUrl -Method GET -Headers $authHeader } catch { Write-Error \u0026#34;Unable to request for security baselines, $_\u0026#34; } A message is sent to an admin if there are baselines out of date. To send messages I created a Send-AlertsToAdmin function.\nSend alerts In the last part, an alert is sent to an administrator. The example below shows how to send a simple message to MS Teams using a webhook. In the function I refer to $env:teamsUrl. This is an environment variable that is configured at the Azure Function level.\nSend-AlertToAdmin -Title \u0026#34;Security Baseline\u0026#34; -SubTitle \u0026#34;New version available\u0026#34; -Description \u0026#34;New version with date $(Get-Date)\u0026#34; How to configure a webhook for MS Teams check the documentation: https://learn.microsoft.com/en-us/microsoftteams/platform/webhooks-and-connectors/how-to/add-incoming-webhook\n","date":"September 28, 2022","image":"http://localhost:1313/monitor-intune-using-azure-functions-powershell-and-graph-api/monitor-graph-thumbnail.png","permalink":"/monitor-intune-using-azure-functions-powershell-and-graph-api/","title":"Monitor Intune using Azure Functions, PowerShell, Graph API and MS Teams"},{"categories":["Microsoft Intune","Graph API"],"contents":"I show in this blog how to create a device configuration profile based on the settings catalog in Intune. A while ago I created a profile with power settings and deployed the profile to all devices including a filter.\nIn this blog, I show how to grab settings from an existing policy and create a template from that.\nLet\u0026rsquo;s say this is also IAAC (Intune As A Code)\nTable Of Contents What is the Settings Catalog Why Graph API? Interact Intune with PowerShell and Graph API Graph API Methods Authenticate to Graph API Why use an application registration Settings catalog from an automation perspective How do I get settings before automation Save the settings into a JSON file Deploy profile based on settings catalog Assignment What is the Settings Catalog The Settings catalog in Microsoft Intune lists all the settings you can configure in one place. This feature simplifies how you create a policy, and how you see all the available settings. More settings are continually being added. If you prefer to configure settings at a granular level, similar to on-premises GPO, then the settings catalog is a natural transition.\nFor more information about the settings catalog please check https://docs.microsoft.com/en-us/mem/intune/configuration/settings-catalog\nWhy Graph API? There are a lot of PowerShell modules available for managing Intune. Think about Microsoft.Graph.Intune or IntuneWin32App. They all work fine but rely on other PowerShell modules.\nThe most important module is the old AzureAD and AzureAD.Preview PowerShell modules that are deprecated. Also, the AzureAD modules rely on an old .NET assembly for PowerShell 5.1. I use DevOps to automate Intune configuration deployments that use PowerShell core and having issues installing the extra modules or Windows-specific dll-files that I don\u0026rsquo;t have on MAC. In automation, I try to be less dependent as possible.\nAnother reason for using the Graph API instead of PowerShell modules is the use of parameters. PowerShell commands need parameters. REST API needs a body with content.\nThe advantage of using a body is all the configuration is in that body. The \u0026lsquo;body\u0026rsquo;-file is acting like a template. Having that body is also having a backup.\nThe last example is complexity. It sounds a bit weird but using the Graph API in PowerShell is way simpler. You only have to remember just one command instead of a bunch of commands for every configuration. The command to be used is Invoke-RestMethod. And, if you are not that familiar with PowerShell also other options like using Graph Explorer or Postman.\nIn the further steps I use PowerShell.\nInteract Intune with PowerShell and Graph API In the automation world, I try to be less dependent as possible. That means to me that I force myself to use out-of-the-box options as much as possible, with no extra module downloads etc. The most out-of-the-box option to interact with Intune is the Graph API. In the end, every module (and browsers) interacts over the Graph API.\nGraph API Methods There are several API methods available that tell how you interact with the Graph API.\nMethod Description GET Read data from a resource. POST Create a new resource, or perform an action. PATCH Update a resource with new values. PUT Replace a resource with a new one. DELETE Remove a resource. For more information about Graph API methods, check this URL: https://docs.microsoft.com/en-us/graph/use-the-api#http-methods\nAuthenticate to Graph API To deploy settings in Microsoft Intune we need to authenticate. I\u0026rsquo;ve configured an application registration in Azure AD and set the correct API permissions. To get the correct permissions for a specific action check the Graph API documentation.\nIn this case, I create a device management configuration policy. The documentation tells me I need the following permissions:\nDeviceManagementConfiguration.ReadWrite.All\n(Allows the app to read and write properties of Microsoft Intune-managed device configuration and device compliance policies and their assignment to groups, without a signed-in user.) https://docs.microsoft.com/en-us/graph/api/intune-deviceconfigv2-devicemanagementconfigurationpolicy-create?view=graph-rest-beta#prerequisites\nTo authenticate against the Graph API I used the PowerShell code below:\n$appId = \u0026#34;\u0026#34; $appSecret = \u0026#34;\u0026#34; $tenantId = \u0026#34;\u0026#34; $body = @{ grant_Type = \u0026#34;client_credentials\u0026#34; scope = \u0026#34;https://graph.microsoft.com/.default\u0026#34; client_Id = $appId client_Secret = $appSecret } $connectParams = @{ uri = \u0026#34;https://login.microsoftonline.com/{0}/oauth2/v2.0/token\u0026#34; -f $tenantId method = \u0026#34;POST\u0026#34; body = $body } $connect = Invoke-RestMethod @connectParams $authHeader = @{ \u0026#39;Content-Type\u0026#39; = \u0026#39;application/json\u0026#39; Authorization = \u0026#39;Bearer \u0026#39; +$connect.access_token } If you are not that familiar with REST API authentication then it also possible to use the PowerShell command Get-AzAccessToken. To use that command you need to connect to Azure first with Connect-AzAccount. I also connect with the application credentials. If you use these commands, install the Az.Accounts PowerShell module (it\u0026rsquo;s a dependency).\n$passwd = ConvertTo-SecureString $appSecret -AsPlainText -Force $pscredential = New-Object System.Management.Automation.PSCredential($appId, $passwd) Connect-AzAccount -ServicePrincipal -Credential $pscredential -Tenant $tenantId $token = Get-AzAccessToken -ResourceUrl \u0026#39;https://graph.microsoft.com\u0026#39; $authHeader = @{ \u0026#39;Content-Type\u0026#39; = \u0026#39;application/json\u0026#39; Authorization = \u0026#39;Bearer \u0026#39; + $token.Token } Why use an application registration The answer why using an application registration is because an application has a multi-purpose solution. An application can be used to log in from my device as it can be used in an automation task configured as a service connection in DevOps for example.\nYou should only use application registrations if the source is out of Azure, mostly during deployments. When using Azure resources like Logic Apps or Azure Functions you should consider the use of managed identities. For more information about managed identity please check: https://docs.microsoft.com/en-us/azure/app-service/overview-managed-identity?tabs=portal%2Chttp\nSettings catalog from an automation perspective Yes, we have settings, a lot :). But what I mean with parts is that there is more than just settings. Settings are a part of the total. Let\u0026rsquo;s take a look when we search for a configuration policy using the Graph API.\nSettings do have almost a not to guess name and value. Building a configuration profile from scratch directly in code is therefore not very easy. That is the reason I configure a profile in the portal first and convert it back to code afterward.\nIf you have to deploy a configuration profile just one time, I suggest also create a JSON export from the profile. In that case the file can be used as backup.\nHow do I get settings before automation A good place to start is at https://endpoint.microsoft.com and then press F12. On the right side, a new window comes up. Go to the network tab in the new window and look at what is filling out there after refreshing the page. In the long list search for results that look like an URL.\nLook into the headers tab and copy the representing request URL. Based on the URL in the screenshot, A good step is searching for deviceManagement/configurationPolicies graph api. No doubt you will find the correct Microsoft docs about how to use this Graph API URL.\nSave the settings into a JSON file I extracted the URL from the example above. https://graph.microsoft.com/beta/deviceManagement/configurationPolicies('be4ac8b6-2a48-470b-a6bb-8332b191a9b9')/settings?$expand=settingDefinitions\u0026amp;top=1000 When executing this URL I only got the settings where I need the policy itself as well. Because of that, I modified the URL to the one below. https://graph.microsoft.com/beta/deviceManagement/configurationPolicies('be4ac8b6-2a48-470b-a6bb-8332b191a9b9')?$expand=settings\nThis URL requests the configuration policy itself including the settings. Save the URL as a variable in PowerShell and execute the request with the Invoke-RestMethod PowerShell command.\n$url = \u0026#34;https://graph.microsoft.com/beta/deviceManagement/configurationPolicies(\u0026#39;be4ac8b6-2a48-470b-a6bb-8332b191a9b9\u0026#39;)?`$expand=settings\u0026#34; $method = \u0026#34;GET\u0026#34; $request = Invoke-RestMethod -Uri $url -Method $method -Headers $authHeader $request | ConvertTo-Json -Depth 99 | Out-File .\\configpolicy.json I use the GET method for reading the data and saving the data into a JSON file.\nDeploy profile based on settings catalog To deploy new configurations with the REST API I use the POST method. Imagine we have a new tenant and have to deploy a configuration profile. Based on the created configpolicy.json file I used almost the same code as above. Only we have to add more parameters.\nWhen using POST or PATCH we have to provide a body. The REST API accepts a JSON body. To tell the API we have a JSON body I declare a content type in the request with -ContentType 'application/json'\n$requestParameters = @{ uri = \u0026#34;https://graph.microsoft.com/beta/deviceManagement/configurationPolicies\u0026#34; method = \u0026#34;POST\u0026#34; body = Get-Content .\\configpolicy.json contentType = \u0026#39;application/json\u0026#39; headers = $authHeader } $request = Invoke-RestMethod @requestParameters $request The example of the configpolicy.json is stored in my GitHub.\nAssignment Assigning policies is the next step where more options are available. In this part of my earlier power-setting blog post, I show how to assign a policy to all devices based on filters.\nhttps://www.rozemuller.com/deploy-power-settings-automated-in-microsoft-endpoint-manager/#power-settings-assignment-with-filters\nIf you want to try the Graph API I would suggest start reading here: https://docs.microsoft.com/en-us/graph/auth/. It starts with authentication and continues the Graph API journey.\nThank you for reading my blog deploy intune settings catalog automated from scratch with graph api. I hope you got a bit inspired. Enjoy your day and happy automating üëã ","date":"September 11, 2022","image":"http://localhost:1313/deploy-intune-settings-catalog-automated-from-scratch-with-graph-api/catalog-thumbnail.png","permalink":"/deploy-intune-settings-catalog-automated-from-scratch-with-graph-api/","title":"Deploy Intune settings catalog automated from scratch with Graph API"},{"categories":["Static Web App","Azure"],"contents":"We all know WordPress. It is a good platform for running websites with almost every purpose. Since the start of this blog, I also used WordPress. It has a lot of templates and plug-ins available. For me running this blog on WordPress was good enough. Now a few years later it became more a frustration than a pleasure to keep my website up and running. In this blog, I explain why and how I changed my website into a static webpage.\nTable Of Contents Why change direction? Speed Security Plugins Updates Backups Efficiency Goodby dynamic, hello static What is a static website generator? Choose a generator How to migrate WordPress to Hugo Where to start Export WordPress content Build up your new static website Running Hugo local Configure your website Publish your website in public Create a GitHub repository Create Azure Static web app automated Github action explained Add a custom domain to Azure static web app My workflow Leftovers and considerations Google Analytics SEO and Hugo Comments Social Sharing Fixes Custom 404 page Azure Static Web App Why change direction? After almost two years I overlooked my website and noticed that around 98% is static. The only thing that changes are blog posts (updates or new) and comments.\nAlso, a big thing for me is the website‚Äôs speed. Besides speed some other topics become frustrating to me:\nUpdates ( from different perspectives) Security Plug-ins Backups Efficiency Speed WordPress is a client-server website which means that all data is stored on a server (database). Every (configuration) item is written in the database. Think about blog posts, images, settings, etc. Every request is a search in the database that takes time.\nWordPress isn‚Äôt that efficient when it comes to data management. Data is shattered in many tables and many files are placed in the host. For example, every image uploaded creates several resolutions to give a responsive feeling.\nAlso, it depends on how fast the hosting provider is.\nSecurity Another thing that matters is security. Client-server means you have to use credentials to log in at the website‚Äôs backend to create/edit content. With the come of MFA it became better but also WordPress has its own user to manage the website and write to the DB. WordPress is pretty vulnerable to SQL injections and needs to be updated almost every week.\nPlugins To keep my site as secure and fast as possible I use plug-ins. For every purpose I need a different plugin. In the end, I used more than 15 plug-ins to run my website in a way I like. Good to know is that the more plug-ins you have the more your website is slowing down. Adding more plugins is working oppositely.\nUpdates Mentioned a bit before are updates. At first we have WordPress itself that has an update almost every week. Then we have the plug-ins and also the WordPress theme. In the end, there is a long list with updates almost every day.\nBackups Not the most annoying thing but you need a WordPress backup. This is the database and files. Most hosting providers have the ability to schedule WordPress backups from their control panel. But it is an extra step that needs to be monitored as well.\nEfficiency In the end, maintaining the website is a job in itself. But there is more. I was looking for a faster way of working from a writing experience.\nOften I write about new features that depends on specific lines of code. During the time these line of code are changing. Think about specific configuration versions like the AVD desired stated configuration artifact. If these lines change I have to update these lines in every single blog and hopefully, I updated them all.\nThis isn\u0026rsquo;t very efficient. Also in every article I use components like a table of contents, a summary, and a goodbye saying.\nEvery time I have to create that myself (or use an extra plug-in(s) for using shortcodes or reusable blocks.\nAnother extra topic is the post format. A WordPress post consists of many chapters, sub-chapters, paragraphs, and code blocks. In WordPress, every block must be selected from a library. While writing costs a lot of time.\nSo for me enough reasons to change direction.\nGoodby dynamic, hello static As said, speed is my first topic to solve. Because client-server isn‚Äôt that fast I need to look into a non-server solution. Because of that dynamic platforms are out of sight. Luckily my site is actually as static as an old oak so that is not a problem üôÉ.\nSo I started exploring the world of static website generators.\nWhat is a static website generator? A lot of content is available on the web about this subject. Because of that, I skip the deep dive static website generator part.\nIn short, from Cloudflare:\nA static site generator is a tool that generates a full static HTML website based on raw data and a set of templates. Essentially, a static site generator automates the task of coding individual HTML pages and getting those pages ready to serve users ahead of time. Because these HTML pages are pre-built, they can load very quickly in users\u0026rsquo; browsers.* In basics, a static website generator generates a website once, based on fixed data. From then nothing changes till you rebuild the site again. (Keep this in mind)\nhttps://www.cloudflare.com/learning/performance/static-site-generator\nChoose a generator Ok, now static web generator it is, what is next?\nGoogle helped me üòâ¬†while searching for the best static web generator. Several popped up, where the most popular are Hugo and Jekyll.\nEvery generator has its pros and cons. Where I have chosen Hugo. For me the main reason is speed. Take a look at this benchmark Hugo vs Jekyll: Benchmarked\nThe fact, that the initial setup is a bit more complex and very less available templates (from Jekyll\u0026rsquo;s perspective) I took for granted.\nFurther, I skip the in-detail part because it does not matter for now.\nCheck this detailed article that explains Hugo vs. Jekyll in detail.\nHugo vs. Jekyll: Which is Right for Your Blog?\nHow to migrate WordPress to Hugo Ok, I decided to go all-in for a static page generator with Hugo. Now I need to migrate from WordPress to Markdown. Markdown is the raw data page format that Hugo renders into a nice webpage.\nWith that in mind, I started searching again on how to migrate WordPress to Hugo. I read a lot but unfortunately, I ran into problems. Not so much technical but more in the process.\nWhere to start After trial and error (a lot), I think this is a good process.\nIt sounds a bit weird, maybe, but I started with a Hugo theme first. I started looking for a theme that fits the most to my needs.\nThe reason for that is every template has its own layouts, settings, and options. Options at the page level, table of contents, shortcodes, etc.\nHaving the right theme first saves a lot of time later because you don‚Äôt need to rewrite your blog articles with the correct options and shortcodes every time!\nSuggestion: write down your requirements. That helps find the best template for you.\n(The thing that WordPress does well is the easiness of using a sidebar, left or right). That‚Äôs a requirement for me also in Hugo.\nAnother thing was having blog features like tags and categories. Also, a search option must be available.\nWith that in mind, only a few templates fit my need. From that point, it‚Äôs just the look and feel you like.\nHugo themes are mostly free. A good place to find Hugo themes is at https://github.com/gohugoio/hugoThemes\nExport WordPress content After you have found a theme, the next step is to export your WordPress posts and pages. Exporting WordPress content is simple via the WordPress menu. However, a default WordPress export results in an XML file which is not what we need. Because we need Markdown format I installed a plugin called Jekyll Exporter. Using this plugin results in a ZIP file that contains markdown files of every post and page.\nNow every post is in Markdown format it was time for me to check every post on the correct image location, removing Wordpress blocks and other unwanted syntaxes.\nIt was a very time-consuming job. But looking at the bright sight, rebuilding my website gave me the time to get all content nice and clean. (WordPress makes a mess from it)\nBuild up your new static website Now the fun part starts. Setting up your new Hugo website. At first, I deploy a Hugo site on my own system before we start creating an Azure static web app connected to GitHub. I started based on this Microsoft Document about how to publish a Hugo site to Azure Static Web Apps\nRunning Hugo local Running Hugo locally is because of the development speed. Let me explain in short. As mentioned earlier a static website generator builds a website based on static content and never changed till a new build. During developing and testing the website changes all the time. When running the website in a public place I have to wait till the website has been rebuilt. This takes around 2 minutes every time. Running Hugo locally the build is done directly after saving. So I create the website till it is finished and then upload all the files at once into my Github repository. After a commit, a GitHub action takes care of rebuilding the website to my Azure static app.\nHow to run Hugo locally check this URL.\nQuick Start\nBefore running these commands browse to a location you like. That will be the website root and the place to store the Markdown files. Adding your own chosen theme is simple by doing by repeating step 3. But this way has a downside because it fully depends on the theme‚Äôs repository. I have chosen to do it another way because I used a theme as a basis and made modifications like adding shortcodes, widgets, and the way how to display blogs.\nTo add a standalone theme I downloaded the theme from the repository and removed all the hidden GitHub folders. Thereafter I put the folder in the themes folder. After starting the Hugo website with Hugo server -D then go to https://localhost:1313\nHugo runs on a MAC almost natively. If you use a Windows client make sure you read this post. https://bwaycer.github.io/hugo_tutorial.hugo/tutorials/installing-on-windows/\nIn the post they refer to a Hugo for Windows package. Make sure you download the extended version. Otherwise some features won\u0026rsquo;t work. https://github.com/gohugoio/hugo/releases\nI found a solution in this post: https://discourse.gohugo.io/t/resize-image-this-feature-is-not-available-in-your-current-hugo-version/34682\nI tried with succes.\nConfigure your website After your website is running locally it is time for configuration. Hugo depends on a configuration file. This file can be stored in the website\u0026rsquo;s root or in the /config folder under the name config.toml (or .yaml / .json)\nFirst, enable your founded theme. When the theme is enabled it is time to format the menus, colors, and your exported blog posts to markdown format. Take a look at the theme‚Äôs page options. Options to set a title, image, categories, etc and are set at the head of the post.\nAfter every save, you will notice every change directly in your browser at https://localhost:1313\nMost work is done for now.\nFor more information about Hugo configuration check: https://gohugo.io/getting-started/configuration/\nPublish your website in public When you are happy it is time to put your website online.\nI use an Azure Static Web App because of its many benefits. It supports CI/CD connected to my GitHub repository, custom domains, and SSL for example. In the repository, all Hugo content is stored.\nWhat is Azure Static Web Apps?\nCreate a GitHub repository The source of all is my GitHub repository. In there all Hugo content is stored. How do you get the correct Hugo content? That is the content for your local Hugo installation. What I did is created a GitHub repository and cloned it to my device. In the empty cloned folder I created a Hugo instance with the steps above.\nFrom now this is my center point. I‚Äôm able to start the Hugo website locally for testing purposes and after a save I‚Äôm able to commit all changes directly to the repository.\nCheck my repo online: github-repo\nCreate Azure Static web app automated The Azure Static web app I create is connected to my GitHub account. Another option is an Azure DevOps repository.\nTo create an Azure static web app automated I used the code below.\n$websiteName = \u0026#34;newStaticWebsite\u0026#34; $url = \u0026#34;https://management.azure.com/subscriptions/xxxx/resourceGroups/website/providers/Microsoft.Web/staticSites/{0}?api-version=2022-03-01\u0026#34; -f $websiteName $body = @{ \u0026#34;location\u0026#34; = \u0026#34;WestEurope\u0026#34; \u0026#34;properties\u0026#34; = @{ \u0026#34;repositoryUrl\u0026#34; = \u0026#34;https://github.com/srozemuller/rozemuller_blog\u0026#34; \u0026#34;branch\u0026#34; = \u0026#34;main\u0026#34; \u0026#34;repositoryToken\u0026#34; = \u0026#34;github_pat_token\u0026#34; \u0026#34;buildProperties\u0026#34; = @{ \u0026#34;appLocation\u0026#34; = \u0026#34;app\u0026#34; \u0026#34;apiLocation\u0026#34; = \u0026#34;api\u0026#34; \u0026#34;appArtifactLocation\u0026#34; = \u0026#34;build\u0026#34; } } \u0026#34;sku\u0026#34; = @{ \u0026#34;name\u0026#34; = \u0026#34;standard\u0026#34; \u0026#34;tier\u0026#34; = \u0026#34;standard\u0026#34; } } | ConvertTo-Json -Depth 6 $authHeader = GetAuthToken -resource \u0026#34;https://management.azure.com\u0026#34; Invoke-RestMethod -Method put -uri $url -Headers $authheader -Body $body Read this doc about how to create a GitHub PAT token.\nGithub action explained You will notice there is a GitHub action created in your repository after creation.\nThe GitHub action (or DevOps pipeline in case of a DevOps connection) takes care of rebuilding your website after a commit to the main branch.\nAfter a commit, the action starts automatically. When the action is finished the website will be rebuilt again. Add a custom domain to Azure static web app A Static Web App in Azure does have a unique URL by default. But it is nice to have your domain connected. Connecting your domain is simple. Adding a custom domain is the first step followed by a validation step. (CNAME or TXT)\nI used the code below for adding a custom domain to an Azure Static Web App automated.\n$url = \u0026#34;https://management.azure.com/subscriptions/xxxx/resourceGroups/website/providers/Microsoft.Web/staticSites/rozemuller/customDomains/rozemuller.com?api-version=2022-03-01\u0026#34; $body = @{ \u0026#34;properties\u0026#34; = @{ domainName = \u0026#34;rozemuller.com\u0026#34; validationMethod = \u0026#34;dns-txt-token\u0026#34; } } | ConvertTo-Json $authHeader = GetAuthToken -resource \u0026#34;https://management.azure.com\u0026#34; Invoke-RestMethod -Method put -uri $url -Headers $authheader -Body $body The custom domain is now added and waiting for validation. The only thing I have to do now is added a TXT record to my DNS configuration and fill it with the provided code.\nUse cname-delegation under validationMethod in case you like adding a CNAME in your DNS.\nFor more information about adding custom domains check: https://docs.microsoft.com/en-us/azure/static-web-apps/custom-domain Take a good look at the preparation part.\nMy workflow Now everything is in place and the website is globally available. I like to share my workflow on how I update my website.\nIt is very simple and repeats steps from above.\nI start the local Hugo server first from the repository folder with hugo server -D In the content folder, I create a new folder for my blog post. (I use the branch bundle structure ) Page Bundles\nIn the created folder, I create an index.md file with the blog content. All other content like images is also stored in this folder. I used the leaf bundle to have all related blog content in one place.\nAfter I finished writing I commit the changes to the main branch. From that moment a GitHub action takes care of rebuilding the website with the GitHub Action mentioned earlier.\nLeftovers and considerations WordPress has some great plug-ins that helped me building my website. Think about Google Analytics and Yoast SEO.\nThose where for me the biggest issues to solve.\nGoogle Analytics This one is easy. Google Analytics plug-in places a small Javascript in the head of the page. A lot of Hugo themes also have this option available. Also mine. So that was an easy fix.\nThe only thing I did was move the script from the head to the footer. This is to load my site even faster.\nSEO and Hugo A good page ranking is important. Search engine optimization helps with that and has many factors. Yoast SEO for WordPress helps you. When using Hugo you have to fix some by yourself.\nThe most important things are a good title, meta description, and keywords. However, sometimes it‚Äôs hard to come up with a good title, etc. you have full control over these.\nAnother thing is having a responsive theme. Also supported by most themes.\nSearch engines don‚Äôt reveal their search algorithm but another big thing is load speed.\nI know Yoast helps you get your SEO better but all these little extras don‚Äôt fill the gap in relation to speed. So I agree with the little things I miss.\nTips on Hugo SEO (Search Engine Optimize)\nComments People do react to my blog posts. Most comments are a thank you or code updates (thank you for that :)). Hugo supports Disqus this is fine for me. For code updates also Github provides an option to create a new issue. At last, we have social media like Twitter. I tried to update my blog pages based on the comments. However, due the migration some comments felt of. If you have any question please let me know.\nEnough options to reach out to me.\nSocial Sharing After I finished all I ran into a problem while sharing a post on the social media. The problem was that the image was not loaded. After searching the web I found this post that describes my problem. After all I created an image with a max width of 600px and stored it into the same folder as the rest of the blog post images and named it xxx-thumbnail.extension.\nFixes Now a short time later the website is running fine but I noticed some things were handled by Apache which is now Hugo. Think about URL redirection in the case of 404.\nCustom 404 page Azure Static Web App Azure Static Web App can handle 404, but it redirects the client to a default Azure 404 page. I do have my 404 that I want to use. To use your redirect pages you need response overrides\nhttps://learn.microsoft.com/en-us/azure/static-web-apps/configuration#response-overrides\nIn the website`s root, I create a file staticwebapp.config.json. In the file, I created a responseOverrides with the 404 code and the page where clients need to be sent.\n\u0026#34;responseOverrides\u0026#34;: { \u0026#34;404\u0026#34;: { \u0026#34;rewrite\u0026#34;: \u0026#34;/404.html\u0026#34; } } I still have some work to do on the new format that will cost some time. Overall I\u0026rsquo;m happy about the way how the migration went and the website`s speed. I hope it will help you get the right content even better. I also hope you\u0026rsquo;ll agree with this new format. If you have any suggestions to make this site better please let me know.\nThank you for reading my blog move from wordpress to azure static web app with hugo and github. I hope you got a bit inspired. Enjoy your day and happy automating üëã ","date":"August 26, 2022","image":"http://localhost:1313/move-from-wordpress-to-azure-static-app-hugo/movement.jpeg","permalink":"/move-from-wordpress-to-azure-static-app-hugo/","title":"Move from WordPress to Azure Static Web App with Hugo and GitHub"},{"categories":["Identity","Security","Azure Virtual Desktop"],"contents":"In this blog post I show how to deploy an AVD session host with the Trusted Launch, secure boot and vTPM. This option is now Generally Available. I show how to deploy the AVD session host with the REST API and with the Az.Avd PowerShell module from a Compute Gallery image.\nTable Of Contents Before you start Create an AVD session host from Compute Gallery with Trusted Launch Starting variables Deploy AVD session host with the REST API Deploy AVD session host with Az.Avd PowerShell module Summary Azure offers trusted launch as a seamless way to improve the security of generation 2 VMs. Trusted launch protects against advanced and persistent attack techniques. A trusted launch comprises several coordinated infrastructure technologies that can be enabled independently. Each technology provides another layer of defense against sophisticated threats.\nAttention: Trusted launch requires the creation of new virtual machines. You can‚Äôt enable trusted launch on existing virtual machines that were initially created without it. This also means for images in the Azure Compute Gallery. Before you start Make sure you have the following in place before you start.\nA Compute Gallery Image definition with a Trusted Launch image. You receive an error like the one below if you haven‚Äôt. Updating an image afterward is not possible.\nI also planning a blog about how to create an Azure Compute Gallery with the Trusted Launch feature enabled. Because creating a new image with Trusted Launch is not possible through the portal. Expecting the blog in a few weeks.\nCreate an AVD session host from Compute Gallery with Trusted Launch In this chapter, I show two ways how to create a session host from the Azure Compute Gallery and Trusted Launch. First I show how to create a session host with the REST API. In the second part, I show how to deploy a session host (AAD) with the Az.Avd PowerShell module.\nStarting variables I created predefined variables because it makes the deployments easier.\n$azureApiUrl = \u0026#34;https://management.azure.com\u0026#34; $subscriptionId = \u0026#34;\u0026lt;subscriptionID\u0026gt;\u0026#34; $location = \u0026#34;westeurope\u0026#34; $hostpoolName = \u0026#34;Rozemuller-AAD\u0026#34; $resourceGroupName = \u0026#34;rg-roz-avd-01\u0026#34; $vmSize = \u0026#34;Standard_D2s_v4\u0026#34; $imageVersionId = \u0026#34;\u0026lt;enter the image version resourceID\u0026gt;\u0026#34; $prefix = \u0026#34;avd\u0026#34; $subnetId = \u0026#34;\u0026lt;enter the subnet resourceID\u0026gt;\u0026#34; $localAdmin = \u0026#34;\u0026lt;username\u0026gt;\u0026#34; $localPass = \u0026#34;\u0026lt;password\u0026gt;\u0026#34; $accesToken = Get-AzAccessToken -ResourceUrl $azureApiUrl $token = @{ \u0026#39;Content-Type\u0026#39; = \u0026#39;application/json\u0026#39; Authorization = \u0026#34;{0} {1}\u0026#34; -f $accesToken.Type, $accesToken.Token } Deploy AVD session host with the REST API Deploying a session host with the REST API consists of two main parts. First, we create the VM itself. This is the part where Trusted Launch is enabled. We add the VM as a session host in the second part.\nCreating the VM has two sub-parts, deploying the network and the rest. This is because the REST API does not support creating a network and the rest in one request.\nCreate the rest of the VM with the code below with first the network part.\n$vmName = \u0026#34;{0}-0\u0026#34; -f $Prefix $nicName = \u0026#34;{0}-nic\u0026#34; -f $vmName $nicBody = @{ \u0026#34;properties\u0026#34; = @{ \u0026#34;enableAcceleratedNetworking\u0026#34; = $true \u0026#34;ipConfigurations\u0026#34; = @( @{ \u0026#34;name\u0026#34; = \u0026#34;ipconfig1\u0026#34; \u0026#34;properties\u0026#34; = @{ \u0026#34;subnet\u0026#34; = @{ id = $SubnetId } } } ) } \u0026#34;location\u0026#34; = $Location } $nicJson = $nicBody | ConvertTo-Json -Depth 15 $nicUrl = \u0026#34;{0}/subscriptions/{1}/resourceGroups/{2}/providers/Microsoft.Network/networkInterfaces/{3}?api-version=2021-03-01\u0026#34; -f $azureApiUrl, $subscriptionId, $ResourceGroupName, $nicName $NIC = Invoke-RestMethod -Method PUT -Uri $nicUrl -Headers $token -Body $nicJson $vmBody = @{ location = $location identity = @{ type = \u0026#34;SystemAssigned\u0026#34; } properties = @{ licenseType = \u0026#34;Windows_Client\u0026#34; hardwareProfile = @{ vmSize = $VmSize } securityProfile = @{ securityType = \u0026#34;TrustedLaunch\u0026#34; uefiSettings = @{ secureBootEnabled = $true vTpmEnabled = $true } } storageProfile = @{ imageReference = @{ id = $ImageVersionId } osDisk = @{ caching = \u0026#34;ReadWrite\u0026#34; name = \u0026#34;{0}-os\u0026#34; -f $vmName createOption = \u0026#34;FromImage\u0026#34; } } osProfile = @{ adminUsername = $localAdmin computerName = $vmName adminPassword = $localPass } networkProfile = @{ networkInterfaces = @( @{ id = $NIC.Id properties = @{ primary = $true } } ) } } } | ConvertTo-Json -Depth 99 $vmUrl = \u0026#34;{0}/subscriptions/{1}/resourceGroups/{2}/providers/Microsoft.Compute/virtualMachines/{3}?api-version={4}\u0026#34; -f $azureApiUrl, $subscriptionId, $resourceGroupName, $vmName, \u0026#39;2021-11-01\u0026#39; Invoke-RestMethod -Method PUT -Uri $vmUrl -Headers $token -Body $vmBody About deploying the Azure AD and AVD extensions I would like to refer to this blog article I wrote a while ago about join Azure AD in an automated way.\nDeploy AVD session host with Az.Avd PowerShell module Using the Az.Avd Powershell module gives you a lot advantages. You don‚Äôt have to think about all the separate steps and write a lot of code.\nAdding a new session host is done with one single line of code. I turned the need starting variables into a parameter object because it keep my line of code clean.\n$newShParameters = @{ location = \u0026#34;westeurope\u0026#34; hostpoolName = \u0026#34;Rozemuller-AAD\u0026#34; resourceGroupName = \u0026#34;rg-roz-avd-01\u0026#34; vmSize = \u0026#34;Standard_D2s_v4\u0026#34; imageVersionId = \u0026#34;\u0026lt;imageversionID\u0026gt;\u0026#34; prefix = \u0026#34;avd\u0026#34; subnetId = \u0026#34;\u0026lt;subnetID\u0026gt;\u0026#34; localAdmin = \u0026#34;\u0026lt;username\u0026gt;\u0026#34; localPass = \u0026#34;\u0026lt;password\u0026gt;\u0026#34; initialNumber = 9 sessionHostCount = 1 disktype = \u0026#34;Premium_LRS\u0026#34; } New-AvdAadSessionHost @newShParameters -TrustedLaunch Az.Avd is available in the official PowerShell Gallery. Install the Az.Avd module with the code below.\nInstall-Module Az.Avd Import-Module Az.Avd If you have installed the Az.Avd module already, thank you for that üôè. In that case, just update the module with the command below.\nUpdate-Module Az.Avd Summary Thank you for reading my blog post about how to deploy an AVD session host with Trusted Launch from a custom image from the Azure Compute Gallery. I show two ways, the REST API and the Az.Avd PowerShell module. The Compute Gallery image must be ‚Äútrusted-launch-enabled‚Äù.\nI hope you got a bit inspired.\nEnjoy your day and happy automating üëã\n","date":"July 26, 2022","image":"http://localhost:1313/enroll-avd-session-host-with-trusted-launch-automated/computer-lock.png","permalink":"/enroll-avd-session-host-with-trusted-launch-automated/","title":"Enroll AVD session host with Trusted Launch automated"},{"categories":["Azure","Azure Virtual Desktop","Monitoring"],"contents":"In this post, I show how to make it possible to fix a personal AVD host as a user. There could be a situation where your AVD session host becomes unresponsive. In that case, you want to try to restart the host but are not able to log in to the host. Also, you can\u0026rsquo;t restart the host in the web portal / RD client.\nA healthy session host depends on several components. A session host must be online (heartbeat) and domain-joined. Also, a monitor- and AVD agent must be installed and running. If due some reason one of these components isn‚Äôt working well the session host becomes unresponsive. Unresponsive AVD session hosts are not available for the end-user.\nAt that moment you have to call the environments admin or support to restart your session host.\nIn this blog, I explain how to monitor an AVD session host status and send a remediation task to an end-user. (Almost) all configured in an automated way. That makes it possible to fix a personal AVD host as a user.\nTable Of Contents The idea Permissions Startup Prerequisites Authentication Starting variables AVD monitoring preparation Log Analytics Configure AVD host pool diagnostics Logic App Create MS Teams API connector Create Logic App workflow automated Assign Logic App permissions to resources Final (manual) step Azure Monitor Azure monitor action group Azure Monitor rule The process Summary The idea The main idea to fix an AVD host as a user is to send a Microsoft Teams message to the end-user if the user‚Äôs session host becomes unresponsive. In the message, the end-user is able to restart the session host. After the session host has restarted, the end-user is able to log in.\nIn the end, the result saves a call to support from an end-user.\nPermissions In this blog, we have two contexts, deployment, and execution. For deployment, we need a contributor role.\nIn the Logic App, tasks are executed to get VM and session-host information and restart the session-host. Before assigning the permissions I configured the Logic App as a managed identity. Thereafter the identity is assigned to the resource groups with the AVD resources and the virtual machines.\nThe Logic App permissions are:\nDesktop Virtualization Reader (Read AVD configuration, session host information, and more) Virtual Machine Contributor (to restart the machine. Check the permissions overview) Startup This is the deployment context. In this chapter, I explain which components we need and how to deploy these components in an automated way. For deployment, I use the Azure REST API and the Az.Avd PowerShell module.\nPrerequisites Before you start keep the following in mind:\nYou have a personal AVD environment enrolled; (if you do not have an AVD environment yet, check my blog post about deploying AVD automated) Creating a resource group is out of scope, but not that hard to create :); Authentication In the upcoming chapter, I show the needed components where it is used, and how to configure the component in an automated way. To deploy resources we need to log in first with an account with proper permissions. In the deployment context, I use a user account with contributor permissions. And use PowerShell to deploy the resources. Because I use the REST API, I need an authentication header to send in the request.\nUse the command below to get the authentication header.\n$token = Get-AzAccessToken -resource https://management.azure.com $graphAuthHeader = @{ \u0026#39;Content-Type\u0026#39; = \u0026#39;application/json\u0026#39; Authorization = \u0026#39;Bearer \u0026#39; +$token.token } In the case of a DevOps pipeline or another automation sequence, you might use an application. If you use an application please read this article where I explain how to log in with an application registration.\nStarting variables At the top of the script, I created a few variables ahead. This is because I do need these values multiple times.\n$azureApiUrl = \u0026#34;https://management.azure.com\u0026#34; $location = \u0026#34;WestEurope\u0026#34; $resourceGroupName = \u0026#39;rg-roz-avd-mon\u0026#39; $subscriptionId = (Get-azcontext).Subscription.Id $hostpoolName = \u0026#39;Rozemuller-Hostpool\u0026#39; $hpResourceGroupName = \u0026#39;rg-roz-avd-01\u0026#39; $logicAppWorkflowName = \u0026#34;la-sessionhost-alert\u0026#34; $laWorkspace = \u0026#34;log-analytics-avd-\u0026#34; + (Get-Random -Maximum 99999) $connectorName = \u0026#34;teams-connector\u0026#34; $actionGroupName = \u0026#34;ag-to-logicApp\u0026#34; $monitorRuleName = \u0026#34;monrule-avd-sessionhosthealth\u0026#34; AVD monitoring preparation Before we are able to fix an AVD host as a user we need to prepare our AVD environment. In the first deployment block, we prepare the AVD environment to make sure we capture all the needed information into Log Analytics.\nLog Analytics The basis is Log Analytics. This is where all the logs are stored and where I point the alert rule to. Creating a Log Analytics workspace is the first step in line.\nUse the code below to create a Log Analytics workspace automated with the REST API.\n$laWorkspace = \u0026#34;log-analytics-avd-\u0026#34; + (Get-Random -Maximum 99999) $lawsBody = @{ location = $location properties = @{ retentionInDays = \u0026#34;30\u0026#34; sku = @{ name = \u0026#34;PerGB2018\u0026#34; } } } $lawsUrl = \u0026#34;{0}/subscriptions/{1}/resourcegroups/{2}/providers/Microsoft.OperationalInsights/workspaces/{3}?api-version=2020-08-01\u0026#34; -f $azureApiUrl, $subscriptionId, $resourceGroupName, $laWorkspace $loganalyticsParameters = @{ URI = $lawsUrl Method = \u0026#34;PUT\u0026#34; Body = $lawsBody | ConvertTo-Json Headers = $graphAuthHeader } $laws = Invoke-RestMethod @loganalyticsParameters $laws Configure AVD host pool diagnostics In the next step, we configure the AVD hostpool to sent data to the Log Analytics Workspace. In this step, we configure the diagnostics settings at AVD hostpool level. The setting we need at least is AgentHealthStatus. For more information about configuring monitoring for AVD checks the monitoring part of this article.\nTo configure the diagnostics at the host pool level we need the host pool ID first. To get the host pool‚Äôs resource ID quick I use the Az.Avd PowerShell module.\n$hostpoolId = Get-AvdHostPool -HostPoolName $hostpoolName -ResourceGroupName $hpResourceGroupName After gathering the host pool information I set up the diagnostics with the code below.\n$diagnosticsBody = @{ Properties = @{ workspaceId = $laws.id logs = @( @{ Category = \u0026#39;AgentHealthStatus\u0026#39; Enabled = $true } ) } } $diagnosticsUrl = \u0026#34;{0}{1}/providers/microsoft.insights/diagnosticSettings/{2}?api-version=2017-05-01-preview\u0026#34; -f $azureApiUrl, $hostpoolId.id, $laws.name $diagnosticsParameters = @{ uri = $diagnosticsUrl Method = \u0026#34;PUT\u0026#34; Headers = $graphAuthHeader Body = $diagnosticsBody | ConvertTo-Json -Depth 4 } $diagnostics = Invoke-RestMethod @diagnosticsParameters $diagnostics Logic App The next step in line is creating the Logic App. The Logic App is triggered by the Azure Monitor rule that is created later. The reason why I create the Logic App first is because of the webhook URL. The webhook URL is needed in the Azure Monitor action group. Because we are receiving the webhook URL after saving the Logic App, this is the most efficient way of our deployment. (Instead of creating an action group first and updating it later)\nCreate MS Teams API connector Use an MS Teams connector to send messages to MS Teams. The connector is a separate Azure resource, create the connector first. The connector is created automatically in the background when using the portal. To create the API connector for Logic Apps automated, I used the code below.\nSkipping the deep code analysis for now but, make a note about the **$connectorName** variable. This is the variable configured in the starting variables block. This is also the name that the Logic App uses.\nSearch in the send-toteams-if-unhealthy.json for \u0026lt;‚ÄìconnectorName‚Äì\u0026gt; and change that into the connector name.\n$apiConnectionWebBody = @{ location = $location type = \u0026#34;Microsoft.Web/connections\u0026#34; properties = @{ api = @{ brandColor = \u0026#34;#4B53BC\u0026#34; category = \u0026#34;Standard\u0026#34; description = \u0026#34;Microsoft Teams enables you to get all your content, tools and conversations in the Team workspace with Office 365.\u0026#34; displayName = \u0026#34;Microsoft Teams\u0026#34; iconUri = \u0026#34;https://connectoricons-prod.azureedge.net/releases/v1.0.1585/1.0.1585.2895/teams/icon.png\u0026#34; id = \u0026#34;/subscriptions/{0}/providers/Microsoft.Web/locations/westeurope/managedApis/teams\u0026#34; -f $subscriptionId name = \u0026#34;teams\u0026#34; type = \u0026#34;Microsoft.Web/locations/managedApis\u0026#34; } } } $apiConnectionWebUrl = \u0026#34;{0}/subscriptions/{1}/resourceGroups/{2}/providers/Microsoft.Web/connections/{3}?api-version=2018-07-01-preview\u0026#34; -f $azureApiUrl, $subscriptionId, $resourceGroupName, $connectorName $apiConnectionWebParameters = @{ uri = $apiConnectionWebUrl Method = \u0026#34;PUT\u0026#34; Headers = $graphAuthHeader Body = $apiConnectionWebBody | ConvertTo-Json -Depth 99 } $logicApiConnectioncApp = Invoke-RestMethod @apiConnectionWebParameters $logicApiConnectioncApp Create Logic App workflow automated In this part, we create the Logic App workflow based on a JSON configuration file. As mentioned above, make sure you changed the \u0026lt;‚ÄìconnectorName‚Äì\u0026gt; token in the JSON file into the correct name. The Logic App is created and gets a system-assigned identity.\n$logicAppBody = @{ location = $location identity = @{ type = \u0026#34;SystemAssigned\u0026#34; } properties = @{ definition = Get-Content ./send-toteams-if-unhealty.json | ConvertFrom-Json parameters = @{ \u0026#34;`$connections\u0026#34; = @{ value = @{ $connectorName = @{ \u0026#34;connectionId\u0026#34; = \u0026#34;{0}\u0026#34; -f $logicApiConnectioncApp.id \u0026#34;connectionName\u0026#34; = \u0026#34;{0}\u0026#34; -f $connectorName \u0026#34;id\u0026#34; = \u0026#34;{0}\u0026#34; -f $logicApiConnectioncApp.properties.api.id } } } } } } $logicAppUrl = \u0026#34;{0}/subscriptions/{1}/resourceGroups/{2}/providers/Microsoft.Logic/workflows/{3}?api-version=2016-06-01\u0026#34; -f $azureApiUrl, $subscriptionId, $resourceGroupName, $logicAppWorkflowName $logicAppParameters = @{ uri = $logicAppUrl Method = \u0026#34;PUT\u0026#34; Headers = $graphAuthHeader Body = $logicAppBody | ConvertTo-Json -Depth 99 } $logicApp = Invoke-RestMethod @logicAppParameters Assign Logic App permissions to resources The Logic App needs permissions to get the session host information and to restart a VM. I searched in the build-in roles documentation for the roles I need. Next is to assign the Logic App on these roles as scope the permissions at resource group level. The session hosts are stored in this resource group.\nAssign Azure roles to the resource groups automated with the code below. I created a small loop based on the build-in role Id‚Äôs and assign (scope) the roles at the AVD resource group. Fill in the Logic App identity at the principalId location.\n# Virtual Machine Contributor : 9980e02c-c2be-4d73-94e8-173b1dc7cf3c # Desktop Virtualization Reade : 49a72310-ab8d-41df-bbb0-79b649203868 $rolesIds = @(\u0026#34;9980e02c-c2be-4d73-94e8-173b1dc7cf3c\u0026#34;,\u0026#34;49a72310-ab8d-41df-bbb0-79b649203868\u0026#34;) $rolesIds | ForEach-Object { $assignGuid = (New-Guid).Guid $assignURL = \u0026#34;{0}/subscriptions/{1}/resourcegroups/{2}/providers/Microsoft.Authorization/roleAssignments/{3}?api-version=2015-07-01\u0026#34; -f $azureApiUrl, $subscriptionId,$hpResourceGroupName , $assignGuid $assignBody = @{ properties = @{ roleDefinitionId = \u0026#34;/subscriptions/{0}/resourcegroups/{1}/providers/Microsoft.Authorization/roleDefinitions/{2}\u0026#34; -f $subscriptionId, $hpResourceGroupName, $_ principalId = $logicapp.identity.principalId } } | ConvertTo-Json Invoke-RestMethod -Method PUT -Uri $assignURL -Headers $graphAuthHeader -Body $assignBody } Final (manual) step I left a manual step especially for the people who are not that familiar with automation :). After deploying all the resources there is only one thing left, authorization. This is by design and how OAuth2 works. There is no way to fully automate an api-connection authorization. After some research I found the following:\nFrom the Microsoft docs: Authorize OAuth connections\nAlso in this topic I found an answer which says the same.\nIn the Azure portal the connection will be in error state with the following error. If you click on the error, you are able to authenticate the connector.\nThere are way to avoid also this manual step by creating a preconfigured api-connection and refer to that connection in the Logic App. This is a good choice when enrolling more Logic Apps.\nAzure Monitor The next big part to fix a personal AVD host by an user is Azure Monitor. In the Azure Monitor, we create an action group and a monitor rule. An action group is a collection of notification preferences. Preferences like sending an SMS, email, or triggering a webhook. In this blog we the webhook trigger. A monitor rule is one that checks for unwanted situations. If an error occurred for a specific duration, the rule creates an alert. The rule is assigned to the action group.\nAzure monitor action group As mentioned we create an action group first. Besides the default settings, in this action group, I configure to trigger a webhook if an alert hits this group. The first step is getting the webhook URL from the just created Logic App. The webhook URL is the entry point to trigger the Logic App to run. The Logic App trigger can handle parameters as well.\nTo get the Logic App trigger URL I used the code below.\n$triggerUrl = \u0026#34;{0}{1}/triggers/manual/listCallbackUrl?api-version=2016-10-01\u0026#34; -f $azureApiUrl, $logicApp.id $triggerParameters = @{ uri = $triggerUrl Method = \u0026#34;POST\u0026#34; Headers = $graphAuthHeader } $trigger = Invoke-RestMethod @triggerParameters $trigger.value After searching for the trigger URL, it is time to create an action group. As mentioned earlier an action group can have multiple actions. In this part, we configure an action group that triggers the Logic App. Besides the general settings like location and names, we have the ability to configure several breakouts like a Logic App.\nThe following receivers are available at this time. All with their own settings.\narmRoleReceivers automationRunbookReceivers azureAppPushReceivers azureFunctionReceivers emailReceivers eventHubReceivers itsmReceivers logicAppReceivers smsReceivers voiceReceivers webhookReceivers To create an Azure Monitor Action Group automated, I used the code below. The code creates a breakout to the logicAppReceivers. To trigger a Logic App you also can use the webhookReceivers breakout.\n$actionGroupBody = @{ location = \u0026#34;Global\u0026#34; properties = @{ groupShortName = \u0026#34;agToLa\u0026#34; enabled = $true logicAppReceivers = @( @{ name = \u0026#34;{0}\u0026#34; -f $logicApp.name resourceId = \u0026#34;{0}\u0026#34; -f $logicApp.id callbackUrl = \u0026#34;{0}\u0026#34; -f $trigger.value useCommonAlertSchema = $true } ) } } $actionGroupUrl = \u0026#34;{0}/subscriptions/{1}/resourceGroups/{2}/providers/Microsoft.Insights/actionGroups/{3}?api-version=2021-09-01\u0026#34; -f $azureApiUrl, $subscriptionId, $resourceGroupName, $actionGroupName $actionGroupParameters = @{ uri = $actionGroupUrl Method = \u0026#34;PUT\u0026#34; Headers = $graphAuthHeader Body = $actionGroupBody | ConvertTo-Json -Depth 5 } $actionGroup = Invoke-RestMethod @actionGroupParameters $actionGroup https://docs.microsoft.com/en-us/azure/azure-monitor/alerts/action-groups\nAzure Monitor rule The next step in line is creating a monitor role. A monitor rule in basics consists of scope, condition, and action.\nAt first, we scope the rule to Log Analytics Workspace we created earlier in this post. Based on the scope we get the applicable monitor signals.\nIn the second step, we configure a condition. The condition tells where the rule needs to look at and when to send an alert. Because we scoped at a Log Analytics resource, we are able to execute a custom log query. Based on the query output we configure when the result is an error.\nThe Kusto query looks like the below. In this query, I search for the AVD session host statuses. The query outputs every status where I configure the dimensions in the monitor rule.\nYou could also choose to filter in the query directly. The advantage of filtering afterward is that, during configuring dimensions, the portal provides all values where you just have to select the one you need. (This avoids typos)\nWVDAgentHealthStatus | project TimeGenerated, LastHeartBeat, SessionHostName, SessionHostResourceId, Status, sessionHostId = strcat(_ResourceId,\u0026#34;/sessionhosts/\u0026#34;,SessionHostName), _ResourceId , EndpointState The further configuration like measures and threshold depends on your situation. In my case, the rule checks every 5 minutes and counts the rows. If the row count is greater than 0 an alert is sent.\nAt last, we configure the action which sent the alerts to the action group we created above. You can have multiple action groups configured.\nWhat does this look like in automation?\nThere are a few monitor rule types. In my case, I use the scheduled query rule. This rule type gives the ability to run Kusto queries every n minute.\n$monitorRuleBody = @{ location = $location properties = @{ severity = 0 enabled = $true evaluationFrequency = \u0026#34;PT5M\u0026#34; scopes = @( $laws.id ) targetResourceTypes = @( $laws.type ) windowSize = \u0026#34;PT5M\u0026#34; criteria = @{ allOf = @( @{ query = \u0026#34;WVDAgentHealthStatus | project TimeGenerated, LastHeartBeat, SessionHostName, SessionHostResourceId, Status, sessionHostId = strcat(_ResourceId,\u0026#39;/sessionhosts/\u0026#39;,SessionHostName), _ResourceId , EndpointState\u0026#34; timeAggregation = \u0026#34;Count\u0026#34; dimensions = @( @{ name = \u0026#34;SessionHostName\u0026#34; operator = \u0026#34;Include\u0026#34; values = @(\u0026#34;*\u0026#34;) } @{ name = \u0026#34;SessionHostResourceId\u0026#34; operator = \u0026#34;Include\u0026#34; values = @(\u0026#34;*\u0026#34;) } @{ name = \u0026#34;Status\u0026#34; operator = \u0026#34;Include\u0026#34; values = @(\u0026#34;Unavailable\u0026#34;) } @{ name = \u0026#34;sessionHostId\u0026#34; operator = \u0026#34;Include\u0026#34; values = @(\u0026#34;*\u0026#34;) } @{ name = \u0026#34;_ResourceId\u0026#34; operator = \u0026#34;Include\u0026#34; values = @(\u0026#34;*\u0026#34;) } @{ name = \u0026#34;EndpointState\u0026#34; operator = \u0026#34;Include\u0026#34; values = @(\u0026#34;Unhealthy\u0026#34;) } ) operator = \u0026#34;GreaterThanOrEqual\u0026#34; threshold = 1 failingPeriods = @{ numberOfEvaluationPeriods = 1 minFailingPeriodsToAlert = 1 } } ) } autoMitigate = $false actions = @{ actionGroups = @( $actionGroup.id ) } } } $monitorRuleUrl = \u0026#34;{0}/subscriptions/{1}/resourceGroups/{2}/providers/Microsoft.Insights/scheduledQueryRules/{3}?api-version=2021-08-01\u0026#34; -f $azureApiUrl, $subscriptionId, $resourceGroupName, $monitorRuleName $monitorRuleParameters = @{ uri = $monitorRuleUrl Method = \u0026#34;PUT\u0026#34; Headers = $graphAuthHeader Body = $monitorRuleBody | ConvertTo-Json -Depth 8 } $monitorRule = Invoke-RestMethod @monitorRuleParameters $monitorRule The process Before we go to the next steps let‚Äôs see what we have so far. We have configured the following:\nA new Log Analytics Workspace We configured diagnostics settings in the AVD host pool to send logs to the workspace A Logic App with an authenticated MS Teams connector An Azure Monitor Action group that send alerts to a Logic App An Azure Monitor Rule which looks for unhealthy session hosts every 5 minutes Now it is time to look at what is happening after the Logic App received an alert. Imagine a session host went unresponsive and there is an alert.\nAfter an alert occurred the action group triggers the Logic App. The Logic App searches for the virtual machine first. If the machine is NOT deallocated the Logic App search for the session host and gathers the assigned user (that is the reason why this only works with personal AVD host pools).\nThere after the Logic App sends an MS Teams adaptive card for the first time with the option to restart the session host. After the end-user responded the Logic App sends a second message when the restart was successful.\nAfter the end-user‚Äôs response a new message is sent when the session host is restarted.\nYou see log rules in the Actitity Log like below.\nSummary I want to thank you reading this post about how to fix a personal AVD session host as an user\nI showed in this blogpost how to create a kind of AVD self-service option for end-users. An end-user is able to restart his personal session host if it becomes unavailable. The message is sent to Microsoft Teams based on an adaptive card.\nWe created a Log Analytics Workspace and prepared the AVD hostpool to sent information to the workspace. Also, we created a Logic App and a MS Teams connector. At last we created an Azure Monitor action group and an alert rule. All those resources are deployed in automated way.\nThank you for reading my blog fix personal avd host as a user with ms teams. I hope you got a bit inspired. Enjoy your day and happy automating üëã ","date":"July 11, 2022","image":"http://localhost:1313/fix-personal-avd-host-as-a-user-with-ms-teams/fix-hardware.png","permalink":"/fix-personal-avd-host-as-a-user-with-ms-teams/","title":"Fix personal AVD host as a user with MS Teams"},{"categories":["Microsoft Intune","Monitoring"],"contents":"Happy users are key. But with all the changes in hardware, software distributions, security- and OS updates, it could take a lot of effort. Now remote working is embedded into our culture it costs even more effort to keep sight of what is happening. Endpoint analytics gives you insight into how the organization is working and the quality of experience which is delivered to users. I show how to enable endpoint analytics automated.\nTable Of Contents Endpoint Analytics in short Prerequisites Enable Endpoint Analytics automated Assignment Summary Endpoint Analytics in short It also helps identify issues with hardware or policies which slow down devices (slow boot times for example). So Endpoint Analytics gives you important information which helps fix issues before end-users created a ticket. In the upcoming paragraphs, I show how to deploy enable Endpoint Analytics in an automated way.\nPrerequisites Before we enable Endpoint Analitycs make sure you pass the following:\nMicrosoft Intune enrolled or co-managed devices running the following: Windows 10 version 1903 or later The cumulative update from July 2021 or later installed Pro, Pro Education, Enterprise, or Education. Home and long-term servicing channel (LTSC) isn‚Äôt supported. Windows devices must be Azure AD joined or hybrid Azure AD joined. Workplace joined or Azure AD registered devices aren‚Äôt supported. The Connected User Experiences and Telemetry service on the device is running Another requirement is that the device can reach: https://*.events.data.microsoft.com. This is the URL where endpoints send the data to. For more about data collection check https://docs.microsoft.com/en-us/mem/analytics/data-collection#bkmk_datacollection\nAt last, you need at least one of these licenses:\nEnterprise Mobility + Security E3 or higher Microsoft 365 Enterprise E3 or higher. Source: https://docs.microsoft.com/en-us/mem/analytics/enroll-intune#bkmk_prereq\nEnable Endpoint Analytics automated Onboarding devices to Endpoint Analytics is very simple. It is just a matter of configuring a device configuration policy with a Windows Health monitoring profile type. So, how do we enable Endpoint Analytics automated?\nBecause I want a separate policy, I‚Äôm using the code below to enable Endpoint Analytics. Make a note about the @odata.type value which is the windowsHealthMonitoringConfiguration. This type allows providing parameters that belong to this kind of policy.\nWhen using the portal, you will notice the scope has two options. In this situation, I only choose Endpoint Analytics (EA). The configDeviceHealthMonitoringScope items represent this value which is a String. Use the bootPerformance value to configure EA.\nThis sounds a bit weird but actually, EA is capturing the Windows boot performance data.\nSource: https://docs.microsoft.com/en-us/mem/analytics/enroll-intune#bkmk_onboard\nDuring configuration, you will have the ability to configure applicability rules. In these rules, you specify device profiles to where a policy is assigned or not. In the case of Azure Virtual Desktop (AVD), choose the Windows 10 Enterprise option.\nAfter log-in using Connect-AzAccount, I request a Graph token. Because endpoint analytics is also a device configuration policy, we need the device configuration URL. By pointing to the correct @odata.type, we can configure the correct objects. In my blog about Update AVD with MEM, I explained more about @odata.types.\nfunction GetAuthToken($resource) { $context = [Microsoft.Azure.Commands.Common.Authentication.Abstractions.AzureRmProfileProvider]::Instance.Profile.DefaultContext $Token = [Microsoft.Azure.Commands.Common.Authentication.AzureSession]::Instance.AuthenticationFactory.Authenticate($context.Account, $context.Environment, $context.Tenant.Id.ToString(), $null, [Microsoft.Azure.Commands.Common.Authentication.ShowDialog]::Never, $null, $resource).AccessToken $authHeader = @{ \u0026#39;Content-Type\u0026#39; = \u0026#39;application/json\u0026#39; Authorization = \u0026#39;Bearer \u0026#39; + $Token } return $authHeader } $script:deviceConfigurl = \u0026#34;https://graph.microsoft.com/beta/deviceManagement/deviceConfigurations\u0026#34; $script:token = GetAuthToken -resource \u0026#39;https://graph.microsoft.com\u0026#39; $endpointAnalyticsBody = @{ \u0026#34;@odata.type\u0026#34; = \u0026#34;#microsoft.graph.windowsHealthMonitoringConfiguration\u0026#34; description = \u0026#34;Shows the analytics about endpoints\u0026#34; displayName = \u0026#34;Windows Health - Endpoint Analytics\u0026#34; version = 1 allowDeviceHealthMonitoring = \u0026#34;enabled\u0026#34; configDeviceHealthMonitoringScope = \u0026#34;bootPerformance\u0026#34; } $endpointAnalyticsPostBody = $endpointAnalyticsBody | ConvertTo-Json -Depth 3 $deployEndpointAnalytics = Invoke-RestMethod -Uri $script:deviceConfigurl -Method POST -Headers $script:token -Body $endpointAnalyticsPostBody For more information about the endpoint analytics feature, check the Mircosoft documentation: https://docs.microsoft.com/en-us/mem/analytics/overview\nAssignment The last step in configuring endpoint analytics the automated way is the assignment. Because I want to capture as much info as possible, I have chosen to assign the policy to all devices. To assign to all devices, use the ‚Äú#microsoft.graph.allDevicesAssignmentTarget‚Äù odata type.\n$assignBody = @{ \u0026#34;@odata.type\u0026#34; = \u0026#34;#microsoft.graph.deviceConfigurationAssignment\u0026#34; target = @{ \u0026#34;@odata.type\u0026#34; = \u0026#34;#microsoft.graph.allDevicesAssignmentTarget\u0026#34; deviceAndAppManagementAssignmentFilterType = $null deviceAndAppManagementAssignmentFilterId = $null } } $assignParameters = @{ method = \u0026#34;POST\u0026#34; uri = \u0026#34;https://graph.microsoft.com/v1.0/deviceManagement/deviceConfigurations/{0}/assignments\u0026#34; -f $deployEndpointAnalytics.id Headers = $token body = $assignBody | ConvertTo-Json } $assignValues = Invoke-RestMethod @assignParameters $assignValues Summary In this blog post, I explained that Endpoint Analytics gives you insight into how the organization is working. It gives you an overview of the experience delivered to users. Also, I showed how to configure Endpoint Analytics automated. In the next blog (writing on it) I show how to use the data and how to create alerts for unwanted situations.\nThank you for reading this blog about configuring Endpoint Analytics automated. I hope you got a bit inspired.\nEnjoy your day and happy automating üëã\n","date":"July 3, 2022","image":"http://localhost:1313/enable-endpoint-analytics-automated-in-microsoft-endpoint-manager/measure-band.jpeg","permalink":"/enable-endpoint-analytics-automated-in-microsoft-endpoint-manager/","title":"Enable Endpoint Analytics automated in Microsoft Intune"},{"categories":["Microsoft Intune"],"contents":"Intune filters are a great way to assign MEM policies to devices. In fact, filters go over groups in several scenarios. Filters help you keep your environment reliable and fast. In this blog, I do some basic explanations about filters, how to use filters, and show how to create Intune filters in an automated way. At last, I show how to assign filters to policies.\nTable Of Contents Filters in basics Why filters instead of dynamic groups? Authentication Create Intune filters automated Assing profiles with Intune filters automated Filters in basics Filters has to do with targeting. When you create a policy, you can use filters to assign a policy based on rules you create. A filter allows you to narrow the assignment scope of a policy. For example, to target devices with a specific OS version or a specific manufacturer. In relation to AD groups, filtering is high performance and low latency. In the rule syntax you add expressions to create a set of rules based on device properties. Good to know is that expressions are only at the device level.\nWhy filters instead of dynamic groups? Several ways are available to group a set of devices dynamically. Using filters is one, and secondly there are dynamic groups. In both there is a rule syntax so what is the difference. I‚Äôm not going into detail in this post but the main fact it is all about performance. Azure AD and Intune are two different enviroment/platforms. Between the platforms synchronisation takes place. When having a 10k devices + environment every synchronisation has a large impact.\nIf you assign Intune workloads to large Azure AD groups containing many users or devices, it may cause large synchronization backlogs in your account. This impacts policy and app deployments, which will take longer to reach managed devices.\nFilters takes place at the All Users and All Devices virtual groups. These groups only exists in Intune. When using filters over virtual groups no new synchronisation is needed. The best way to understand is to compare filters with a SQL view on a database. A SQL view shows information over (multiple) SQL tables to get all needed information together without creating new tables every time with all needed information.\nFor more information about grouping, filtering and targeting the URL below.\nhttps://techcommunity.microsoft.com/t5/intune-customer-success/intune-grouping-targeting-and-filtering-recommendations-for-best/ba-p/2983058\nOr the the recording at: https://techcommunity.microsoft.com/t5/video-hub/microsoft-endpoint-manager-a-deep-dive-on-grouping-targeting-and/ba-p/2911265\nFor a filter overview please check the Microsoft documentation: https://docs.microsoft.com/en-us/mem/intune/fundamentals/filters\nAuthentication To deploy filters in Microsoft Intune we need to authenticate. I use the Graph API for automated filter deployment. To authenticate against the Graph API we need API permissions. The API permissions are set at an application registration. The application registration needs the following permissions:\nDeviceManagementConfiguration.ReadWrite.All\n(Allows the app to read and write properties of Microsoft Intune-managed device configuration and device compliance policies and their assignment to groups, without a signed-in user.) To authenticate against the Graph API I used the PowerShell code below:\n$appId = \u0026#34;077d124e\u0026#34; $appSecret = \u0026#34;0UF7Q\u0026#34; $tenantId = \u0026#34;dag7\u0026#34; $body = @{ grant_Type = \u0026#34;client_credentials\u0026#34; scope = \u0026#34;https://graph.microsoft.com/.default\u0026#34; client_Id = $appId client_Secret = $appSecret } $connectParams = @{ uri = \u0026#34;https://login.microsoftonline.com/{0}/oauth2/v2.0/token\u0026#34; -f $tenantId method = \u0026#34;POST\u0026#34; body = $body } $connect = Invoke-RestMethod @connectParams $authHeader = @{ \u0026#39;Content-Type\u0026#39; = \u0026#39;application/json\u0026#39; Authorization = \u0026#39;Bearer \u0026#39; +$connect.access_token } If you are not that familiar with REST API authentication then it also possible to use the PowerShell command Get-AzAccessToken. To use that command you need to connect to Azure first with Connect-AzAccount. I also connect with the application. If you use these commands, install the Az.Accounts PowerShell module.\n$passwd = ConvertTo-SecureString $appSecret -AsPlainText -Force $pscredential = New-Object System.Management.Automation.PSCredential($appId, $passwd) Connect-AzAccount -ServicePrincipal -Credential $pscredential -Tenant $tenantId $token = Get-AzAccessToken -ResourceUrl \u0026#39;https://graph.microsoft.com\u0026#39; $authHeader = @{ \u0026#39;Content-Type\u0026#39; = \u0026#39;application/json\u0026#39; Authorization = \u0026#39;Bearer \u0026#39; + $token.Token } Create Intune filters automated Now it is time to create Intune filters the automated way. The body is quite simple. It consists of a displayname, a show description, the platform, and the rule syntax. The displayname and description are free values. The platform has some fixed values: android, androidForWork, iOS, macOS, windowsPhone81, windows81AndLater, windows10AndLater, androidWorkProfile or unknown.\nAt last is the rule itself. As shown above, filter rules consists of a device‚Äôs property. To clarify, I underlined the property names. For example: devices.deviceName or devices.manufacturer.\n$filterBody = @{ displayName = \u0026#34;Automated Filter Creation\u0026#34; description = \u0026#34;This filter is created the automated way\u0026#34; platform = \u0026#34;windows10AndLater\u0026#34; rule = \u0026#39;(device.manufacturer -eq \u0026#34;Microsoft Corporation\u0026#34;)\u0026#39; } After that, the body is prepared let‚Äôs send the request. To send the request I used the PowerShell code below. That filters are a way to filter devices is also proven by the API URL. This because it is a part of the devices management system.\nhttps://graph.microsoft.com/beta/deviceManagement/assignmentFilters\n$filterUrl = \u0026#34;https://graph.microsoft.com/beta/deviceManagement/assignmentFilters\u0026#34; $filterParams = @{ URI = $filterUrl Method = \u0026#34;POST\u0026#34; Headers = $authHeader Body = $filterBody | ConvertTo-Json } $filters = Invoke-RestMethod @filterParams $filters ![intune filter created automated](https://www.rozemuller.com/wp-content/uploads/2022/03/image-29-1024x214.png) For more information about creating filters check the URL: Assing profiles with Intune filters automated As shown above, filters are using in combination with virtual groups (All Users, All Devices). In the last step I explain how to assign the All Devices virtual group to a policy with a filter. To achieve that goal I first search for the policy.\nMain goal is to assign the Windows Power Settings policy to non virtual machines.\nThe result is stored in the $policy variable which I need later.\n$policyUrl = \u0026#34;https://graph.microsoft.com/beta/deviceManagement/configurationPolicies?`$filter=name eq \u0026#39;Windows Power Settings\u0026#39;\u0026#34; $policyParams = @{ URI = $policyUrl Method = \u0026#34;GET\u0026#34; Headers = $authHeader } $policy = Invoke-RestMethod @policyParams $policy.value Secondly I search for the non virtual machines filter. In this command I store the filter also in a variable.\n$filterUrl = \u0026#34;https://graph.microsoft.com/beta/deviceManagement/assignmentFilters\u0026#34; $filterParams = @{ URI = $filterUrl Method = \u0026#34;GET\u0026#34; Headers = $authHeader } $filters = Invoke-RestMethod @filterParams $filter = $filters.value | Where-Object {$_.displayName -eq \u0026#39;No virtual machines\u0026#39;} In the last step I bring all the parts together. The Windows Power Settings policy and the non virtual machines filter. First I create the assignments body. In the code I use the allDevicesAssignmentTarget type. This represents the All Devices virtual group. As every assignment also this one needs a unique ID. Generating an unique ID with PowerShell is simply done with the New-Guid command.\nThe filter expression is to search for device models which are NOT virtual machines. The results must be included in this policy. The filter type in these is Include (non virtual machines). At last we need the filter ID from the $filter variable. The whole package is converted to JSON.\n$assignBody = @{ \u0026#34;assignments\u0026#34; = @( @{ id = $(New-Guid).Guid target = @{ \u0026#34;@odata.type\u0026#34; = \u0026#34;#microsoft.graph.allDevicesAssignmentTarget\u0026#34; deviceAndAppManagementAssignmentFilterType = \u0026#34;include\u0026#34; deviceAndAppManagementAssignmentFilterId = $filter.id } } ) } $assignBody = $assignBody | ConvertTo-Json -Depth 4 Last, we need to assign the package to the policy. This is the time where the $policy variable comes up.\n$assignmentUrl = \u0026#34;https://graph.microsoft.com/beta/deviceManagement/configurationPolicies/{0}/assign\u0026#34; -f $policy.id $policyParams = @{ URI = $assignmentUrl Method = \u0026#34;POST\u0026#34; Headers = $authHeader Body = $assignBody } $assignment = Invoke-RestMethod @policyParams $assignment ![intune filter assigned automated](https://www.rozemuller.com/wp-content/uploads/2022/03/image-31-1024x460.png)## Summary In this blog post, I showed how to create a filter in Intune automated and how to assign a filter. Filters help target policies to the correct devices without creating new AD groups every time and improve the environment‚Äôs performance.\nThank you for reading my blog about how to create Intune filters automated into Microsoft Intune. If you need another example, take a look at my blog about how to deploy power management settings to non-virtual machines\nEnjoy your day and happy automating üëã\n","date":"June 21, 2022","image":"http://localhost:1313/intune-filters-explained-and-create-automated/filters-coffee-e1646681551213.jpeg","permalink":"/intune-filters-explained-and-create-automated/","title":"Intune filters explained create automated and assignments"},{"categories":["Azure AD","Monitoring","Security"],"contents":"An Azure tenant is the Microsoft public cloud base. It is very important to keep your bases as secure as possible. To keep your tenant secure, Microsoft provides actions that give you insights into your tenant‚Äôs security and how to improve it. In this blog, I show how to monitor the secure score security improvement action status.\nAs mentioned it is very important to keep your tenant as secure as possible and Mircosoft provides recommendations to help. Every recommendation has its own impact, score, and status. In this blog post, I look at the improvement actions and their status. I will explain in this blog post why that status is that important.\nResolving actions is out of scope in this post.\nTable Of Contents Prerequisites Main idea and philosophy Identity Secure Score How is the score calculated? Security Improvement Actions overview Security Improvement Actions detailed Monitor Identity Secure Score improvement action status Get improvement actions overview Get improvement action details Summary Prerequisites To start with this blog post, make sure you have configured an application with the correct permissions. Thereafter make sure you are authenticated.\nThe needed API permissions for reading security improvement actions are:\nDelegated or Application: SecurityEvents.Read.All, SecurityEvents.ReadWrite.All To authenticate against the Graph API with PowerShell, check this part.\nMain idea and philosophy The main idea is to get insights into which security improvement action has a risk accepted state but actually needs to be fixed. Yes, there is also an overview at the security.microsoft.com portal. But, when the status has changed, I want a trigger. This over multiple environments. In this blog post, I check if a self-defined action is set to ‚Äòrisk accepted‚Äò and has the identity category.\nThe reason I want to know is that, in my opinion, identity is the basis for every platform. In this blog post, I show how to get information and store the output for later use. I use PowerShell to get the data. From that point, I am able to send data to any system.\nIdentity Secure Score It all starts with the Microsoft Secure Score. The Microsoft Secure Score is divided into three subcategories: Identity, Device, and Apps. The focus of this post is on the identity part. The Identity Secure Score tells how secure your tenant is from an identity perspective. You can find the identity score also in the Azure portal under Azure Active Directory -\u0026gt; Security -\u0026gt; Identity Secure Score.\nMake a note that the Identity Secure Score is part of the grand total.\nHow is the score calculated? Where the Identity Secure Score is a part of the grand total, an improvement action is part of the identity score.\nThe score is calculated every 24 hours. Think about enabling password expiration, turning on the sign-in risk policy, or removing global admins to get the max of 5. And so, many more. Every task has an impact on the score, users, and costs. Some actions have a high improvement on the score while others have a high user impact.\nFor me getting a high score is not a goal in itself. Resolving every task won‚Äôt say it is the best for your situation. The best is to find a balance between what works and your security level.\nTo clarify, an example from Microsoft:\nFor example, an improvement action states you get 10 points by protecting all your users with multi-factor authentication. You only have 50 of 100 total users protected, so you‚Äôd get a partial score of 5 points (50 protected / 100 total * 10 max pts = 5 pts).\nIn addition to the example above, it means that when you have more users you will have a higher risk. In the case of MFA for administrative roles. The screenshot below shows, that we have 3 administrative users. For this action, the max score is 10 (fixed). In this situation, 10 is divided by 3 which makes the current score of 3.33 per user. Currently, there is 1 administrative user with MFA who makes a score of 3.33.\nIn the case of 4 users, the score per user is 2.5. So when you add administrative users with no MFA the score decrease.\nFor more information about score calculation, check the documentation.\nSecurity Improvement Actions overview As the name says we have to do something to improve your security. An action could be a rule like a user risk policy or enforcing MFA for administrative roles. Every role has its own score- and user impact, and implementation costs. Let‚Äôs say this is the overview layer.\nSecurity Improvement Actions detailed But there is more when looking into an action. It has some really interesting information, also from an automation perspective. Information about how and where to solve the issue, the implementation status, and the general action status. The last one is where it all goes about in this blog post on how to monitor secure score improvement actions. This is the detail layer.\nThe general status tells everything about YOUR action and the next steps. There are five statuses available: To Address, Planned, Risk Accepted, Resolved through third party and Resolved through alternate mitigation. I‚Äôm not going into detail about all these statuses. but pick the risk accepted.\nThe risk accepted is quite interesting status. Let‚Äôs take a look at what Microsoft tells about this status:\nWhen the risk accepted status is selected, the action is no longer visible in the portal. There could be a good reason the accept a risk but in any circumstance, you want to get notified about the fact someone has accepted the risk. To get this information I use the REST API. The API also provides who has accepted the risk and when. Let‚Äôs take a look in the next chapter at how to get this data.\nFor more information about the improvement actions, check the Microsoft documentation.\nMonitor Identity Secure Score improvement action status Below you see a screenshot of an improvement action and the details. As mentioned above, I divided an improvement action into two layers. The overview layer and the detail layer. In this chapter, I explain both layers of how to get data and give you some takeaways.\nGet improvement actions overview At first, we get the improvement actions from the overview layer. I use PowerShell for that in combination with the Graph API. To clarify I posted the API endpoint for secure score improvement actions below.\nhttps://graph.microsoft.com/beta/security/secureScoreControlProfiles After authenticating with the application, let‚Äôs search for actions. To keep my code clean I often work with the splatting technique. In fact, you create a table and push all the parameters as one single parameter. In the case of 3 parameters, it looks a bit overkill but if you have 5 or more it really helps.\nThe $authHeader variable is the output from the code in the authentication part. I used a filter to gather only the identity actions.\n$improvementsParams = @{ method = \u0026#34;GET\u0026#34; uri = \u0026#34;https://graph.microsoft.com/beta/security/secureScoreControlProfiles?`$filter=controlCategory eq \u0026#39;Identity\u0026#39;\u0026#34; headers = $authHeader } $improvements = Invoke-RestMethod @improvementsParams $improvements.value For more information about security profiles, check the Microsoft documentation.\nI store the output into a variable. Storing output into a variable helps you search for the content in further actions. Let‚Äôs take a look at what is in the variable. You see all the information out of the overview layer (title, implementation costs, and user impact). This is the overview of the Identity Secure Score in the AzureAD. If you look at the same actions from the security center you will get more overview information.\nGet improvement action details Now we have information. In the query above I already filtered the Identity actions out. Now it is time to look into the action details to get the current state and previous states. For the test, I changed the status to risk accepted and query the action again.\nTo query the action I only search for the UserRiskPolicy. This is the id from the output above. This is also the part when you are looking for specific actions only. After querying I look into the controlStateUpdates object. This is the place where the states are held.\n$parameters = @{ method = \u0026#34;GET\u0026#34; uri = \u0026#34;https://graph.microsoft.com/beta/security/secureScoreControlProfiles(\u0026#39;UserRiskPolicy\u0026#39;)\u0026#34; headers = $authHeader } $userRiskPolicy = Invoke-RestMethod @parameters $userRiskPolicy.controlStateUpdates Again, I stored the output in a variable that I need in the next steps. In the screenshot below you see I toggled the status a few times. Per switch, there is an object where you can see a new status, who changed that status, and when. Also, you need to provide a reason why you did change the status.\nFinally, this is the point where we need to be. Now we have the correct data, we can go back to the overview part. The part where we searched for all Identity actions. I picked the $improvents variable again where I search in the output for actions with an Ignored state. The state is stored in the controlStateUpdates object.\nI search for the last n statuses. If the ignored state (Risk Accepted) is in there, I will get the action back on my screen.\nIn my example below I search for the last 3 statuses. Normally you would say the last status is leading but I got no output due to all changes I did.\n$n = 3 $improvements.value | ForEach-Object { if (($_.controlStateUpdates.state | Select-Object -Last $n) -eq \u0026#39;Ignored\u0026#39;) { $_ #result to send anywhere } } From this point, you can choose what to do. Sending the output to any place you like.\nSummary To sum up the main parts of this blog post about how to monitor the secure score security improvement action status I explained what an improvement status is. I showed your actions depend on statuses in the portal. In the most not-ideal situation, an action gets a Risk Accepted state whereafter the action is out of sight. Especially when you have multiple environments.\nI showed how to get the improvement action status with the Graph API and how to search for Risk Accepted statuses.\nI hope you got a bit inspired.\nEnjoy your day and happy automating üëã\n","date":"May 29, 2022","image":"http://localhost:1313/monitor-identity-secure-score-security-improvement-action-status/identity-security-improvements.jpeg","permalink":"/monitor-identity-secure-score-security-improvement-action-status/","title":"Monitor Identity Secure Score security improvement action status"},{"categories":["Azure","DevOps"],"contents":"In this 4th slice of Prepare Azure DevOps for AVD deployment series, I will show how to copy or clone an existing YAML pipeline automated from a source project into a new AVD project. This will help you keep one main pipeline as a source and will allow you to create a ‚Äúlinked‚Äù pipeline into a new project directly from the source.\nIntroduction In this series about Prepare Azure DevOps for Azure Virtual Desktop deployment, I will post a few small blog posts which will help you set up an AVD prepared DevOps environment, fully automated. At the end of this series, you will be able to create a script that lets you fill in an application name, a project name, and a PAT code for connecting to DevOps and will run all the needed steps to start with DevOps.\nWhat we have so far:\nWe have created an App registration in Azure Active Directory which allows us to build a connection from DevOps to a subscription We have created an Azure DevOps project. This is the first step in DevOps (after you created an organization first) Then we create a Service connection in the DevOps project. This is the connection itself Finally, it is time to create the first pipeline into the project.\nTable Of Contents Introduction Why? Clone the YAML pipeline Code Explained Why? The first steps are pretty clear and need to get started in the first place. Now we are getting to a point where the discussion starts. First of all, there is no good or bad. Whatever you do, if it works for you it‚Äôs OK :).\nSo why do I create a pipeline in every new project instead of using one pipeline in one project with all the service connections within the project?\nWell, the answer is very simple in my opinion. A project represents a tenant and a connection to a subscription. A project could have more connections but all pointing to the same tenant.\nEvery tenant could have its parameters (in files) and properly also specific pipelines. So I‚Äôm creating a new project anyway. Now I have to manage only one service connection for a subscription.\nHope that will make things clear up a bit.\nClone the YAML pipeline We are creating an existing pipeline. That means we are creating a pipeline into a new project based on an exciting pipeline. The source always remains in the source repository. If you edit a pipeline it will be changed over all other pipelines based on this source.\nAnother side is when changing the pipeline in another than the source project then the pipeline will be also changed. To avoid that make sure you have set the correct permissions for end-users on the projects.\nCreating an existing pipeline through the portal is very simple. Go to the project, choose pipelines and create a new pipeline. Then select the source, for me Azure Repos Git, and select the correct repository.\nAfter selecting the correct repository choose the existing pipeline and we‚Äôre all set.\nHowever, this isn‚Äôt very hard it is always nice to play around with automation and it can help you create a new project fully automated.\nCode Explained We are creating the pipeline with a PowerShell script and some REST API. The script needs some basic information\nOrganization: This is the Azure DevOps organization and the first thing you need. Please check About organization management in Azure DevOps about creating an Azure DevOps organization or go to https://dev.azure.com.\nPersonalToken: A personal access token (PAT) is used as an alternate password to authenticate into Azure DevOps. You are able to create a PAT under your own personal account. Want to learn how to create, use, modify, and revoke PATs for Azure DevOps please check to Authenticate with personal access tokens.\nCopyToProjectName: This is the project where to copy the pipeline.\nsource project: This is de source project.\nPipelineName: Which pipeline do you like to copy.\nParam( [Parameter(Mandatory)] [string]$personalToken¬†=¬†\u0026#39;\u0026#39;, [Parameter(Mandatory)] [string]$organisation¬†=¬†\u0026#39;\u0026#39;, [Parameter(Mandatory)] [string]$CopyToProjectName¬†=¬†\u0026#39;\u0026#39;, [Parameter(Mandatory)] [string]$SourceProject¬†=¬†\u0026#39;\u0026#39;, [Parameter(Mandatory)] [string]$PipelineName¬†=¬†\u0026#39;\u0026#39; ) # Creating a header for autorisation $token¬†=¬†[System.Convert]::ToBase64String([System.Text.Encoding]::ASCII.GetBytes(\u0026#34;:$($personalToken)\u0026#34;)) $header¬†=¬†@{authorization¬†=¬†\u0026#34;Basic¬†$token\u0026#34;¬†} In this first API call, we are requesting the pipeline settings from the source project. In the second call, we are gathering all the pipeline information and storing it into a result variable.\n$GetDefinitionsUrl¬†=¬†\u0026#34;https://dev.azure.com/$organisation/$SourceProject/_apis/build/definitions?name={$PipelineName}api-version=5.0\u0026#34; $Parameters¬†=¬†@{ Uri¬†=¬†$GetDefinitionsUrl Method¬†=¬†\u0026#34;GET\u0026#34; Headers¬†=¬†$header ContentType¬†=¬†\u0026#34;application/json\u0026#34; Erroraction¬†=¬†\u0026#34;Stop\u0026#34; } $Defintions¬†=¬†Invoke-RestMethod¬†@Parameters $NeededDefinitionId¬†=¬†$GetDefinitionsUrl.id $DefinitionUrl¬†=¬†\u0026#34;https://dev.azure.com/$organisation/$SourceProject/_apis/build/definitions/$NeededDefinitionId`?api-version=4.1\u0026#34; $Parameters¬†=¬†@{ Uri¬†=¬†$DefinitionUrl Method¬†=¬†\u0026#34;GET\u0026#34; Headers¬†=¬†$Header ContentType¬†=¬†\u0026#34;application/json\u0026#34; Erroraction¬†=¬†\u0026#34;Stop\u0026#34; } $Result¬†=¬†Invoke-RestMethod¬†@Parameters The next few lines of code will remove some unnecessary information like unique IDs, creation date, etc.\n$values¬†=¬†@(\u0026#34;id\u0026#34;,¬†\u0026#34;url\u0026#34;,¬†\u0026#34;uri\u0026#34;,¬†\u0026#34;revision\u0026#34;,¬†\u0026#34;createdDate\u0026#34;,¬†\u0026#34;project\u0026#34;) foreach¬†($value¬†in¬†$values)¬†{ $Result.PSObject.Properties.Remove($value) } At last, the new result will be used as a body to create a pipeline into the new project.\n$CloneToUrl¬†=¬†\u0026#34;https://dev.azure.com/$organisation/$CopyToProjectName/_apis/build/definitions?api-version=4.1\u0026#34; $Parameters¬†=¬†@{ Uri¬†=¬†$CloneToUrl Method¬†=¬†\u0026#34;Post\u0026#34; Body¬†=¬†($Result¬†|¬†ConvertTo-Json¬†-Depth¬†3) Headers¬†=¬†$Header ContentType¬†=¬†\u0026#34;application/json\u0026#34; Erroraction¬†=¬†\u0026#34;Stop\u0026#34; } $Result¬†=¬†Invoke-RestMethod¬†@Parameters Thank you for reading my post about how to clone a YAML pipeline from a source project automated. Hopefully, it will help you.\n","date":"May 16, 2022","image":"http://localhost:1313/the-devops-project-clone-a-pipeline-from-a-source-project-automated/slices.png","permalink":"/the-devops-project-clone-a-pipeline-from-a-source-project-automated/","title":"The DevOps Project Clone a pipeline from a source project automated"},{"categories":["Azure","Microsoft Intune","Monitoring"],"contents":"Out of the box, Microsoft Intune has a reporting platform where lots of information is stored. Think about device management or endpoint analytics. For troubleshooting, reports can help. However, if you need to troubleshoot you are too late in the first place. It would be nice to get alerts out of Microsoft Intune. Luckily, there are options to achieve that goal. In this post, I show a way to monitor and get alerts from Microsoft Intune.\nTo get alerts we have several methods. Microsoft Intune has the ability to connect with an Azure Log Analytics workspace (LAWS). Based on LAWS I use the Azure Monitor. To monitor and get alerting from Microsoft Intune I used the strategy below.\nTable Of Contents Authentication Microsoft Intune diagnostics settings Create a Log Analytics Workspace automated Configure diagnostics settings for Microsoft Intune automated Using Azure Monitor for monitoring Microsoft Intune Create a monitor action group automated Monitor MEM policy deletion with Log Analytics Create an alert rule manually Summary Authentication I created an application registration with a Contributor role at the subscription level. This is because I want to create Azure resources. A resource group, a Log Analytics workspace, alert rules, and an action group If you have a resource group already, a contributor role at the resource group level is also fine.\nIt is also possible to log in with an account with contributor permissions. The reason why I use an application registration is because of the following:\nOne of my automation ‚Äòrules‚Äô is trying to authenticate as little as possible. In this case, we are dealing with two environments. Microsoft 365 (Graph API) and Azure I‚Äôm able to use one login for both worlds. That is the point where I‚Äôm using application registrations a lot. Application registrations can have an Azure role as API permissions.\nMicrosoft Intune diagnostics settings In the basics, Microsoft Intune has the ability to send diagnostic information to an Azure log analytics workspace (LAWS). You can find the Diagnostics settings under Tenant admin.\nThe information contains data about the following subjects:\nAudit Logs Operational Logs Device Compliance Org Devices In this chapter, I‚Äôm going to find out what information is exactly sent and how we can use the data for alerting. First, let‚Äôs create a log analytics workspace automated.\nCreate a Log Analytics Workspace automated Before creating a workspace, we need a resource group first. Creating a resource group is quite simple. Providing a location and a name is enough.\n$subscriptionId = [Microsoft.Azure.Commands.Common.Authentication.Abstractions.AzureRmProfileProvider]::Instance.Profile.DefaultContext.Subscription.id $resourceGroupName = \u0026#39;rg-mem-monitoring\u0026#39; $location = \u0026#34;West Europe\u0026#34; $url = \u0026#34;https://management.azure.com/subscriptions/{0}/resourcegroups/{1}?api-version=2021-04-01\u0026#34; -f $subscriptionId, $resourceGroupName $body = @{ location = $location } $parameters = @{ uri = $url method = \u0026#39;PUT\u0026#39; header = $authHeader body = $body | ConvertTo-Json } $resourceGroup = Invoke-RestMethod @Parameters After the resource group is created use the code below to create a log analytics workspace. A workspace must have a unique name in a resource group. By using the Get-Random PowerShell command the workspace name will be completed with a random number.\n$workspace = \u0026#34;la-mem-workspace\u0026#34; + (Get-Random -Maximum 99999) $LawsBody = @{ location = $location properties = @{ retentionInDays = \u0026#34;30\u0026#34; sku = @{ name = \u0026#34;PerGB2018\u0026#34; } } } $lawsUrl = \u0026#34;https://management.azure.com/{0}/providers/Microsoft.OperationalInsights/workspaces/{1}?api-version=2020-08-01\u0026#34; -f $resourceGroup.id, $workspace $loganalyticsParameters = @{ URI = $lawsUrl Method = \u0026#34;PUT\u0026#34; Body = $LawsBody | ConvertTo-Json Headers = $authHeader } $laws = Invoke-RestMethod @loganalyticsParameters $laws Configure diagnostics settings for Microsoft Intune automated Next is configuring diagnostics settings Microsoft Intune automated. In this step, I also use the Azure management API. Yes, it is a setting on the Microsoft 365 side but this is the place where M365 and Azure come together. But there are some settings that need attention.\nFirst, let‚Äôs take a look at the diagnostics REST API with a main focus on the create URL.\nhttps://management.azure.com/{resourceUri}/providers/Microsoft.Insights/diagnosticSetting\nIn this URL two environmental values are needed. A resource URI (ResourceID) and a name. The name is the diagnostics name (red box).\nSecondly, is the resource URI. This is the Azure resource ID. This is the point where we need to look closer. Because Microsoft Intune (Intune) is not an Azure resource. After some digging, I noticed there is a way to configure diagnostic settings for Intune. This is by calling the microsoft.intune resource provide directly. It looks like the whole Intune site is a complete resource in the Azure cloud.\nWhat happens is that we are configuring diagnostic settings over a whole resource provider. In contrast to the mentioned URL above with a resource ID, the diagnostics settings REST API URL for Intune looks like the URL below.\nhttps://management.azure.com/providers/microsoft.intune/diagnosticSettings/{name}\nWith that in mind, I configured a body with logs and added the workspace ID to connect with.\n$diagnosticsName = \u0026#34;mem\u0026#34; $url = \u0026#34;https://management.azure.com/providers/microsoft.intune/diagnosticSettings/{0}?api-version=2017-04-01-preview\u0026#34; -f $diagnosticsName $body = @{ properties = @{ workspaceid = $laws.id logs = @( @{ category = \u0026#34;AuditLogs\u0026#34; enabled = $true }, @{ category = \u0026#34;OperationalLogs\u0026#34; enabled = $true }, @{ category = \u0026#34;DeviceComplianceOrg\u0026#34; enabled = $true }, @{ category = \u0026#34;Devices\u0026#34; enabled = $true } ) } } $diagParam = @{ URI = $url Method = \u0026#34;PUT\u0026#34; Headers = $authHeader Body = $($body | ConvertTo-Json -Depth 5) } $diagnostics = Invoke-RestMethod @diagParam $diagnostics In the end, the diagnostics settings are configured.\nFor more information about the diagnostics settings, check the Microsoft documentation: https://docs.microsoft.com/en-us/rest/api/monitor/diagnostic-settings\nUsing Azure Monitor for monitoring Microsoft Intune To monitor and get alerts from Microsoft Intune we configure Azure Monitor. In the chapters above we configured a Log Analytics workspace and diagnostics settings in MEM. In this chapter, I give monitor some examples and send an alert with Azure Monitor. Before we are able to send alerts we need two things. An alert rule and an action group. The last one is the first thing we create. I create an action group that sent an e-mail to me, the most basic action group.\nAs a result of configuring diagnostics settings in MEM, we see new tables in the workspace.\nTo get data out of a workspace, we need to write a query in the Kusto language (KQL). In the examples below I have some basic queries to clarify the idea.\nCreate a monitor action group automated If we have an alert we need to send it somewhere. That somewhere is an action group. An action group can send an e-mail or an SMS. Also, an action group can send a voice message of an Azure app alert. Besides device notifications, an action group also can trigger Azure resources. Resources like a function app or logic app.\nFor now, I have chosen to send an e-mail. The code below creates a SendMail action group that sends an e-mail. The $actionGroup variable is used in the next step.\n$actionName = \u0026#34;SentMail\u0026#34; $actionUrl = \u0026#34;https://management.azure.com/{0}/providers/Microsoft.Insights/actionGroups/{1}?api-version=2021-09-01\u0026#34; -f $resourceGroup.id, $actionName $actionBody = @{ location = \u0026#34;Global\u0026#34; properties = @{ enabled = $true groupShortName = \u0026#34;SentMail\u0026#34; emailReceivers = @( @{ name = \u0026#34;MailToMe\u0026#34; emailAddress = \u0026#34;sander@email.com\u0026#34; status = \u0026#34;Enabled\u0026#34; useCommonAlertSchema = $true } ) } } $actionGroupParams = @{ URI = $actionUrl Method = \u0026#34;PUT\u0026#34; Headers = $authHeader Body = $actionBody | ConvertTo-Json -Depth 4 } $actionGroup = Invoke-RestMethod @actionGroupParams $actionGroup Monitor MEM policy deletion with Log Analytics First I wrote an audit query to determine when a policy was deleted and by who. Idea is to get notified about the fact someone has deleted a policy. This can be a security baseline profile, a configuration policy or a compliance policy.\nIntuneAuditLogs | extend PropertiesObject = parse_json(Properties) | extend actorObject = parse_json(PropertiesObject.Actor) | extend targetsObject = parse_json(PropertiesObject.TargetDisplayNames) | extend activityString = case( PropertiesObject.ActivityType == 0, \u0026#34;Create\u0026#34;, PropertiesObject.ActivityType == 1, \u0026#34;Delete\u0026#34;, PropertiesObject.ActivityType == 2, \u0026#34;Patch\u0026#34;, PropertiesObject.ActivityType == 3, \u0026#34;Action\u0026#34;, PropertiesObject.ActivityType == 4, \u0026#34;SetReference\u0026#34;, PropertiesObject.ActivityType == 5, \u0026#34;RemoveReference\u0026#34;, PropertiesObject.ActivityType == 6, \u0026#34;Get\u0026#34;, PropertiesObject.ActivityType == 7, \u0026#34;Search\u0026#34;, \u0026#34;Other\u0026#34; ) | project PolicyName=targetsObject[0],TimeGenerated, TenantId, SourceSystem, OperationName, activityString, ResultType, Properties, UPN=actorObject.UPN | where activityString == \u0026#34;Delete\u0026#34; The results show there are two policies deleted. A compliance policy and a management intent (which is a security baseline profile). At the moment this happens an alert must be sent.\nThe query works and the results are correct. Now it is time to create an Azure Monitor rule automated. Also in this situation, I use the REST API.\nTo keep my code clean, I firstly store the query into a variable.\n$query = @\u0026#34; IntuneAuditLogs | extend PropertiesObject = parse_json(Properties) | extend actorObject = parse_json(PropertiesObject.Actor) | extend targetsObject = parse_json(PropertiesObject.TargetDisplayNames) | extend activityString = case( PropertiesObject.ActivityType == 0, \u0026#34;Create\u0026#34;, PropertiesObject.ActivityType == 1, \u0026#34;Delete\u0026#34;, PropertiesObject.ActivityType == 2, \u0026#34;Patch\u0026#34;, PropertiesObject.ActivityType == 3, \u0026#34;Action\u0026#34;, PropertiesObject.ActivityType == 4, \u0026#34;SetReference\u0026#34;, PropertiesObject.ActivityType == 5, \u0026#34;RemoveReference\u0026#34;, PropertiesObject.ActivityType == 6, \u0026#34;Get\u0026#34;, PropertiesObject.ActivityType == 7, \u0026#34;Search\u0026#34;, \u0026#34;Other\u0026#34; ) | project PolicyName=targetsObject[0],TimeGenerated, TenantId, SourceSystem, OperationName, activityString, ResultType, Properties, UPN=actorObject.UPN | where activityString == \u0026#34;Delete\u0026#34; \u0026#34;@ Secondly, I create a body to send in the request. In the body, I use two variables. The $query variable (from above) and the $laws.Id variable from the workspace creation part.\n$ruleBody = @{ location = \u0026#34;westeurope\u0026#34; properties = @{ displayName = \u0026#34;MEM policy check for deletion\u0026#34; description = \u0026#34;Query which checks policy deletion every 5 minutes\u0026#34; severity = 0 enabled = $true evaluationFrequency = \u0026#34;PT5M\u0026#34; scopes = @( $diagnostics.properties.workspaceId ) windowSize = \u0026#34;PT5M\u0026#34; criteria = @{ allOf = @( @{ query = $query timeAggregation = \u0026#34;Count\u0026#34; operator = \u0026#34;GreaterThan\u0026#34; threshold = 0.0 failingPeriods = @{ numberOfEvaluationPeriods = 1 minFailingPeriodsToAlert = 1 } } ) } actions = @{ actionGroups = @( $actionGroup.id ) } } } At last, we bring all parts together in the request parameters and sent the request.\n$ruleName = \u0026#34;MEM check for deletion\u0026#34; $url = \u0026#34;https://management.azure.com/{0}/providers/Microsoft.Insights/scheduledQueryRules/{1}?api-version=2021-08-01\u0026#34; -f $resourceGroup.id, $ruleName $ruleParam = @{ URI = $url Method = \u0026#34;PUT\u0026#34; Headers = $authHeader Body = $ruleBody | ConvertTo-Json -Depth 8 } $rules = Invoke-RestMethod @ruleParam $rules https://docs.microsoft.com/en-us/rest/api/monitor/scheduledqueryrule-2021-08-01/scheduled-query-rules/create-or-update\nCreate an alert rule manually To create an alert rule via the Azure portal go to the Log Analytics workspace and click on alerts in the menu on the left.\nFor more information about creating rules manually check the documentation: https://docs.microsoft.com/en-us/azure/azure-monitor/alerts/alerts-activity-log\nhttps://docs.microsoft.com/en-us/mem/intune/fundamentals/review-logs-using-azure-monitor\nSummary Thank you for reading this blog about how to monitor and get alerts for Microsoft Intune with Log Analytics automated. I hope you got a bit inspired.\nEnjoy your day and happy automating üëã\n","date":"April 19, 2022","image":"http://localhost:1313/monitor-and-get-alerts-from-microsoft-endpoint-manager/remote-monitoring-and-alerting.jpg","permalink":"/monitor-and-get-alerts-from-microsoft-endpoint-manager/","title":"Monitor and get alerts from Microsoft Intune"},{"categories":["Microsoft Intune","Monitoring","Security"],"contents":"A security baseline has some values which are interesting to know. It has a status that tells us the current deployment status if a profile is assigned or is deprecated. In this post, I show how to monitor security baselines. The current status, versions, and if a profile is assigned. I will explain which data is important, how to get the data and show different ways to send alerts.\nTo understand security baselines please read my security baseline explained post first. This post is going further on that base.\nTable Of Contents Security baselines data Application and permissions Authenticate to Graph API with PowerShell Get security baseline data Monitor security baselines Monitoring strategy Security baseline version monitoring Security baseline assignment monitoring Security baseline status monitoring Summary Security baselines data Security baselines have some important data which need attention. In this chapter, I will show why this data is important and how to get this data. As shown in my earlier post, security baselines are templates with predefined settings. A security baseline consists of profiles with versions. Based on the settings you are able to create a profile. The profile can be assigned to a group, all users, or all devices. After assignment, a profile gets a status.\nThose three values are important and tell if a security baseline is working fine and is up-to-date.\nApplication and permissions To get security baseline data, we use the Graph API. Before getting data from the Grapp API we need to authenticate first. For authentication, I use an application registration in the Azure Active Directory. This application has only read permissions at the application level:\nDeviceManagementConfiguration.Read.All\nThereafter go to certificates \u0026amp; secrets and create an application secret.\nAuthenticate to Graph API with PowerShell After we have created an application registration with the correct permissions, it is time to authenticate. To authenticate to Graph API with PowerShell I used the code below. Use the just created application‚Äôs ID and secret. Also, make sure you have the tenant ID. Find the tenant ID under the application‚Äôs overview.\nWe are able to log in now.\n$secMonAppId = \u0026#34;xx\u0026#34; $secMonAppSecret = \u0026#34;xx\u0026#34; $tenantId = \u0026#34;xx\u0026#34; $body = @{ grant_Type = \u0026#34;client_credentials\u0026#34; scope = \u0026#34;https://graph.microsoft.com/.default\u0026#34; client_Id = $secMonAppId client_Secret = $secMonAppSecret } $connectParams = @{ uri = \u0026#34;https://login.microsoftonline.com/{0}/oauth2/v2.0/token\u0026#34; -f $tenantId method = \u0026#34;POST\u0026#34; body = $Body } $connect = Invoke-RestMethod @connectParams $authHeader = @{ \u0026#39;Content-Type\u0026#39; = \u0026#39;application/json\u0026#39; Authorization = \u0026#39;Bearer \u0026#39; +$connect.access_token } Get security baseline data Next, after authentication, we get security baseline data. I used the code below to get security baseline data and saved the output in a variable (we need that later).\n$getParams = @{ method = \u0026#34;GET\u0026#34; uri = \u0026#34;https://graph.microsoft.com/beta/deviceManagement/templates\u0026#34; headers = $authHeader } $templates = Invoke-RestMethod @getParams $templates.value How do you know when a new version becomes available? Let‚Äôs take a look closer at the content below. This content is an output of a security baseline.\n@odata.type : #microsoft.graph.securityBaselineTemplate id : 034ccd46-190c-4afc-adf1-ad7cc11262eb displayName : MDM Security Baseline for Windows 10 and later for November 2021 description : MDM Security Baseline for Windows 10 and later versionInfo : November 2021 isDeprecated : False intentCount : 3 templateType : securityBaseline platformType : windows10AndLater templateSubtype : none publishedDateTime : 22/10/2021 00:00:00 As you can see there are some values that tell us something about the version. At first, the version name is in the display name. Also, we have the version info object, which tells us the version is from November 2021. Also, we have the publishedDateTime object. The published data is on 22 October 2021. A change in one of these objects indicated there was a change.\nAnother interesting object is the isDeprecated object. The False value means NOT deprecated. When the value becomes True the version is deprecated and not up-date-date anymore.\nAfter doing some research I also found out that the security baseline‚Äôs ID is unique. Unique in that way that when a new version becomes available also the ID will change.\nMonitor security baselines In the upcoming paragraphs, I will show how to get the correct data and how to use it. After we have the data I will show some alerting mechanisms.\nMonitoring strategy The idea is to use the baseline information (from above) as the source. The source information has an object called intentCount. When the intent count value is greater than 0, it means the baseline has profiles. If a baseline has profiles then we need to look for the profiles.\nAfter requesting the profiles I use the output to compare with the source baseline.\nSecurity baseline version monitoring As mentioned in my security baseline explain post, security baselines have versions. A version represents a baseline with settings and values. Each new version instance of a baseline can add or remove settings or introduce other changes. Because baselines are not updating to new versions automatically, it is a good idea to monitor your own baselines as Microsoft‚Äôs baseline. An older version remains to work but it is a best practice to update profiles to the latest versions as soon as possible.\nI asked the Microsoft product team what is happening when a new version becomes available. One of the things which happen is the change of the security baseline ID. Every security baseline version has its own ID. With that in mind, we could make a match between the security baseline ID and the templateID object from the profile.\nI used the code below, to get the profiles. I reused the authentication header from the first chapter.\n$getPolicies = @{ method = \u0026#34;GET\u0026#34; uri = \u0026#34;https://graph.microsoft.com/beta/deviceManagement/intents\u0026#34; headers = $authHeader } $policies = Invoke-RestMethod @getPolicies $policies.value In the output below I have the security baseline and a profile in the baseline. In the profile output, you see a templateID. This is the ID of the source template. In the case of a new version, the ID value of the template will change. In that case, there won‚Äôt be a match which means the source template has changed.\nAt this moment of writing, there is only one version available at every security baseline.\nSecurity baseline assignment monitoring A security baseline profile only makes sense when it is assigned. Otherwise, an ‚Äôempty‚Äô baseline profile is quite useless. From another perspective, I want to keep my environment as clean as possible. I want to know if a profile is assigned.\nMonitoring assignment in basics is not very hard. When requesting profiles you will notice there is an isAssigned object. The true value means the profile is assigned.\nI‚Äôm also working on a blog post about advanced assignment monitoring. In this blog, I will show how to get more detailed assignment information.\nSecurity baseline status monitoring This monitoring rule differs from the other two above. This is because a security baseline profile has no status in itself. Monitoring security baseline statuses is at the device level. A profile in combination with a device has several status events. It tells if a profile is active or has some errors. To monitor the status we have to look into the profile.\nA profile can have the following statuses:\nSucceeded: Policy is applied. Error: The policy failed to apply. The message typically displays with an error code that links to an explanation. Conflict: Two settings are applied to the same device, and Intune can‚Äôt sort out the conflict. An administrator should review. Not applicable: The device can‚Äôt receive the policy. For example, the policy updates a setting specific to the latest version of Windows, but the device runs an older (earlier) version that doesn‚Äôt support that setting Pending is also a security baseline profile status but is not added to the global profile overview. But with the Graph API, this status is catchable.\nPending: The device hasn‚Äôt checked in with Intune to receive the policy yet. Source: https://docs.microsoft.com/en-us/mem/intune/protect/security-baselines-monitor\nTo get profile statuses you need the profile ID. The ID is retrieved with the code below.\n$getParams = @{ method = \u0026#34;GET\u0026#34; uri = \u0026#34;https://graph.microsoft.com/beta/deviceManagement/intents?`$filter=displayName eq \u0026#39;Win11 - Security Baseline Automated\u0026#39;\u0026#34; headers = $authHeader } $templates = Invoke-RestMethod @getParams $templates.value The template output is used below to read the device statuses per profile.\n$profileId = $templates.value.id $getStatus = @{ method = \u0026#34;GET\u0026#34; uri = \u0026#34;https://graph.microsoft.com/beta/deviceManagement/intents/{0}/deviceStates\u0026#34; -f $profileId headers = $authHeader } $status = Invoke-RestMethod @getStatus $status.value In the case below we have a pending state.\nSummary In this post, I showed how to get data to monitor security baseline profiles.\nThank you for reading this blog about how to monitor security baselines in Endpoint Security. I hope you got a bit inspired.\nEnjoy your day and happy automating üëã\n","date":"April 5, 2022","image":"http://localhost:1313/monitor-security-baselines-in-endpoint-security/Monitoring-security.jpg","permalink":"/monitor-security-baselines-in-endpoint-security/","title":"Monitor security baselines in Endpoint Security"},{"categories":["Graph API","Azure Virtual Desktop","Microsoft Intune"],"contents":"Azure Virtual Desktop (AVD) is a great addition to your IT environment. To connect to an AVD environment you need a browser or the Remote Desktop client. When using the browser you have to go to the ‚Äòrdweb‚Äô-URL. When using the remote desktop client, you need to subscribe to a workplace. In this blog post, I show how to configure the subscribe URL for AVD automated. How to configure this setting in MEM automated. Also, I show a way to deploy the setting if you are not using MEM.\nThis specific setting became available on 7 March 2022 as a new setting in the settings catalog. The main idea of this post is to show some ways how to configure auto subscribe for AVD in the remote desktop client. This is the URL that you need to subscribe to in the case you are using Azure Virtual Desktop. The AVD subscribe URL is https://rdweb.wvd.microsoft.com/api/arm/feeddiscovery. Till the setting became available in MEM we had to configure this in other ways (later in this blog). Now the setting is available I show how to configure a configuration policy with the Remote Desktop ‚Äì Auto subscription URL automated.\nTable Of Contents AVD Auto subscription with Microsoft Intune Auto subscribe policy settting Auto subscription configuration on Azure VM (for image management) Auto subscribe without MEM or in image management Summary AVD Auto subscription with Microsoft Intune In this first chapter, I show how to deploy a configuration policy with the Remote Desktop Auto Subscribe URL automated. The policy is assigned to all devices. Before we are able to deploy the AVD auto subscribe policy automated we need to authenticate first. In earlier posts, I described how to authenticate based on an application registration. For now, I skip that part. The following permissions are needed:\nDeviceManagementConfiguration.ReadWrite.All\n(Allows the app to read and write properties of Microsoft Intune-managed device configuration and device compliance policies and their assignment to groups, without a signed-in user.) Auto subscribe policy settting The auto subscribe setting is quite simple. The settings have an input field where to store an URL. I searched for this setting at the back and created a JSON file which I provide in the body in the next step. The JSON file is in my GitHub repository.\n{ \u0026#34;id\u0026#34;: \u0026#34;0\u0026#34;, \u0026#34;settingInstance\u0026#34;: { \u0026#34;@odata.type\u0026#34;: \u0026#34;#microsoft.graph.deviceManagementConfigurationSimpleSettingCollectionInstance\u0026#34;, \u0026#34;settingDefinitionId\u0026#34;: \u0026#34;user_vendor_msft_policy_config_remotedesktop_autosubscription\u0026#34;, \u0026#34;settingInstanceTemplateReference\u0026#34;: null, \u0026#34;simpleSettingCollectionValue\u0026#34;: [ { \u0026#34;@odata.type\u0026#34;: \u0026#34;#microsoft.graph.deviceManagementConfigurationStringSettingValue\u0026#34;, \u0026#34;settingValueTemplateReference\u0026#34;: null, \u0026#34;value\u0026#34;: \u0026#34;https://rdweb.wvd.microsoft.com/api/arm/feeddiscovery\u0026#34; } ] } } In the post body below I set up the name and description. The platforms and technologies objects are Microsoft predefined settings. The settings object is the JSON content file. I convert the content back to a PowerShell object first. This is to create a request body with PowerShell objects only. In the end, I convert the whole body into a JSON object.\nIf you are converting JSON content twice you get really strange content that the REST API is not accepting.\n$policyBody = @{ \u0026#34;@odata.type\u0026#34; = \u0026#34;#microsoft.graph.deviceManagementConfigurationPolicy\u0026#34; \u0026#34;name\u0026#34; = \u0026#34;AVD - Auto subscription settings\u0026#34; \u0026#34;description\u0026#34; = \u0026#34;Settings for AVD\u0026#34; \u0026#34;platforms\u0026#34; = \u0026#34;windows10\u0026#34; \u0026#34;technologies\u0026#34; = \u0026#34;mdm\u0026#34; \u0026#34;settings\u0026#34; = @( Get-Content ./avd-autosubscription-settings.json | ConvertFrom-Json ) } $policyUrl = \u0026#34;https://graph.microsoft.com/beta/deviceManagement/configurationPolicies\u0026#34; $policyParams = @{ URI = $policyUrl Method = \u0026#34;POST\u0026#34; Headers = $authHeader Body = $policyBody | ConvertTo-Json -Depth 99 } $policy = Invoke-RestMethod @policyParams $policy In the end, I have a policy with the auto-subscribe URL.\nAuto subscription configuration on Azure VM (for image management) Most customers are not using an Azure VM as a client. But in some cases, I use an Azure VM for image management. If you are not using Microsoft Intune (MEM) or have an Azure virtual machine, there is another option to configure the auto subscription setting. At the back also MEM adds a registry key with a value. Thank you Tom Hickling for providing the key.\nHKEY_CURRENT_USER\\Software\\Policies\\Microsoft\\Windows NT\\Terminal Services\\AutoSusbscription\nReg_SZ: https://rdweb.wvd.microsoft.com/api/arm/feeddiscovery\nI‚Äôve created a PowerShell script to run on the Azure virtual machine. (NOTE: These machines are not the AVD session host but normal virtual machines)\nYou are able to run the script during imaging or on the virtual machine in later steps. In this example, I use the RunCommands REST API command to run the script on an Azure virtual machine. If you use the Invoke-AzVmRunCommand, make sure you have stored the file on the device from where the command is started. The script I created is at my GitHub repository.\nTo use the Azure management API we also need to authenticate. This time we need to authenticate in Azure. In PowerShell use the Connect-AzAccount command. Make sure you log in with an account with at least a Virtual Machine Contributor role. https://docs.microsoft.com/en-us/azure/virtual-machines/windows/run-command#limiting-access-to-run-command\n$vm = Get-Azvm -ResourceGroupName \u0026#34;rg-vms\u0026#34; -Name \u0026#34;vm-01\u0026#34; $runCommandName = \u0026#34;ConfigureAutoAVDSubscribe\u0026#34; $url = \u0026#34;https://management.azure.com/{0}/runCommands/{1}?api-version=2021-04-01\u0026#34; -f $vm.Id, $runCommandName $body = @{ location = $vm.Location properties = @{ source = @{ scriptUri = \u0026#34;https://raw.githubusercontent.com/srozemuller/AVD/main/Auto%20subscription/auto-subscription.ps1\u0026#34; commandId = \u0026#34;RunPowerShellScript\u0026#34; } } } | ConvertTo-Json $postParams = @{ uri = $url method = \u0026#34;PUT\u0026#34; body = $body headers = $authHeader } Invoke-RestMethod @postParams The PowerShell script has been downloaded on the virtual machine after executing the command. Thereafter the PowerShell is executed.\nUse the URL to get the status for monitoring.\n$getParams = @{ uri = $url method = \u0026#34;GET\u0026#34; headers = $authHeader } Invoke-RestMethod @getParams ! avd-autosubscribe-registry\nAuto subscribe without MEM or in image management There could be also a situation where Microsoft Intune is not involved. Because you are managing endpoints in another way or in the case of creating images for autopilot or in the Azure Compute gallery. In that case, it is just a matter of executing the PowerShell script on the device. The script is in my GitHub repository.\nWhen creating images with platforms Azure DevOps add the script in a YAML task and add it to your sequence. I‚Äôve added a short example below.\n- task: AzurePowerShell@5 name: AVD_AutoSubscribe displayName: Deploying AVD auto subscribe URL enabled: true inputs: azureSubscription: ${{ parameters.ServiceConnection }} ScriptType: \u0026#39;FilePath\u0026#39; ScriptPath: \u0026#34;$(Pipeline.Workspace)/auto-subscription.ps1\u0026#34; errorActionPreference: \u0026#39;stop\u0026#39; azurePowerShellVersion: \u0026#39;LatestVersion\u0026#39; Summary In this blog post, I show different ways to configure the AVD subscription URL automated. I showed how to configure this setting in Microsoft Intune, as on an Azure virtual machine. Also, I showed an example of how to embed the configuration script into a YAML DevOps pipeline.\nThank you for reading this blog about how to configure auto subscribe for AVD in the remote desktop client automated.\nThank you for reading my blog configure subscribe to avd in rdp client automated. I hope you got a bit inspired. Enjoy your day and happy automating üëã ","date":"March 8, 2022","image":"http://localhost:1313/configure-subscribe-to-avd-in-rdp-client-automated/subscribe-now.jpeg","permalink":"/configure-subscribe-to-avd-in-rdp-client-automated/","title":"Configure subscribe to AVD in RDP Client automated"},{"categories":["Graph API","Automation","Microsoft Intune","Powershell"],"contents":"Climate changes are the talk of the day. Every little step to help the world is great. Power consumption is a big topic in those discussions. In this blog post, I show how to deploy power management settings automated in Microsoft Intune. This is to save battery on hardware devices. I also show how to assign the policy to the devices part with a filter.\nTogether with Mattias Melkersen (Twitter), we decided to write a blog about how to configure power settings in Microsoft Intune. Matthias has written a blog about how to configure power settings in the portal. Based on that post, I‚Äôm writing a post about how to configure power settings the automated way.\nTable Of Contents Authentication Deploy power settings policy automated Power management settings Settings body Deploy configuration policy automated Power settings assignment with filters Summary Authentication To deploy settings in Microsoft Intune we need to authenticate. During deployment, I use the Graph API. To authenticate against the Graph API we need API permissions. The API permissions are set at an application registration. The application registration needs the following permissions:\nDeviceManagementConfiguration.ReadWrite.All\n(Allows the app to read and write properties of Microsoft Intune-managed device configuration and device compliance policies and their assignment to groups, without a signed-in user.) To authenticate against the Graph API I used the PowerShell code below:\n$appId = \u0026#34;077d124e\u0026#34; $appSecret = \u0026#34;0UF7Q\u0026#34; $tenantId = \u0026#34;dag7\u0026#34; $body = @{ grant_Type = \u0026#34;client_credentials\u0026#34; scope = \u0026#34;https://graph.microsoft.com/.default\u0026#34; client_Id = $appId client_Secret = $appSecret } $connectParams = @{ uri = \u0026#34;https://login.microsoftonline.com/{0}/oauth2/v2.0/token\u0026#34; -f $tenantId method = \u0026#34;POST\u0026#34; body = $body } $connect = Invoke-RestMethod @connectParams $authHeader = @{ \u0026#39;Content-Type\u0026#39; = \u0026#39;application/json\u0026#39; Authorization = \u0026#39;Bearer \u0026#39; +$connect.access_token } If you are not that familiar with REST API authentication then it also possible to use the PowerShell command Get-AzAccessToken. To use that command you need to connect to Azure first with Connect-AzAccount. I also connect with the application. If you use these commands, install the Az.Accounts PowerShell module.\n$passwd = ConvertTo-SecureString $appSecret -AsPlainText -Force $pscredential = New-Object System.Management.Automation.PSCredential($appId, $passwd) Connect-AzAccount -ServicePrincipal -Credential $pscredential -Tenant $tenantId $token = Get-AzAccessToken -ResourceUrl \u0026#39;https://graph.microsoft.com\u0026#39; $authHeader = @{ \u0026#39;Content-Type\u0026#39; = \u0026#39;application/json\u0026#39; Authorization = \u0026#39;Bearer \u0026#39; + $token.Token } Deploy power settings policy automated In this chapter, we configure a configuration policy named Windows ‚Äì Power management settings. The policy is created with the Graph API. In the first part, I discuss the settings and how it is formatted. In the second part, I show how to bring all parts together and sent the request to the Graph API.\nPower management settings A configuration policy consists of settings. Within this policy, I configure the following settings based on the settings catalog.\nThe settings are based on Mattias Melkersen his blogpost and Kenneth Van Surksum his blog post (scroll down to the Power Settings chapter) about power management. I also added some extra settings which I think also helps.\nCategory Setting Location Setting Value General Administrative Templates \u0026gt; System \u0026gt; Power Management Select an active power plan Enabled General Administrative Templates \u0026gt; System \u0026gt; Power Management Active Power Plan: (Device) Automatic Power Administrative Templates \u0026gt; System \u0026gt; Power Management \u0026gt; Hard Disk Settings Turn Off the hard disk (plugged in) Disabled Power Administrative Templates \u0026gt; System \u0026gt; Power Management \u0026gt; Hard Disk Settings Turn Off the Hard Disk (seconds)(Device) 0 Power Administrative Templates \u0026gt; System \u0026gt; Power Management \u0026gt; Notification Settings Low battery notification action Enabled Power Administrative Templates \u0026gt; System \u0026gt; Power Management \u0026gt; Notification Settings Low Battery Notification Action (Device) Sleep Power Administrative Templates \u0026gt; System \u0026gt; Power Management \u0026gt; Notification Settings Low battery notification level Enabled Power Administrative Templates \u0026gt; System \u0026gt; Power Management \u0026gt; Notification Settings Low Battery Notification Level (Device) (in percent) 5 Power Administrative Templates \u0026gt; System \u0026gt; Power Management \u0026gt; Sleep Settings Require a password when a computer wakes (plugged in) Enabled Power Administrative Templates \u0026gt; System \u0026gt; Power Management \u0026gt; Sleep Settings Specify the system sleep timeout (plugged in) Enabled Power Administrative Templates \u0026gt; System \u0026gt; Power Management \u0026gt; Sleep Settings System Sleep Timeout (seconds) 1200 Power System \u0026gt; Power Management \u0026gt; Video and Display Settings Turn off the display (plugged in) Enabled Power System \u0026gt; Power Management \u0026gt; Video and Display Settings When plugged in, turn display off after (seconds) 300 Power Power Select Lid Close Action Plugged In Sleep Power Power Select Power Button Action Plugged In System Power plugged in settings\nCategory Setting Location Setting Value General Administrative Templates \u0026gt; System \u0026gt; Power Management Select an active power plan Enabled General Administrative Templates \u0026gt; System \u0026gt; Power Management Active Power Plan: (Device) Automatic Battery Administrative Templates \u0026gt; System \u0026gt; Power Management \u0026gt; Hard Disk Settings Turn Off the hard disk (on battery) Enabled Battery Administrative Templates \u0026gt; System \u0026gt; Power Management \u0026gt; Hard Disk Settings Turn Off the Hard Disk (seconds)(Device) 1 Battery Administrative Templates \u0026gt; System \u0026gt; Power Management \u0026gt; Sleep Settings Require a password when a computer wakes (battery) Enabled Battery Administrative Templates \u0026gt; System \u0026gt; Power Management \u0026gt; Sleep Settings Specify the system sleep timeout (plugged in) Enabled Battery Administrative Templates \u0026gt; System \u0026gt; Power Management \u0026gt; Sleep Settings System Sleep Timeout (seconds) 600 Battery System \u0026gt; Power Management \u0026gt; Video and Display Settings Turn off the display (battery) Enabled Battery System \u0026gt; Power Management \u0026gt; Video and Display Settings When plugged in, turn display off after (seconds) 60 Power Power Select Lid Close Action Plugged In Sleep Power Power Select Power Button Action Plugged In System General Power Energy Saver Battery Threshold On Battery 10 Battery settings Settings body Because it makes life much easier, I converted all the settings into a JSON file. The idea is to read the content and add the content to the request body. The file with the power configuration policy setting is stored in my GitHub repository. A part file content looks like below.\n{ \u0026#34;id\u0026#34;: \u0026#34;1\u0026#34;, \u0026#34;settingInstance\u0026#34;: { \u0026#34;@odata.type\u0026#34;: \u0026#34;#microsoft.graph.deviceManagementConfigurationChoiceSettingInstance\u0026#34;, \u0026#34;settingDefinitionId\u0026#34;: \u0026#34;device_vendor_msft_policy_config_admx_power_diskacpowerdowntimeout_2\u0026#34;, \u0026#34;settingInstanceTemplateReference\u0026#34;: null, \u0026#34;choiceSettingValue\u0026#34;: { \u0026#34;settingValueTemplateReference\u0026#34;: null, \u0026#34;value\u0026#34;: \u0026#34;device_vendor_msft_policy_config_admx_power_diskacpowerdowntimeout_2_0\u0026#34;, \u0026#34;children\u0026#34;: [] } } } Deploy configuration policy automated In the post body below I set up the name and description. The platforms and technologies objects are Microsoft predefined settings. The settings object is filled with the JSON content file. I convert the content back to a PowerShell object first. This to create a request body with PowerShell objects only. In the end, I convert the whole body into a JSON object.\nIf you are converting JSON content twice you get really strange content that the REST API is not accepting.\nFor more information about the request body, check the URL: https://docs.microsoft.com/en-us/graph/api/intune-deviceconfigv2-devicemanagementconfigurationpolicy-create?view=graph-rest-beta#request-body\n$policyBody = @{ \u0026#34;@odata.type\u0026#34; = \u0026#34;#microsoft.graph.deviceManagementConfigurationPolicy\u0026#34; \u0026#34;name\u0026#34; = \u0026#34;Windows - Power management Settings\u0026#34; \u0026#34;description\u0026#34; = \u0026#34;Settings for Windows power management\u0026#34; \u0026#34;platforms\u0026#34; = \u0026#34;windows10\u0026#34; \u0026#34;technologies\u0026#34; = \u0026#34;mdm\u0026#34; \u0026#34;settings\u0026#34; = @( Get-Content ./power-management-settings.json| ConvertFrom-Json ) } After the body is done, I‚Äôm putting all the parts together into a PowerShell object. The URL, method, headers (from the first chapter), and the body with the settings. I‚Äôm using the PowerShell splatting technique to execute the command with all the parameters.\n$policyUrl = \u0026#34;https://graph.microsoft.com/beta/deviceManagement/configurationPolicies\u0026#34; $policyParams = @{ URI = $policyUrl Method = \u0026#34;POST\u0026#34; Headers = $authHeader Body = $policyBody | ConvertTo-Json -Depth 99 } $policy = Invoke-RestMethod @policyParams $policy The result\nPower settings assignment with filters In the last part, we assign the policy to all NOT virtual machines. This is because virtual machines actually don‚Äôt have a battery and the policy is not applicable to those machines. To create a ‚ÄòNOT‚Äô-group I use filters. Based on the device‚Äôs model I created a filter where the module is not equal to virtual machines.\nFor more information about filters, check the URL: https://docs.microsoft.com/en-us/mem/intune/fundamentals/filters\nCreating filters automated is out of the scope of the blog.\nI searched for the created filter with the code below and used the ID in the assignment body.\n$filterUrl = \u0026#34;https://graph.microsoft.com/beta/deviceManagement/assignmentFilters\u0026#34; $filterParams = @{ URI = $filterUrl Method = \u0026#34;GET\u0026#34; Headers = $authHeader } $filters = Invoke-RestMethod @filterParams $filters.value | Where-Object {$_.displayName -eq \u0026#39;No virtual machines\u0026#39;} To assign policies automated with PowerShell I used the code below. First is the request body. I create a new GUID and provided the $filters.value.id value as the filter ID. At last, I convert to body into JSON format.\n$assignBody = @{ \u0026#34;assignments\u0026#34; = @( @{ id = $(New-Guid).Guid target = @{ \u0026#34;@odata.type\u0026#34; = \u0026#34;#microsoft.graph.allDevicesAssignmentTarget\u0026#34; deviceAndAppManagementAssignmentFilterType = \u0026#34;include\u0026#34; deviceAndAppManagementAssignmentFilterId = $filters.value.id } } ) } $assignBody = $assignBody | ConvertTo-Json -Depth 4 In the request, I provide the $assignBody and the $policy.Id from the steps above.\n$assignmentUrl = \u0026#34;https://graph.microsoft.com/beta/deviceManagement/configurationPolicies/{0}/assign\u0026#34; -f $policy.id $policyParams = @{ URI = $assignmentUrl Method = \u0026#34;POST\u0026#34; Headers = $authHeader Body = $assignBody } $assignment = Invoke-RestMethod @policyParams $assignment Summary In this post, I showed how to configure power management settings in Microsoft Intune the automated way. I showed how to create a configuration policy and how to assign the policy to all devices. A part of the assignment is the use of filters.\nThank you for reading this blog about how to enroll power management settings automated in Microsoft Intune.\nThank you for reading my blog deploy power settings automated in microsoft intune. I hope you got a bit inspired. Enjoy your day and happy automating üëã ","date":"March 7, 2022","image":"http://localhost:1313/deploy-power-settings-automated-in-microsoft-endpoint-manager/green-it.jpg","permalink":"/deploy-power-settings-automated-in-microsoft-endpoint-manager/","title":"Deploy power settings automated in Microsoft Intune"},{"categories":["Graph API","Microsoft Intune","Powershell","Security"],"contents":"During writing automation blog posts about deploying security baselines automated, I felt the need to write a new blog about how these security baselines actually work. Not in the way how security baselines are deployed but how security baselines are built. Which components does a baseline have and how do settings fit together. In this blog post, I peel off a security baseline and explain how to grab the correct settings.\nEndpoint Security is very important, no new till so far I think. Because of the Microsoft introduced security baselines. A set of pre-configured Windows settings that help to apply and enforce granular security settings. Settings recommended by relevant security teams.\nIn the blog post, I explain the different layers in security baselines and how to manage security baselines. The main goal is to understand how security baselines actually work. Another part is how to configure these baselines with the main goal, deploy security baselines automated. In the blog post, I try to map the portal (https://endpoint.microsoft.com) view with the information under the hood. To get sight under the hood, I use the Graph API.\nPermissions To get started, make sure you have an application registration with at least the following permissions:\nDeviceManagementConfiguration.ReadWrite.All Table Of Contents Permissions General information Graph API Authentication Profiles Create a security baseline profile Create a security baseline profile automated Get the source template Create new profile Customize security baseline profiles Getting security baseline settings Configure custom settings Create POST body with name and settings Sending POST request Assign security baseline Assign security baseline to all devices automated Assign security baseline to all users automated Versions Summary General information Security baselines are groups of pre-configured Windows settings that help you apply and enforce granular security settings that are recommended by the relevant security teams. You can also customize each baseline you deploy to enforce only those settings and values you require. When you create a security baseline profile in Intune, you‚Äôre creating a template that consists of multiple device configuration profiles.\nSecurity baselines are stored under Endpoint Security in the security baselines blade. Currently, there are four types of security baselines. A security baseline is a template with predefined settings. A security baseline includes the best practices and recommendations on settings that impact security. Every type has its own versions and settings.\nhttps://docs.microsoft.com/en-us/mem/intune/protect/security-baselines#available-security-baselines\nGraph API Authentication Before requesting the Graph API, we need to authenticate first.\n$resourceUrl = \u0026#34;https://graph.microsoft.com\u0026#34; $token = Get-AzAccessToken -Resource $resourceUrl $authHeader = @{ \u0026#39;Content-Type\u0026#39; = \u0026#39;application/json\u0026#39; Authorization = \u0026#39;Bearer \u0026#39; + $token.Token } Let‚Äôs take a look at what the Graph API output looks like. However security baselines are stored at a different location in Microsoft Intune, in the Graph API, you can find the baselines under the /templates endpoint. To get all security baselines, we need the Graph URL below.\nhttps://graph.microsoft.com/beta/deviceManagement/templates This is the endpoint where every template is behind. In the code below, I requested all the available templates.\n$getParams = @{ method = \u0026#34;GET\u0026#34; uri = \u0026#34;https://graph.microsoft.com/beta/deviceManagement/templates\u0026#34; headers = $authHeader } $templates = Invoke-RestMethod @getParams $templates.value After the request the API response with all the available templates. To clarify, I selected two different template types. The first template is a security baseline (Windows 365 Security Baseline). The second is a device configuration template for Email.\nSo, how do you see the difference? It all has to do with @odata.type. The @odata.type tells what kind of template is it. In the case of a GET request (like above), the type isn‚Äôt that important. In the case of a POST request (when creating a new profile), it is a MUST. Because we have one single templates endpoint we need to tell what kind of template it is and where the request needs to put the template. I will discuss this point later.\n@odata.type : #microsoft.graph.securityBaselineTemplate id : cef15778-c3b9-4d53-a00a-042929f0aad0 displayName : Windows 365 Security Baseline description : Windows 365 settings as recommended by Microsoft versionInfo : November 2021 isDeprecated : False intentCount : 1 templateType : cloudPC platformType : windows10AndLater templateSubtype : none publishedDateTime : 21/10/2021 00:00:00 id : 54dd8d4d-f159-4da0-9d8f-40f44aef5608 displayName : Email description : Mifo Windows 10 and later Email versionInfo : Version 1 isDeprecated : False intentCount : 0 templateType : deviceConfigurationForOffice365 platformType : windows10AndLater templateSubtype : none publishedDateTime : 01/01/2001 00:00:00 Profiles Security baselines are templates with predefined settings. To use these settings and assign a baseline template to devices and/or users we need to create a new profile.\nCreate a security baseline profile To create a profile click on the correct security baseline and go to the profiles blade. Then click on Create Profile. In the wizard fill in the needed settings and you‚Äôre all set.\nCreate a security baseline profile automated Creating a security baseline profile through the portal isn‚Äôt that hard. But what about creating a security baseline profile automated and assigning the profile to a user group. To create a security baseline profile automated you need to create a new instance. To create a new instance use the Graph API URL below.\nhttps://graph.microsoft.com/beta/deviceManagement/templates/{deviceManagementTemplateId}/createInstance https://docs.microsoft.com/en-us/graph/api/intune-deviceintent-devicemanagementtemplate-createinstance?view=graph-rest-beta\nWhat is the {deviceManagementTemplateId} and how to get it? This is the id of the source template. Let‚Äôs take a look at an example. In the screenshot below we have the security baselines. The baseline with the red box is the one we are focussing on.\nGet the source template With the createInstance URL in mind, we need to get the source template id. To get the source baseline template id we first need to list all the templates. After authentication, I used the code below to get the correct security baseline from the /templates endpoint.\n$getParams = @{ method = \u0026#34;GET\u0026#34; uri = \u0026#34;https://graph.microsoft.com/beta/deviceManagement/templates?`$filter=startswith(displayName, \u0026#39;MDM Security Baseline for Windows 10\u0026#39;)\u0026#34; headers = $authHeader } $templates = Invoke-RestMethod @getParams $templates.value The result of $templates.value is shown below. As shown in the result, this baseline is from November 2021. The isDeprecated object is False which means that this baseline is currently the latest version. Because I created a profile in the portal the intentCount value is 1. The intentCount represents the associated profiles.\n@odata.type : #microsoft.graph.securityBaselineTemplate id : 034ccd46-190c-4afc-adf1-ad7cc11262eb displayName : MDM Security Baseline for Windows 10 and later for November 2021 description : MDM Security Baseline for Windows 10 and later versionInfo : November 2021 isDeprecated : False intentCount : 1 templateType : securityBaseline platformType : windows10AndLater templateSubtype : none publishedDateTime : 22/10/2021 00:00:00 https://docs.microsoft.com/en-us/graph/api/intune-deviceintent-securitybaselinetemplate-list?view=graph-rest-beta\nCreate new profile Now we have the correct Id, we can call the createInstance endpoint. As the most POST requests, we do need to provide a body in JSON format.\n$postBody = @{ displayName = \u0026#34;Win11 - Security Baseline Automated\u0026#34; description = \u0026#34;MDM Security Baseline for Windows 10 and later\u0026#34; versionInfo = \u0026#34;February 2022\u0026#34; isDeprecated = $false templateType = \u0026#34;securityBaseline\u0026#34; platformType = \u0026#34;windows10AndLater\u0026#34; templateSubtype = \u0026#34;none\u0026#34; } $postParams = @{ uri = \u0026#34;https://graph.microsoft.com/beta/deviceManagement/templates/{0}/createInstance\u0026#34; -f $templates.value.id headers = $authHeader Method = \u0026#34;POST\u0026#34; body = $($postBody | ConvertTo-Json) } Invoke-RestMethod @postParams After creation, I requested the template endpoint URL again. The intentCount value is now 2. In the portal, the intentCount is called Associated Profiles.\n@odata.type : #microsoft.graph.securityBaselineTemplate id : 034ccd46-190c-4afc-adf1-ad7cc11262eb displayName : MDM Security Baseline for Windows 10 and later for November 2021 description : MDM Security Baseline for Windows 10 and later versionInfo : November 2021 isDeprecated : False intentCount : 2 templateType : securityBaseline platformType : windows10AndLater templateSubtype : none publishedDateTime : 22/10/2021 00:00:00 As you can see, however, I provided a custom version in the POST body (February 2022), the profile is using the template‚Äôs current version.\nIn this chapter, we created a profile based on the MDM Security Baseline for Windows 10 (and above). If you want Microsoft‚Äôs default setting then you‚Äôre all set and ready to assign the profile.\nCustomize security baseline profiles Now we know the security baselines are templates and we need to create profiles (intents). Sometimes you like to override the default settings. A scenario could be in the base if you have your own antivirus product and want to edit the Microsoft Defender settings.\nIf you like to edit the defaults then this is the chapter to follow. In this chapter, I explain how to get all settings and how to deploy settings in a baseline with custom settings.\nThe main goal is to override the Microsoft Defender settings. To edit settings it is good to know what the settings look like in the background?\nGetting security baseline settings At first, we have to query the source template, the MDM Security Baseline. To get baseline settings, two extra things are needed. First, use the same Graph API URL added by the template Id. The second is adding the $expand=settings at the end of the URL.\nhttps://graph.microsoft.com/beta/deviceManagement/templates/034ccd46-190c-4afc-adf1-ad7cc11262eb?`$expand=settings (Without the extend parameter, you just get the general template information. There is also an option with the URL below.\nhttps://graph.microsoft.com/beta/deviceManagement/templates(\u0026#39;034ccd46-190c-4afc-adf1-ad7cc11262eb\u0026#39;)/settings However, with that URL you only get the settings and miss all other template information. During coding, I try to get as much information into one request as possible. With that in mind, it helps me keep my code clean. I try to avoid using variables all over the place.\n\u0026lt;meta charset=\u0026#34;utf-8\u0026#34;\u0026gt;\u0026lt;/meta\u0026gt;$getSettings = @{ method = \u0026#34;GET\u0026#34; uri = \u0026#34;https://graph.microsoft.com/beta/deviceManagement/templates/{0}?`$expand=settings\u0026#34; -f \u0026#34;034ccd46-190c-4afc-adf1-ad7cc11262eb\u0026#34; headers = $authHeader } $settings = Invoke-RestMethod @getSettings $settings.settings | Where {$_.definitionId -match \u0026#34;defender\u0026#34;} Based on the screenshot above, I picked some Defender settings from the output of the settings request to clarify.\n# Block Adobe Reader from creating child processes @odata.type : #microsoft.graph.deviceManagementStringSettingInstance id : d0e98a90-a30c-4753-b57d-3adae5af0723 definitionId : deviceConfiguration--windows10EndpointProtectionConfiguration_defenderAdobeReaderLaunchChildProcess valueJson : \u0026#34;enable\u0026#34; value : enable # Defender schedule scan day @odata.type : #microsoft.graph.deviceManagementStringSettingInstance id : c6d23b54-615d-4cef-b24d-4c7443bec8d2 definitionId : deviceConfiguration--windows10GeneralConfiguration_defenderScheduleScanDay valueJson : \u0026#34;everyday\u0026#34; value : everyday # Enter how often (0-24 hours) to check for security intelligence update @odata.type : #microsoft.graph.deviceManagementIntegerSettingInstance id : 7dbc8ff3-f604-4b75-b5dd-15438d0fab7b definitionId : deviceConfiguration--windows10GeneralConfiguration_defenderSignatureUpdateIntervalInHours valueJson : 4 value : 4 Every configurable setting has a ‚Äò@odata.type‚Äô. The type shows what kind of setting it is. In the output above we have two types, deviceManagementIntegerSettingInstance and deviceManagementBooleanSettingInstance. An integer means the value is a number. A boolean means the setting is True or False. Mostly a boolean type is enabling or disabling a setting.\nThen we have an ID, this is the setting unique identifier. The definitionId is also a unique identifier but then more readable.\nConfigure custom settings As said, when creating a new instance without providing setting the default settings are deployed. In the previous paragraph, I showed how to get settings out of a template. Now it is time to create a JSON body with settings and custom values. Let‚Äôs pick the Microsoft Defender settings and set the following settings:\nSetting Default value New value Block Adobe Reader from creating child processes Enabled Block Enter how often (0-24 hours) to check for security intelligence updates 4 8 Defender schedule scan day Every day Monday To edit settings I copied the output and created a PowerShell object. Within the object, I stored the settings with the new values.\n$settings = @( @{ # Block Adobe Reader from creating child processes \u0026#39;@odata.type\u0026#39; = \u0026#39;#microsoft.graph.deviceManagementStringSettingInstance\u0026#39; id = \u0026#39;d0e98a90-a30c-4753-b57d-3adae5af0723\u0026#39; definitionId = \u0026#34;deviceConfiguration--windows10EndpointProtectionConfiguration_defenderAdobeReaderLaunchChildProcess\u0026#34; valueJson = \u0026#39;\u0026#34;AuditOnly\u0026#34;\u0026#39; } @{ # Defender schedule scan day \u0026#39;@odata.type\u0026#39; = \u0026#39;#microsoft.graph.deviceManagementStringSettingInstance\u0026#39; id = \u0026#39;c6d23b54-615d-4cef-b24d-4c7443bec8d2\u0026#39; definitionId = \u0026#39;deviceConfiguration--windows10GeneralConfiguration_defenderScheduleScanDay\u0026#39; valueJson = \u0026#39;\u0026#34;monday\u0026#34;\u0026#39; } @{ # Enter how often (0-24 hours) to check for security intelligence update \u0026#39;@odata.type\u0026#39; = \u0026#39;#microsoft.graph.deviceManagementIntegerSettingInstance\u0026#39; id = \u0026#39;7dbc8ff3-f604-4b75-b5dd-15438d0fab7b\u0026#39; definitionId = \u0026#39;deviceConfiguration--windows10GeneralConfiguration_defenderSignatureUpdateIntervalInHours\u0026#39; valueJson = \u0026#34;8\u0026#34; } ) Make sure that the valueJson of the #microsoft.graph.deviceManagementStringSettingInstance type are surrounded with ‚Äù ‚Äú. Otherwise, you get an error like below.\n| {\u0026#34;error\u0026#34;:{\u0026#34;code\u0026#34;:\u0026#34;BadRequest\u0026#34;,\u0026#34;message\u0026#34;:\u0026#34;{\\r\\n \\\u0026#34;_version\\\u0026#34;: 3,\\r\\n \\\u0026#34;Message\\\u0026#34;: \\\u0026#34;Setting Block Adobe Reader from creating child processes has invalid ValueJson. Setting Id: deviceConfiguration--windows10EndpointProtectionConfiguration_defenderAdobeReaderLaunchChildProcess - Create POST body with name and settings Second, I create a POST body with the settings in it. The displayName and description do not need any introduction I guess. The settingsDelta value is exactly what the name suggests. It‚Äôs the value of all settings which are different from the default (the delta‚Äôs).\n# Create the request body $postBody = @{ displayName = \u0026#34;Win11 - Security Baseline Automated - Custom\u0026#34; description = \u0026#34;MDM Security Baseline for Windows 10 and later\u0026#34; settingsDelta = $settings } At the PowerShell level, I‚Äôm creating a nested hashtable. The settings hashtable is stored in the post body hashtable. After the values are set I put to provide this nested hashtable as a body in the post parameters.\nSending POST request To keep my code clean and want to avoid extreme long command lines I put all the needed settings in an object again. In there I store the Invoke-RestMethod parameters and values. Because this POST request needs a JSON body, I need to convert the hashtable into JSON. Converting PowerShell hashtables to JSON is easily done with the Convert-ToJson command. By default, the convert command converts the first 2 levels in a hashtable. The rest will be a nested object which is not valid JSON. In the case you have more levels use the -Depth command. I often see scripts with ConvertTo-Json with -Depth 99 regardless of the number of levels. In my opinion, there is nothing wrong with that but I want to keep my code as clean as possible.\nAt last, I provide the object to the Invoke-RestMethod command. Make a note about the @ instead of $.\n$postParams = @{ uri = \u0026#34;https://graph.microsoft.com/beta/deviceManagement/templates/{0}/createInstance\u0026#34; -f $templates.value.id headers = $authHeader Method = \u0026#34;POST\u0026#34; body = $($postBody | ConvertTo-Json) } Invoke-RestMethod @postParams Assign security baseline Before the security baselines work, we need to assign the baseline to something. There are three types of assignment, users, groups, and devices. If you don‚Äôt need special assignments (like to specific users or devices) use add all users or all devices. Currently, filters are not available at the security baseline level so if you need some filtering, then use (dynamic) groups. Please check my post about securing and optimizing AVD hosts and CloudPC to learn more about assigning groups.\nIn this chapter, I focus on assigning the security baseline to all users and all devices. In the upcoming paragraphs, I will show how to assign the security baseline automated to all users and all devices.\nAssign security baseline to all devices automated We are going back to the first security baseline profile I created automated, Win11 ‚Äì Security Baseline Automated profile. As shown in the screenshot below the profile is not assigned.\nAt first, I query for the security baseline profile with the code below. For authentication, I reused the commands in the first chapter.\n$getParams = @{ method = \u0026#34;GET\u0026#34; uri = \u0026#34;https://graph.microsoft.com/beta/deviceManagement/intents?`$filter=displayName eq \u0026#39;Win11 - Security Baseline Automated\u0026#39;\u0026#34; headers = $authHeader } $templates = Invoke-RestMethod @getParams $templates.value As shown in the output, the security baseline is not assigned. Let‚Äôs go further with the next step, assign the security baseline automated.\nI stored the output in the $templates variable. This is because I need the id in the next step.\nTo assign the security baseline to all devices, I use the code below.\nAt first, I create a body that will be sent in the POST request. The body must be in JSON format, so I convert the PowerShell hashtable into JSON format. Because the hashtable has more than 2 nested objects (4), I also provide the -Depth parameter.\n$assignBody = @{ \u0026#34;assignments\u0026#34; = @( @{ \u0026#34;@odata.type\u0026#34; = \u0026#34;#microsoft.graph.deviceManagementIntentAssignment\u0026#34; \u0026#34;id\u0026#34; = $(New-Guid).Guid \u0026#34;target\u0026#34; = @{ \u0026#34;@odata.type\u0026#34; = \u0026#34;#microsoft.graph.allDevicesAssignmentTarget\u0026#34; \u0026#34;deviceAndAppManagementAssignmentFilterType\u0026#34; = \u0026#34;none\u0026#34; } } ) } $assignBody = $assignBody | ConvertTo-Json -Depth 4 After converting the hashtable into a JSON format the content looks like below.\n\u0026lt;pre class=\u0026#34;wp-block-code\u0026#34;\u0026gt;```json { \u0026#34;assignments\u0026#34;: [ { \u0026#34;@odata.type\u0026#34;: \u0026#34;#microsoft.graph.deviceManagementIntentAssignment\u0026#34;, \u0026#34;id\u0026#34;: \u0026#34;9a311654-a756-4089-a6a9-b9ba4a0a8aa3\u0026#34;, \u0026#34;target\u0026#34;: { \u0026#34;@odata.type\u0026#34;: \u0026#34;#microsoft.graph.allDevicesAssignmentTarget\u0026#34;, \u0026#34;deviceAndAppManagementAssignmentFilterType\u0026#34;: \u0026#34;none\u0026#34; } } ] } If you are not sure about the -Depth number then use 99, which is the max depth number. If the number is too low the content looks like something like below. You also will get an error:\nWARNING: Resulting JSON is truncated as serialization has exceeded the set depth of n $assignBody = $assignBody | ConvertTo-Json -Depth 2 WARNING: Resulting JSON is truncated as serialization has exceeded the set depth of 2. { \u0026#34;assignments\u0026#34;: [ { \u0026#34;@odata.type\u0026#34;: \u0026#34;#microsoft.graph.deviceManagementIntentAssignment\u0026#34;, \u0026#34;id\u0026#34;: \u0026#34;6f05b34b-bdb5-4f60-aa9d-b7dbae043349\u0026#34;, \u0026#34;target\u0026#34;: \u0026#34;System.Collections.Hashtable\u0026#34; \u0026lt;----- This is not good } ] } In the end, create the POST request with the JSON body, the header, and the correct URI.\n$newProfileId = $template.value.id $postParams = @{ method = \u0026#34;POST\u0026#34; uri = \u0026#34;https://graph.microsoft.com/beta/deviceManagement/intents/{0}/assign\u0026#34; -f $newProfileId headers = $authHeader body = $assignBody } Invoke-RestMethod @postParams The assignment POST request returns a NULL value. To check afterward use the GET request again.\n$getParams = @{ method = \u0026#34;GET\u0026#34; uri = \u0026#34;https://graph.microsoft.com/beta/deviceManagement/intents?`$filter=displayName eq \u0026#39;Win11 - Security Baseline Automated\u0026#39;\u0026#34; headers = $authHeader } $assignToProfile = Invoke-RestMethod @getParams $assignToProfile.value Assign security baseline to all users automated If you want to assign the profile to all users, then use the microsoft.graph.allLicensedUsersAssignmentTarget as the @odata.type. See the body below.\n$assignBody = @{ \u0026#34;assignments\u0026#34; = @( @{ \u0026#34;@odata.type\u0026#34; = \u0026#34;#microsoft.graph.deviceManagementIntentAssignment\u0026#34; \u0026#34;id\u0026#34; = $(New-Guid).Guid \u0026#34;target\u0026#34; = @{ \u0026#34;@odata.type\u0026#34; = \u0026#34;#microsoft.graph.allLicensedUsersAssignmentTarget\u0026#34; \u0026#34;deviceAndAppManagementAssignmentFilterType\u0026#34; = \u0026#34;none\u0026#34; } } ) } $assignBody = $assignBody | ConvertTo-Json -Depth 4 Versions Every security baseline has its own version provided by Microsoft. Good to know is that when a new version becomes available, the currently selected version becomes read-only. In fact, this means that your current production baseline is not editable anymore. But, the settings still will do their job. Another thing is when a new version becomes available, profiles will NOT be updated automatically to the latest version.\nversion To check the version I used the code below. I return back to the MDM Security Baseline for Windows 10 template.\n$getParams = @{ method = \u0026#34;GET\u0026#34; uri = \u0026#34;https://graph.microsoft.com/beta/deviceManagement/templates?`$filter=startswith(displayName, \u0026#39;MDM Security Baseline for Windows 10\u0026#39;)\u0026#34; headers = $authHeader } $templates = Invoke-RestMethod @getParams $templates.value When looking at the back of the template the information looks like below. The version is stored in the title as in the version info object. Because there is no other version available at this time, I can‚Äôt test with other versions. But I do think the isDeprecated value also represents the current version. I think the isDeprecated value will become True when a new version is available.\nThe URL below points to the MDM Security Baseline for Windows 10. The other baselines are also stored at that location.\nhttps://docs.microsoft.com/en-us/mem/intune/protect/security-baseline-settings-mdm-all?pivots=november-2021\nSummary In this blog post, I security baseline was explained. I showed the components and how to create a profile automated. Also, I explained how to create a baseline with custom settings. In the upcoming post around the security baseline subject, I will show how to monitor security baselines at different points. Which values are important and how to get the correct information.\nThank you for reading this blog about security baselines explained. I hope you got a bit inspired.\nThank you for reading my blog microsoft intune security baselines explained. I hope you got a bit inspired. Enjoy your day and happy automating üëã ","date":"February 23, 2022","image":"http://localhost:1313/microsoft-endpoint-manager-security-baselines-explained/lock-fence-security.jpg","permalink":"/microsoft-endpoint-manager-security-baselines-explained/","title":"Microsoft Intune security baselines explained"},{"categories":["Azure","Monitoring"],"contents":"Earlier I explained how to use the REST API to get more information about Azure file shares. This is because we like to monitor file share usage based on absolute values. In this blog post, I explain the follow-up for Azure file share usage monitoring with Logic Apps. First I explain how the Logic App works and how I did the maths. Second I explain how to deploy this Logic App automated. In the end, we have a deployment of Azure file share usage monitoring in an automated way.\nInspired by Peter Klapwijk his post about Get notified on expiring Azure App Registration client secrets, I decided to write also a Logic App. This Logic App is monitoring file shares usage based on absolute values.\nTable Of Contents Working flow Logic App components Authentication Other variables Get storage account information Get the storage account shares Processing file share information Logic App mathematics Send alert from Logic App to MS Teams Create a Logic App automated Configuring the Logic App Working flow To understand the Logic App, I like to explain the working flow and the goal I want to achieve. The main goal is to check every storage account on files shares and do a check on use per file share. The usage is calculated in percentage based on the file share size. In the end, if the usage percentage is higher than 90% I want a message in Microsoft Teams. The authentication credentials are stored in an Azure key vault.\nLogic App components In the previous post, I showed how to crawl for storage accounts, search the files shares, how to fetch the file share expanded information. If you look at the code you notice there are a few loops with HTTP GET requests to the Azure management REST API. After the information is there, I do some maths to calculate the usage in percentage. In the Logic App, I‚Äôm doing the same. To clarify, I split the Logic App into main components.\nAuthentication Before we are able to request the REST API for the first time, we need to set variables. Variables for authentication and a later also a kind of ‚Äòworking‚Äô variable. To request an API, you need to authenticate first. In this Logic App, I use a service principal with a Reader role on the subscription. (If you have more subscriptions, make sure the service principal has permissions to all needed subscriptions.)\nAbout the authentication variables, I created an Azure key vault first. In the key vault I stored the application details (appId and appSecret). Also, I stored the subscriptionId and tenantId.\nTo set these four Key Vault values into the Logic App, create a new step and search for Azure Key Vault. Under actions, click on Get Secret and select the correct secret.\nTo keep the overview clear, I would suggest to rename the steps into a fitting name.\nThese variables are now recognized and selectable in the upcoming tasks.\nOther variables If a storage account has a file share it returns the file share usage. During the flow, I store the file share usage into a variable with the Set variable action. The fact is, before we can set, we have to initialize the variable first. Initializing variables must be done at the top level.\nTo initialize variables, add a new task, search for variables and click Initialize variable. Because I want to calculate with decimals I use the float type.\nGet storage account information The first stop in Azure file share usage monitoring with Logic Apps is get all storage accounts. As mentioned, make sure the application has at least a Reader role assigned to the subscription. To send requests to an API, we need the HTTP task. In actions, use the HTTP task. In the HTTP task use the GET method. As URI I used the storage account LIST URL. The subscriptionId in the URL is a variable from the Azure Key Vault initiated in the previous steps.\nThe next part is to set authentication. To add the authentication parameters, click on the Add new parameter box and select Authentication.\nI‚Äôve configured the authentication part the following way.\nAuthentication type: Active Directory OAuth Tenant: the tenantId value from the Key Vault Audience: this is the resource where to authenticate to which is in the case https://management.azure.com Client ID: the AppId value from the Key Vault Credential type: Secret Secret: AppSecret value from the Key Vault Now we are able to run the Logic App for the first time. After running go to the Run History to see the output. As you can see the output is in JSON format.\nTo work with this data, we need to parse the JSON output into new variables. Luckily, in Logic Apps, there is a task called Parse JSON. In the task, you have two fields. the content input and the part of how the schema looks like. The schema is to tell how the task must parse the JSON content.\nWhere the content is the body output of the HTTP task you need to fill in the schema yourself. But, take a look at the Use sample payload to generate schema option. Click on the link, and put the JSON HTTP body output content in it.\nGet the storage account shares Now we have storage accounts insights, let‚Äôs search for file shares for every storage account. This is the first for each loop in the Logic App. First, I create a task called ForEach. Search for the Control task. I use the JSON output value as input in the next HTTP request. The ID represents the storage account resource ID. In the for each loops through the JSON content.\nTo clarify, the file share REST API URI looks like this:\nhttps://management.azure.com/subscriptions/{subscriptionId}/resourceGroups/{resourceGroupName}/providers/Microsoft.Storage/storageAccounts/{accountName}/fileServices/default/shares?api-version=2021-04-01\nThe storage account ID output look like this:\n‚Äúid‚Äù: ‚Äú/subscriptions/\u0026lt;subscriptionId\u0026gt;/resourceGroups/\u0026lt;resourceGroup\u0026gt;/providers/Microsoft.Storage/storageAccounts/\u0026lt;account\u0026gt;‚Äù,\nSo, the HTTP request URI is filled dynamically based on the storage account ID. As in the first HTTP request, we also need to authenticate. Use the same values as above.\nOnce we have results, it is time to parse JSON again. Repeat the steps as described above. This time use the sample data from the file share output. Store the value output into the next foreach loop. This is the file share information loop. Actually in this loop the calculation happens.\nProcessing file share information As said, this is the part where it all goes about. Finally, in this loop we got the file shares to work with. For now I skip the HTTP request and JSON parse part. The only thing I like to mention is the URL. Make a note about the $expand=stats at the end. Without this extra option, you will not get the usage statistics. This time, I use the file share ID as dynamic input.\nURI: https://management.azure.com/subscriptions/\u0026amp;lt;subscriptionId\u0026amp;gt;/resourceGroups/\u0026amp;lt;resourceGroupName\u0026amp;gt;/providers/Microsoft.Storage/storageAccounts/\u0026amp;lt;accountName\u0026amp;gt;/fileServices/default/shares/\u0026amp;lt;shareName\u0026amp;gt;?api-version=2021-04-01\nFor more information about the file share, REST API check the overview.\nAfter the JSON parse task, the shareUsageBytes become available. This is the value we are chasing after.\nAs the name says, it is in bytes. So, we need some maths to bring it back to GigaBytes. This is the time when the initial variable is used. First, we set the correct value into the variable.\nLogic App mathematics To calculate file share usage we need some mathematics in the Logic App. To bring bytes back to GigaBytes, we need to divide the usage by 1024 three times. In Logic Apps, some math expressions are available to help. The divide expression in the task looks like below. Besides the div, we need the main value and the value to divide with.\ndiv(div(div(variables(‚ÄòstorageUsage‚Äô),1024),1024),1024)\nLet me show an example to clarify. Suggesting we have usage of 100000000 bytes. In the first step, we set the value as a variable. Then we have the first divide,div(variables(‚ÄòstorageUsage‚Äô),1024). This is the first storageUsage value divided by 1024. For the second we use the first outcome, and so further.\n(294178816 √∑ 1024) = 287284 (287284 √∑ 1024) = 280 (280 √∑ 1024) = 0,27 In the end, we have divided the storage usage 3 times by 1024. The new value is set into the storageUsage variable.\nIn the last step, I pick up the new value and do some maths again. This time to calculate the percentage in relation to the storage quota.\nmul(div(variables(‚ÄòstorageUsage‚Äô),body(‚ÄòParse_fileshare_info_JSON‚Äô)?[‚Äòproperties‚Äô]?[‚ÄòshareQuota‚Äô]),100)\nAgain, we divided the storageUsage variable. This time by the shareQuota (1024) from the file share info JSON output. At last, the final result is multiplied by 100.\n(0,27 √∑ 1024) \\* 100 = 0,274 % Send alert from Logic App to MS Teams The last step in Azure file share usage monitoring with Logic Apps is sending an alert. Finally, we have a percentage to play with. The idea is to send an alert message if the percentage is higher than 90 percent. I use Microsoft Teams to receive alerts by sending an adaptive card. To create adaptive cards go to https://adaptivecards.io/designer/.\nIn the Logic App, I created a Condition task (search for Control). If the output from the previous task is greater or equal to 90 then post an adaptive card in MS Teams.\nTo add the adaptive card task, search for Microsoft Teams and select Post adaptive card in a chat or channel. Fill in the fields. In the adaptive card field, fill in the JSON generated from the adaptive card designer page.\nIt is possible to use Logic App variables in the JSON content.\nAt last, we have a message in Microsoft Teams.\nCreate a Logic App automated The last and final point is creating a Logic App automated. To create a Logic App automated, I use the REST API.\nAs every API request, we need authentication headers, a request method, URL and a body. The most simple part in these is the URL and the request.\nTo deploy the Logic App, I used the settings below.\nURL: https://management.azure.com/subscriptions/\u0026amp;lt;subscriptionID\u0026amp;gt;/resourceGroups/\u0026amp;lt;resourceGroup\u0026amp;gt;/providers/Microsoft.Logic/workflows/\u0026amp;lt;new-app-name\u0026amp;gt;?api-version=2016-06-01\nMethod: PUT\nBody: JSON file content\nFor the body we need a JSON formatted body. To simplify, I created a JSON body already from my Logic App. Before creating the Logic App, make sure you changed the value below.\n\u0026lt;subscriptionId\u0026gt; \u0026lt;resourceGroup\u0026gt; \u0026lt;receipient\u0026gt; The code I used to deploy a Logic App automated\nConnect-AzAccount $azureUrl = \u0026#34;https://management.azure.com\u0026#34; $script:token = GetAuthToken -resource $azureUrl $logicAppName = \u0026#34;NewLogicApp\u0026#34; $context = [Microsoft.Azure.Commands.Common.Authentication.Abstractions.AzureRmProfileProvider]::Instance.Profile.DefaultContext $Token = [Microsoft.Azure.Commands.Common.Authentication.AzureSession]::Instance.AuthenticationFactory.Authenticate($context.Account, $context.Environment, $context.Tenant.Id.ToString(), $null, [Microsoft.Azure.Commands.Common.Authentication.ShowDialog]::Never, $null, $resource).AccessToken $authHeader = @{ \u0026#39;Content-Type\u0026#39; = \u0026#39;application/json\u0026#39; Authorization = \u0026#39;Bearer \u0026#39; + $Token } $uri = \u0026#34;{0}/subscriptions/6d3c408e-b617-44ed-bc24-280249636525/resourceGroups/rg-roz-avd-01/providers/Microsoft.Logic/workflows/{1}?api-version=2016-06-01\u0026#34; -f $AzureUrl, $logicAppName Invoke-RestMethod -Uri $uri -Headers $authHeader -Method PUT -Body (Get-Content ./fileshare-monitoring.json) Configuring the Logic App After creating the Logic App based on the JSON input there are some settings you need to (re)configure.\nYou need the authenticate to MS Teams in the MS Teams step.\nMake sure you‚Äôve setup an Azure Key Vault as well. In the Azure Key Vault you need the secrets as configured in the initial steps.\nThe JSON file is stored at my GitHub.\nFor more information about creating Logic Apps with REST API, check: https://docs.microsoft.com/en-us/rest/api/logic/workflows/create-or-update\nThank you for reading this blog about how to do Azure file share usage monitoring with Logic Apps.\nThank you for reading my blog azure file share usage monitoring with logic apps. I hope you got a bit inspired. Enjoy your day and happy automating üëã ","date":"February 8, 2022","image":"http://localhost:1313/azure-file-share-usage-monitoring-with-logic-apps/monitoring.jpg","permalink":"/azure-file-share-usage-monitoring-with-logic-apps/","title":"Azure file share usage monitoring with Logic Apps"},{"categories":["Azure","Monitoring"],"contents":"In the new world of (Azure) Virtual Desktops and CloudPC, FSLogix is often used for handling user profiles. Now FSLogix is supported with the back of AzureAD only, FSLogix is almost a required component in an AVD or CloudPC environment. With that in mind is it extremely important to know if the FSLogix file share has enough space. In this post, I explain how to monitor the FSLogix file share usage.\nTable Of Contents Introduction What is FSLogix Storage account components Monitoring storage account file shares Storage account monitoring with Azure Monitor (storage insights) File share usage monitoring with REST API Extend extra file share information What is next Introduction The plan is to monitor a storage account file share on his usage. Also, I do some maths to calculate the usage in percentage. There are several ways to monitor a storage account file share usage. LogAnalytics is an option but there is no way to do some maths. Because I don‚Äôt want to know the current bytes in use but a percentage, I use PowerShell. Later, I show the difference (and the gap) between Azure Monitor and PowerShell. Before we start let‚Äôs take a look at some background information. What is FSLogix and how does a storage account looks like.\nWhat is FSLogix FSLogix is a set of solutions that enhance, enable, and simplify non-persistent Windows computing environments. FSLogix solutions are appropriate for virtual environments in both public and private clouds. For combining FSLogix with Azure Virtual Desktop as a desktop virtualization solution on Azure, I store profiles on Azure Files. In this blog post, I show how to configure Azure Files with Azure AD joined session hosts automated.\nIn addition, please check the documentation about FSLogix: https://docs.microsoft.com/en-us/azure/architecture/example-scenario/wvd/windows-virtual-desktop-fslogix\nStorage account components An Azure storage account contains all of your Azure Storage data objects. The following data object are available: blobs, file shares, queues, tables, and disks. The storage account provides a globally unique namespace for storing your data on Azure. The data is accessible from anywhere in the world over HTTP or HTTPS. Data in your storage account is durable and highly available, secure, and massively scalable.\nIn the case of FSLogix, we need a file share. This file share is available from your desktop and used to store the .VHD file representing your profile.\nTo create a storage account automated, check my blog post about Configure FSLogix profile for Azure AD joined AVD session host automated.\nFor more information about Azure storage accounts in general, check the storage account overview.\nMonitoring storage account file shares To monitor the FSLogix storage account file share, we have some options. Azure Monitor with Storage Insights and REST API for example. In the next paragraphs, I show how to configure and use these options. The data I work with within this situation is a file share with one .VHD file. As shown in the screenshots below, I have a .VHD file. The .VHD file represents my FSLogix profile and is around 170MB.\nStorage account monitoring with Azure Monitor (storage insights) First, we have the Azure Monitor with Storage Insights. Storage insights provide comprehensive monitoring of your Azure Storage accounts. It gives you a view of the service‚Äôs performance, capacity, and availability. You can observe storage capacity, and performance in two ways, view directly from a storage account or view from Azure Monitor to see across groups of storage accounts.\nStorage insights work out of the box. After creating a storage account it is available under the Azure Monitor ‚Äì Storage Accounts.\nBy default, the insights option is available and shows some graphs as shown below. The graphs below show the default overview with transactions, bandwidth, and usage. The overview is created by metrics. Based on the metrics, actions are available to create alert rules.\nFor more information check the storage insights overview.\nFile share usage monitoring with REST API As much as I know, in the case of capacity, there is only one actually. The average used capacity. No doubt there are situations where monitoring average usage capacity is needed. In my case, I want to know how much of the grand total is used. Based on that information I know when to extend the file share. To achieve my goal, I need to do some maths and need more storage account information. Information about the current file share usage and the file share size.\nTo get this information, I need to use the REST API.\nAs mentioned in the storage account components chapter, a storage account is in basics a public data entry point. Within a storage account, there are one or more types of data. In the case of file-share monitoring let‚Äôs see what we need.\nBefore we are able to call a REST API, we need the authenticate. At first, I authenticate in PowerShell with connect-azaccount. After the authentication point, I use the function below to create an authentication header for the REST API.\nfunction GetAuthToken($resource) { $context = [Microsoft.Azure.Commands.Common.Authentication.Abstractions.AzureRmProfileProvider]::Instance.Profile.DefaultContext $Token = [Microsoft.Azure.Commands.Common.Authentication.AzureSession]::Instance.AuthenticationFactory.Authenticate($context.Account, $context.Environment, $context.Tenant.Id.ToString(), $null, [Microsoft.Azure.Commands.Common.Authentication.ShowDialog]::Never, $null, $resource).AccessToken $authHeader = @{ \u0026#39;Content-Type\u0026#39; = \u0026#39;application/json\u0026#39; Authorization = \u0026#39;Bearer \u0026#39; + $Token } return $authHeader } $azureUrl = \u0026#34;https://management.azure.com\u0026#34; $script:token = GetAuthToken -resource $azureUrl Now we have everything in place, we need the storage account itself. Finding a storage account with the REST API is simply done with the code below.\n$resourceType = \u0026#34;Microsoft.Storage/storageAccounts\u0026#34; $subscriptionId = [Microsoft.Azure.Commands.Common.Authentication.Abstractions.AzureRmProfileProvider]::Instance.Profile.DefaultContext.Subscription.id $url = \u0026#34;{0}/subscriptions/{1}/resources?`$filter=resourceType eq \u0026#39;$resourceType\u0026#39;\u0026amp;api-version=2021-04-01\u0026#34; -f $azureUrl, $subscriptionId $resources = Invoke-RestMethod -Uri $url -Headers $script:token -Method GET The result of the request is the storage account.\nid : /subscriptions/xxx/resourceGroups/RG-ROZ-STOR-01/providers/Microsoft.Storage/storageAccounts/fslogixxxx name : fslogixxxx type : Microsoft.Storage/storageAccounts sku : @{name=Premium_LRS; tier=Premium} kind : FileStorage location : westeurope tags : Extend extra file share information Now comes the fun part. This is because there is a lot of information available to work with. Good to know is that every data object has its own API available. In this case, we need the file share REST API. If a storage account has no file shares you get an empty response. So in the case of monitoring, we just can request the file share REST API to all storage account. If there is a file share, we get a response.\nFirst, we request the file shares based on the storage account resource ID. If there is a file share, the response is the file share resource ID.\n$resourceId = $resources.value.id $url = \u0026#34;https://management.azure.com{0}/fileServices/default/shares/?api-version=2021-04-01\u0026amp;\u0026#34; -f $resourceId $shares = Invoke-RestMethod -Uri $url -Headers $script:token -Method GET $shares.value ``basic id : /subscriptions/xxx/resourceGroups/RG-ROZ-STOR-01/providers/Microsoft.Storage/storageAccounts/fslogixxxxx/fileServices /default/shares/profiles name : profiles type : Microsoft.Storage/storageAccounts/fileServices/shares etag : \u0026ldquo;0x8D9D837BF9F6598\u0026rdquo; properties : @{leaseStatus=unlocked; leaseState=available; accessTier=Premium; lastModifiedTime=15/01/2022 15:00:19; shareQuota=1024; enabledProtocols=SMB}\nIn basics, the output above is the file share response. When looking into the properties you get the output below. ```basic leaseStatus : unlocked leaseState : available accessTier : Premium lastModifiedTime : 09/12/2021 17:03:12 shareQuota : 1024 enabledProtocols : SMB There is a trick to getting more information out of the REST API by the use of $expand=stats at file share level. The complete URL in PowerShell looks like below. Make a note about ‚Äú`‚Äù before the $expand. This is because of escaping $ which is a reserved character in PowerShell.\n$detailsUrl = \u0026#34;https://management.azure.com/subscriptions/xxxxxx/resourceGroups/RG-ROZ-STOR-01/providers/Microsoft.Storage/storageAccounts/fslogixxx/fileServices/default/shares/profiles?api-version=2021-04-01\u0026amp;`$expand=stats\u0026#34; $fileShareInfo = Invoke-RestMethod -Uri $detailsUrl -Headers $script:token -Method GET $fileShareInfo.properties The properties now are extended with the current statistics. For file share, this means the current usage in bytes. With the code below, I also did some maths to calculate the current usage in percentage and add it to the properties object as well.\n$usageInGb = ($fileShareInfo.properties.shareUsageBytes / 1024 / 1024 / 1024) $PercentUsage = [math]::ceiling($($usageInGb / $fileShareInfo.properties.shareQuota * 100)) $fileShareInfo.properties | Add-Member -MemberType NoteProperty -Name \u0026#34;PercentUsage\u0026#34; -Value $PercentUsage $fileShareInfo.properties leaseStatus : unlocked leaseState : available signedIdentifiers : {} shareUsageBytes : 182878208 accessTier : Premium lastModifiedTime : 15/01/2022 15:00:19 shareQuota : 1024 enabledProtocols : SMB PercentUsage : 1 What is next Now we have an option to calculate file share usage on a storage account, we use this content to send notifications if a specific limit is reached. Because we want alerts, we need a follow-up. To follow up the output, you can use a logic app or a function app that runs at specific times. Currently, I work on the next blog about configuring the logic app.\nThank you for reading this blog about how to monitor FSLogix file share on a storage account.\n{{ \u0026lt; bye \u0026gt;}}\n","date":"January 22, 2022","image":"http://localhost:1313/azure-file-share-usage-monitoring-with-logic-apps/monitoring.jpg","permalink":"/monitor-fslogix-file-share-usage-on-azure-files/","title":"Monitor FSLogix file share usage on Azure Files"},{"categories":["Azure","Azure Virtual Desktop","Microsoft Intune","North Star","Security"],"contents":"In this blog post, I explain how to secure and optimize AVD and CloudPC environment using Microsoft Intune automated. The configuration is all deployed automated based on PowerShell, JSON templates, and Graph API. I explain how to create a security baseline and how to deploy a configuration profile with settings.\nIn this blog post, the focus is on the standardized part of the North Star framework. This means we are going to look at the following subjects:\nSecurity baselines Configuration profiles with a settings catalog instead of GPO However, it is not a part of the North Start framework I also configure a lot of optimization settings to get the best out of the virtual desktop and CloudPC.\nTable Of Contents Operation North-Star Secure AVD and CloudPC using security baselines Create a security baseline profile automated Assign security baseline to a group Optimize AVD with the settings catalog Azure Virtual Desktop optimization settings Deploy configuration profile automated Assign configuration profile automated Operation North-Star This blog post is a part of Operation North-Star. What is operation ‚ÄúNorth-Star‚Äù?\nThe main goal of operation ‚ÄúNorth-Star‚Äù is showing a way how to manage the modern workplace based on the North Star framework. Microsoft has released a framework for Microsoft Endpoint Management called ‚ÄòNorth Star‚Äô. This framework is designed to deploy, secure, optimize and manage Windows devices in the cloud.\nFor more information about this series please read my kick-off blog about the North Star framework.\nSecure AVD and CloudPC using security baselines The first part of secure and optimize AVD and CloudPC is creating a security baseline. Security baselines are a part of Endpoint Security in Microsoft Intune. Security Baselines consist of profiles and versions. Currently, there are four profiles available.\nIn my situation, I choose the Windows 365 Security Baseline (Preview) profile. I have chosen this one because it fits the most to Azure Virtual Desktop and CloudPC. Antivirus exclusions for FSLogix are preconfigured and WiFi are stripped from this settings policy.\nAt this place, I mentioned the word settings a few times already. This is because a Security Baseline profile consists of settings. Just like a configuration profile under device management. Actually, under the hood Security Baselines are a part of the device management REST API. However, a lot more is happening underwater. For that reason, I decided to write another blog post about Security Profiles under de hood. How it fits in Endpoint Manager and how to use the REST API for automation. (keep attention on my website or follow me via Twitter) For now, I continue with standardizing AVD using Microsoft Intune automated.\nCreate a security baseline profile automated As mentioned before, we need a security baseline for Windows 365. This baseline actually is a template from where a profile is created from. To get the correct baseline, I searched for all available templates and filtered for Windows 365 Security Baseline. The template type is categorized under CloudPC.\n$script:templatesUrl = \u0026#34;https://graph.microsoft.com/beta/deviceManagement/templates?`$filter=(displayName eq \u0026#39;Windows 365 Security Baseline\u0026#39;)\u0026#34; $templateId = (Invoke-RestMethod -Uri $script:templatesUrl -Method GET -Headers $script:token).value.id In the next step, I create a profile within the Windows 365 Security Baseline and all the needed settings. In this post, I keep the default template settings. The steps below describe how to create a new security baseline profile automated with the template settings.\nIn the code above we found the correct template Windows 365 Security Baseline. In the next steps, we need ID representing this baseline. At first we grab all the settings from the template and store it into the $securityBsaelineSettings variable.\nMake a note about the `$expand=settings. However the docs says to use /settings at the end of the URL it looks like there is a bug or the documentation isn‚Äôt right yet. When requesting the URL you will get an error like below.\nInvoke-RestMethod: {‚Äúerror‚Äù:{‚Äúcode‚Äù:‚ÄùNo method match route template‚Äù,‚Äùmessage‚Äù:‚ÄùNo OData route exists that match template ~/singleton/navigation/key/navigation with http verb GET for request /DeviceManagementIntent/DeviceManagementIntentService/83661860-ffff-3213-1205-120819383819/deviceManagement/templates(‚Äòcef15778-c3b9-4d53-a00a-042929f0aad0‚Äô)/settings.‚Äù,‚ÄùinnerError‚Äù:{‚Äúdate‚Äù:‚Äù2022-01-11T15:10:28‚Ä≥,‚Äùrequest-id‚Äù:‚Äùd3add198-6b01-49f5-98a2-88e465a3d233‚Ä≥,‚Äùclient-request-id‚Äù:‚Äùd3add198-6b01-49f5-98a2-88e465a3d233‚Ä≥}}}\n$script:baselineSettingsUrl = \u0026#34;https://graph.microsoft.com/beta/deviceManagement/templates/{0}?`$expand=settings\u0026#34; -f $templateId $securityBaselineSettings = Invoke-RestMethod -Uri $script:baselineSettingsUrl -Method GET -Headers $script:token $securityBaselineSettings.settings To clarify the settings part, I searched for the categories and picked Microsoft Defender Antivirus Exclusions. Within this category, FSLogix exclusions are set. The query output shows three collections with values. The screenshot below represents those settings in the portal.\n$script:baselineSettingsUrl = \u0026#34;https://graph.microsoft.com/beta/deviceManagement/templates/{0}/categories\u0026#34; -f $templateId Invoke-RestMethod -Uri $script:baselineSettingsUrl -Method GET -Headers $script:token id : b65b2f09-a697-41e9-add3-1115e09aa393 displayName : Microsoft Defender Antivirus Exclusions hasRequiredSetting : False $script:baselineSettingsUrl = \u0026#34;https://graph.microsoft.com/beta/deviceManagement/templates/{0}/categories\u0026#34; -f $templateId Invoke-RestMethod -Uri $script:baselineSettingsUrl -Method GET -Headers $script:token $url = \u0026#34;https://graph.microsoft.com/beta/deviceManagement/templates/{0}/categories/b65b2f09-a697-41e9-add3-1115e09aa393/recommendedSettings\u0026#34; -f $templateId $catogory = Invoke-RestMethod -Uri $url -Method GET -Headers $script:token $category.value | FL @odata.type : #microsoft.graph.deviceManagementCollectionSettingInstance id : e54bc711-3b81-4202-bf01-6bc7e5e0907d definitionId : deviceConfiguration--windows10EndpointProtectionConfiguration_defenderProcessesToExclude valueJson : [\u0026#34;%ProgramFiles%\\\\FSLogix\\\\Apps\\\\frxccd.exe\u0026#34;,\u0026#34;%ProgramFiles%\\\\FSLogix\\\\Apps\\\\frxccds.exe\u0026#34;,\u0026#34;%ProgramFiles%\\\\FSLogix\\\\Apps\\\\frxsvc.exe\u0026#34;] @odata.type : #microsoft.graph.deviceManagementCollectionSettingInstance id : 91224658-78ae-4e95-a358-ad95b2add175 definitionId : deviceConfiguration--windows10EndpointProtectionConfiguration_defenderFileExtensionsToExclude valueJson : [\u0026#34;%ProgramFiles%\\\\FSLogix\\\\Apps\\\\frxdrv.sys\u0026#34;,\u0026#34;%ProgramFiles%\\\\FSLogix\\\\Apps\\\\frxdrvvt.sys\u0026#34;,\u0026#34;%ProgramFiles%\\\\FSLogix\\\\Apps\\\\frxccd.sys\u0026#34;,\u0026#34;%TEMP%*.VHD\u0026#34;,\u0026#34;%TEMP%*.VHDX\u0026#34;,\u0026#34;%Windir%\\\\TEMP*.VHD\u0026#34;,\u0026#34;%Windir%\\\\TEMP*.VHDX\u0026#34;,\u0026#34;\\\\ \\\\storageaccount.file.core.windows.net\\\\share**.VHD\u0026#34;,\u0026#34;\\\\\\\\storageaccount.file.core.windows.net\\\\share**.VHDX\u0026#34;] @odata.type : #microsoft.graph.deviceManagementCollectionSettingInstance id : eee0be1a-6333-4e38-a43c-0c94df358318 definitionId : deviceConfiguration--windows10EndpointProtectionConfiguration_defenderFilesAndFoldersToExclude valueJson : null Now I have settings and a template, I am able to create a new profile under the Windows 365 Security Baseline. To create a new profile automated, I used the code below. I create a body and store the $securityBaselineSettings variable as the settingsdelta value.\n$jsonbody = @{ \u0026#34;displayName\u0026#34; = \u0026#34;AVD - Security Baseline\u0026#34; \u0026#34;description\u0026#34; = \u0026#34;Used for Azure Virtual Desktop\u0026#34; \u0026#34;settingsDelta\u0026#34; = $securityBaselineSettings \u0026#34;roleScopeTagIds\u0026#34; = @( \u0026#34;0\u0026#34; ) } $jsonbody = $jsonbody | ConvertTo-Json -Depth 99 $script:templatesUrl = \u0026#34;https://graph.microsoft.com/beta/deviceManagement/templates/{0}/createInstance\u0026#34; -f $templateId Invoke-RestMethod -Uri $script:templatesUrl -Method POST -Headers $script:token -Body $jsonbody Assign security baseline to a group At last, we need to assign the policy to the Azure Virtual Desktop hosts AD group.\nIn the Operation North Star Update AVD hosts blog article, I created an AVD ‚Äì Session hosts AD group. In this step, I also use this AD group.\nFirst, I search for the AVD ‚Äì Session hosts AD group again and use the group object ID to assign the security baseline to.\n$group = @{ method = \u0026#34;GET\u0026#34; uri = \u0026#34;https://graph.microsoft.com/v1.0/groups?`$filter=(displayName eq \u0026#39;AVD - Session hosts\u0026#39;)\u0026#34; Headers = $script:token } $groupId = (Invoke-RestMethod @group).value.id $groupId I use the groupId as the groupID input in the body. As with every assignment, we need to create an assignmentId. This can be every valid GUID.\n$assignBody = @{ \u0026#34;assignments\u0026#34; = @( @{ \u0026#34;@odata.type\u0026#34; = \u0026#34;#microsoft.graph.deviceManagementIntentAssignment\u0026#34; \u0026#34;id\u0026#34; = $(New-Guid).Guid \u0026#34;target\u0026#34; = @{ \u0026#34;@odata.type\u0026#34; = \u0026#34;#microsoft.graph.groupAssignmentTarget\u0026#34; \u0026#34;groupId\u0026#34; = $groupId \u0026#34;deviceAndAppManagementAssignmentFilterType\u0026#34; = \u0026#34;none\u0026#34; } } ) } $assignBody = $assignBody | ConvertTo-Json -Depth 99 $script:assignUrl = \u0026#34;https://graph.microsoft.com/beta/deviceManagement/intents/{0}/assign\u0026#34; -f $newProfileId Invoke-RestMethod -Uri $script:assignUrl -Method POST -Headers $script:token -Body $assignBody For more information about device management and templates check the urls: https://docs.microsoft.com/en-us/graph/api/intune-deviceintent-devicemanagementtemplate-list?view=graph-rest-beta\nhttps://docs.microsoft.com/en-us/graph/api/intune-deviceintent-devicemanagementsettinginstance-list?view=graph-rest-beta#http-request\nOptimize AVD with the settings catalog In this second secure and optimize AVD and CloudPC part we create optimization settings. I use the settings catalog in Microsoft Intune to create a configuration profile. The settings catalog lists all the settings you can configure and all in one place. This feature simplifies how you create a policy, and how you see all the available settings. The settings catalog should be compared with the Group Policy Objects (GPO‚Äôs) in the native Active Directory.\nIn the chapter below, I use the settings catalog to configure settings for optimizing Azure Virtual Desktop. I show how to deploy these settings automated. Also, I will show how to assign the configuration profile to the AVD ‚Äì Session hosts group.\nAzure Virtual Desktop optimization settings As you properly know, there is a tool called The Virtual Desktop Optimization Tool. The tool/script helps you set a lot of settings to get the best out of your virtual desktop. Based on the OS version the correct settings are set. Also, things like services and unwanted applications are disabled. And much more.\nIt is a great help and I‚Äôm using it while creating Azure Virtual Desktop images. During my image preparation process, I also run the script. However, there are more options, besides PowerShell, to set the correct settings. What about Microsoft Intune and configuration profiles. I found an article that describes what setting needs to be set in MEM based on the Virtual Desktop Tool.\nFrom that point, I go a step further and show how to deploy AVD optimization settings in MEM all automated.\nFor more information about the settings catalog, check the URL: https://docs.microsoft.com/en-us/mem/intune/configuration/settings-catalog\nDeploy configuration profile automated The next step is deploying the configuration profile with all AVD optimization settings. I managed to put all the settings into a JSON file first. Because it is in a JSON file, I‚Äôm also able to reuse the settings in other environments too. In the body below, I use the avd-optimization.settings.json content as the complete settings package.\n$policyBody = @{ \u0026#34;@odata.type\u0026#34; = \u0026#34;#microsoft.graph.deviceManagementConfigurationPolicy\u0026#34; \u0026#34;name\u0026#34; = \u0026#34;AVD optimization settings\u0026#34; \u0026#34;description\u0026#34; = \u0026#34;AVD Best Practices\u0026#34; \u0026#34;platforms\u0026#34; = \u0026#34;windows10\u0026#34; \u0026#34;technologies\u0026#34; = \u0026#34;mdm\u0026#34; \u0026#34;settings\u0026#34; = @( Get-Content ./avd-optimization.settings.json | ConvertFrom-Json ) } $jsonBody = $policyBody | ConvertTo-Json -Depth 99 $script:PostPolurl = \u0026#34;https://graph.microsoft.com/beta/deviceManagement/configurationPolicies\u0026#34; $policy = Invoke-RestMethod -Uri $script:PostPolurl -Method POST -Headers $script:token -Body $jsonBody -ContentType \u0026#39;Application/json\u0026#39; $policyId = $policy.id The AVD-optimization.settings.json is stored at my GitHub repository.\nAssign configuration profile automated The last step is assigning the just-created configuration profile to the correct AD group. With the code below I create a new GUID again and use the group ID in the earlier step.\n$url = \u0026#34;https://graph.microsoft.com/beta/deviceManagement/configurationPolicies/{0}/assign\u0026#34; -f $policyId $assignBody = @{ \u0026#34;assignments\u0026#34; = @( @{ \u0026#34;@odata.type\u0026#34; = \u0026#34;#microsoft.graph.deviceManagementConfigurationPolicyAssignment\u0026#34; \u0026#34;id\u0026#34; = (New-Guid).Guid \u0026#34;target\u0026#34; = @{ \u0026#34;@odata.type\u0026#34; = \u0026#34;#microsoft.graph.groupAssignmentTarget\u0026#34; \u0026#34;deviceAndAppManagementAssignmentFilterId\u0026#34; = $null \u0026#34;deviceAndAppManagementAssignmentFilterType\u0026#34; = \u0026#34;none\u0026#34; \u0026#34;groupId\u0026#34; = $groupId } } ) } $jsonbody = $assignBody | ConvertTo-Json -Depth 99 Invoke-RestMethod -Uri $url -Headers $script:token -Method POST -Body $jsonbody For more information about configuration policies, check the URL: https://docs.microsoft.com/en-us/graph/api/resources/intune-deviceconfigv2-devicemanagementconfigurationpolicy?view=graph-rest-beta\nThank you for reading this blog about how to secure and optimize AVD and CloudPC with Microsoft Intune automated.\nThank you for reading my blog secure and optimize avd and cloudpc using microsoft intune. I hope you got a bit inspired. Enjoy your day and happy automating üëã ","date":"January 12, 2022","image":"http://localhost:1313/secure-and-optimize-avd-and-cloudpc-using-microsoft-endpoint-manager/standards.jpg","permalink":"/secure-and-optimize-avd-and-cloudpc-using-microsoft-endpoint-manager/","title":"Secure and optimize AVD and CloudPC using Microsoft Intune"},{"categories":["AVD Cocktail","Azure","Azure Virtual Desktop"],"contents":"Welcome to the AVD Automation Cocktail. In this cocktail series, I will show different AVD deployment strategies and languages. In this cocktail, Santa‚Äôs Tree Ride, I will show you how to deploy an AzureAD joined only AVD environment automated with DevOps and the Az.Avd PowerShell module.\nTable Of Contents Recipe Before to drink List DevOps of ingredients Aftertaste Background Introduction to Azure DevOps YAML Pipelines Pipeline variables YAML templates AVD automated with DevOps and Az.Avd Variables Context Resource group Networking Azure Virtual Desktop AVD hostpool, application group and workspace Assign applicationgroup permissions AVD Session hosts Thank you! Recipe In this ‚ÄúSantasRide‚Äù-deployment recipe I will deploy an AVD environment based on AzureAD automated with DevOps and the Az.Avd PowerShell module. Unlike the other cocktails, I am not using the Azure Compute Gallery. I‚Äôm installing a session host directly from the Azure Marketplace (Windows 11) and assigning an Azure AD group to the application group.\nBefore to drink To start enrolling AVD automated with DevOps and Az.Avd you need to have a DevOps organization. Within that organization, you need a project. In that project, you will have to configure a service connection to an Azure tenant. Configuring the DevOps requirements is not the scope of this cocktail. For more info about configuring DevOps the automated way please check my series ‚Äòthe DevOps project‚Äô\nList DevOps of ingredients Az.Avd PowerShell Module Azure DevOps Yaml Aftertaste This cocktail has an Azure DevOps in YAML base as the main ingredient. There is an Az.Avd PowerShell taste. In the end, you will have an AVD environment AAD join environment deployed automated with PowerShell with all the needed resources. These are a hostpool, a workspace, an application group. Also, there are session hosts with some applications installed.\nBackground A little bit about the background. On the left, we have a DevOps repository. I stored some PowerShell files which I‚Äôm going to execute. These PowerShell files deploy the components. I‚Äôm working with YAML templates which come together in a master pipeline. The master pipeline is the one that I‚Äôm going to execute.\nIntroduction to Azure DevOps In contrast to the other cocktails, I feel the need to write a little bit about the techniques I used. This is because these techniques are very important to the whole automation sequence and differ from other cocktails.\nAs I mentioned in earlier cocktails, variables are key in automation. I‚Äôm using variables all the time which helps me avoid providing input all the time.\nIn coding languages like PowerShell, Azure CLI, bash, Python you are able to use variables. In Azure DevOps, it is a bit different because Azure DevOps isn‚Äôt a programming language. But there are options, of course.\nYAML Pipelines Maybe you know there are two ways to create pipelines, classic, and YAML. The classic release pipeline is a clickable page where you are able to create stages and jobs. In this article, I‚Äôm using the other option, YAML.\nOne of the reasons why using YAML is the ability to use output variables over different stages and jobs. Another good reason is that YAML pipelines are saved in the current project‚Äôs repository. This makes the use of version, import, and export possible. Yes, the learning curve is a bit steep but it is worth it.\nhttps://docs.microsoft.com/en-us/azure/devops/pipelines/process/variables?view=azure-devops\u0026amp;tabs=yaml%2Cbatch#use-output-variables-from-tasks\nPipeline variables DevOps provides the ability to use jobs outcome as output variables. To output variables via PowerShell, you need the code below. Make sure you output every value you need with a line of code. Write the output to the host, give\nWrite-Host \u0026#34;##vso[task.setvariable variable=workspaceId;isOutput=true;issecret=false]$($workspace.id)\u0026#34; Make sure you will add the isOutput=true to the line. Otherwise, there is no output.\nYAML templates In DevOps, I love using templates. Using templates helps me keep the code clean and reuse code. If your template is in another repository then your pipeline you will need to add that repository into your pipeline. Because the master pipeline must be able to run in every project I added the repository in the master pipeline by default.\nFor more information about YAML templates check the Microsoft docs\nAVD automated with DevOps and Az.Avd In this chapter, I will explain how to deploy an AVD environment based on AzureAD with DevOps and Az.Avd. The deployment is fully automated with Azure DevOps in YAML and the Az.Avd PowerShell module. I will recommend cloning my AVD GitHub repository to get all the needed files, also for the other cocktails.\nVariables At the beginning of the pipeline, I created a few variables which I need during the pipeline. Pointing to variables in pipelines use $(varname) as value. Using variables at the top is a best practice. In this blog, I also put plain values inside the pipeline, just to clarify what is happening.\nContext Because my service connection has permissions to more than one subscription, I set the subscription context in to my PowerShell files. Otherwise, the service connection will pick the configured subscription Id.\nSet-AzContext -Subscription $subscriptionId Resource group The first step in our deployment is creating a new resource group. In this group, I will deploy all the resources in this blog post. The resource group deployment is configured in a DevOps stage where a start a job template with parameters.\n- stage: BuildResourceGroup dependsOn: - BuildUp jobs: - template: \u0026#39;SantasRide/Pipelines/deploy-resourcegroup.yml@Rozemuller\u0026#39; parameters: serviceConnection: $(serviceConnection) subscriptionId: $(subscriptionId) jobName: deployRG taskName: \u0026#39;rgTask\u0026#39; name: $(resourceGroup) location: $(location) Within the template, I configured an inline PowerShell task to deploy the resource group. In a PowerShell inline task use ${{ parameters.parametername }} to reference the parameter.\n- task: AzurePowerShell@5 name: ${{ parameters.taskName }} enabled: true inputs: azureSubscription: ${{ parameters.serviceConnection }} ScriptType: \u0026#39;InlineScript\u0026#39; Inline: | Set-AzContext -Subscription ${{ parameters.subscriptionId }} $rgParameters = @{ Name = \u0026#34;${{ parameters.name }}\u0026#34; Location = \u0026#39;${{ parameters.location }}\u0026#39; Force = $true } New-AzResourceGroup @rgParameters azurePowerShellVersion: \u0026#39;LatestVersion\u0026#39; Networking After the resource group is deployed, the second step in our ‚ÄúCandyTree‚Äù-deployment is deploying a VNET a subnet, and a network security group. A network security group, or NSG, will help us protect our network from unwanted traffic.\nAt first, I will deploy the NSG with the code below. As you can see the resource group output is used here as a variable. To make this work make sure you have added the needed job in the dependsOn part. I created a stage for the network part. As you can see the stage depends on the previous stage BuildResourceGroup. If you don‚Äôt add this part, this stage will also start to run parallel. Because you can‚Äôt deploy a VNET into a non-existing resource group, I need a resource group first.\n- stage: BuildNetwork dependsOn: - BuildResourceGroup jobs: - template: \u0026#39;SantasRide/Pipelines/deploy-network-resources.yml@Rozemuller\u0026#39; parameters: serviceConnection: $(serviceConnection) subscriptionId: $(subscriptionId) jobName: deployNetwork taskName: \u0026#39;networkTask\u0026#39; resourceGroup: $(resourceGroup) location: $(location) nsgName: \u0026#39;nsg-avd\u0026#39; subnetName: \u0026#39;avd\u0026#39; subnetPrefix: \u0026#39;10.0.1.0/24\u0026#39; vnetName: \u0026#39;vnet-candy-tree\u0026#39; vnetPrefix: \u0026#39;10.0.0.0/16\u0026#39; In the task, I start the deploy-network-resources.ps1 PowerShell file with all the needed parameters.\n- task: AzurePowerShell@5 name: ${{ parameters.taskName }} enabled: true inputs: azureSubscription: ${{ parameters.ServiceConnection }} ScriptType: \u0026#39;FilePath\u0026#39; ScriptPath: \u0026#39;$(Pipeline.Workspace)/drop/Scripts/deploy-network-resources.ps1\u0026#39; ScriptArguments: \u0026#39;-subscriptionId ${{ parameters.subscriptionId }} -resourceGroup \u0026#34;${{ parameters.resourceGroup }}\u0026#34; -location ${{ parameters.location }} -nsgName \u0026#34;${{ parameters.nsgName }}\u0026#34; -subnetName \u0026#34;${{ parameters.subnetName }}\u0026#34; -subnetPrefix \u0026#34;${{ parameters.subnetPrefix }}\u0026#34; -vnetName \u0026#34;${{ parameters.vnetName }}\u0026#34; -vnetPrefix \u0026#34;${{ parameters.vnetPrefix }}\u0026#34;\u0026#39; errorActionPreference: \u0026#39;stop\u0026#39; azurePowerShellVersion: \u0026#39;LatestVersion\u0026#39; Within the PowerShell file, I send the subnet output back as a task variable with:\nWrite-Host \u0026#34;##vso[task.setvariable variable=subnetId;isOutput=true;]$($subnetId.id)\u0026#34; This is the subnet Id I need at the session host deployment step.\nAzure Virtual Desktop After the network is prepared, it is time to deploy the AVD environment. I split the AVD deployment into two parts, the backend and session hosts. In the first part, we deploy the backend with a hostpool, application group, and workspace. Second, I create a registration token and install the session host with the AzureAD extension and AVD extension.\nAVD hostpool, application group and workspace The first step in deploying an AVD environment automated with DevOps and the Az.Avd PowerShell module.\nWithin the stage, I run the template with the needed parameters. These parameters are provided to the PowerShell file. In PowerShell, all the magic happens.\nWithin the template, I start the PowerShell file to deploy AVD resources.\n- task: AzurePowerShell@5 name: ${{ parameters.taskName }} enabled: true inputs: azureSubscription: ${{ parameters.ServiceConnection }} ScriptType: \u0026#39;FilePath\u0026#39; ScriptPath: \u0026#39;$(Pipeline.Workspace)/drop/Scripts/deploy-avd-resources.ps1\u0026#39; ScriptArguments: \u0026#34; -subscriptionId ${{ parameters.subscriptionId }} -resourceGroup ${{ parameters.resourceGroup }} -location \u0026#39;${{ parameters.location }}\u0026#39; -hostpoolName \u0026#39;${{ parameters.hostpoolName }}\u0026#39; -hostpoolDescription\u0026#39; ${{ parameters.hostpoolDescription }}\u0026#39; -hostPoolType \u0026#39;${{ parameters.hostPoolType }}\u0026#39; -loadBalancerType \u0026#39;${{ parameters.loadBalancerType }}\u0026#39; -preferredAppGroupType \u0026#39;${{ parameters.preferredAppGroupType }}\u0026#39; -personalDesktopAssignmentType \u0026#39;${{ parameters.personalDesktopAssignmentType }}\u0026#39; -maxSessionLimit \u0026#39;${{ parameters.maxSessionLimit }}\u0026#39; -startOnConnectRoleName \u0026#39;${{ parameters.startOnConnectRoleName }}\u0026#39; -appGroupName \u0026#39;${{ parameters.appGroupName }}\u0026#39; -appGroupFriendlyName \u0026#39;${{ parameters.appGroupFriendlyName }}\u0026#39; -appGroupDescription \u0026#39;${{ parameters.appGroupDescription }}\u0026#39; -appGroupType \u0026#39;${{ parameters.appGroupType }}\u0026#39; -workspaceName \u0026#39;${{ parameters.workspaceName }}\u0026#39; -workspaceFriendlyName \u0026#39;${{ parameters.workspaceFriendlyName }}\u0026#39; -workspaceDescription \u0026#39;${{ parameters.workspaceDescription }}\u0026#39;\u0026#34; In PowerShell, I‚Äôm installing and importing the Az.Avd PowerShell module. Make sure you set the scope to CurrentUser. This is in case you don‚Äôt have permission to install modules at the system level.\nInstall-Module -Name Az.Avd -AllowPrerelease -Force -Verbose -Scope CurrentUser Import-Module -Name Az.Avd Set-AzContext -Subscription $subscriptionId $hostpoolParameters = @{ HostpoolName = $hostpoolName Description = $hostpoolDescription ResourceGroupName = $ResourceGroupName Location = $location HostpoolType = $hostPoolType LoadBalancerType = $loadBalancerType preferredAppGroupType = $preferredAppGroupType } Write-Verbose \u0026#34;Deploying hostpool $hostpoolName\u0026#34; $avdHostpool = New-AvdHostpool @hostpoolParameters if ($AADJoin) { Update-AvdHostpool -HostpoolName $HostpoolName -ResourceGroupName $ResourceGroupName -CustomRdpProperty \u0026#34;targetisaadjoined:i:1\u0026#34; } The whole PowerShell is stored in my GitHub repository.\nAssign applicationgroup permissions The Az.Avd PowerShell module also has a command to assign permissions to an AVD application group. In the script, I added a command to assign permissions to an application group. I‚Äôm assigning All Users to the group with the command below.\nAdd-AvdApplicationGroupPermissions -ResourceId $application -GroupName \u0026#34;All Users\u0026#34; AVD Session hosts At last, it is time to create a session host. Since update 2.1.1 of the Az.Avd PowerShell module it is possible to create session hosts with Azure AD join. Make sure you will install this version.\nIn the master pipeline, I use the template to deploy AVD session hosts. This is the stage where variables from other stages come together. Because it depends on earlier stages, make sure you add these stages to the DependsOn list.\nIt is looking at the stage in the first place. Second is the jobname, in this case, the deployNetwork and deployAvd jobs. Within the job, it is looking for the task. In this case, the networkTask and avdTask.\nThese tasks execute PowerShell files. In these PowerShell files, I write the output with a name to the host. This is the last part of the variable. In this case the subnetId, hostpoolName and, hostpoolResourceGroup.\nIn the ideal world, you should use output variables for all values.\nIn the task, I provide the parameters to the PowerShell file with script arguments.\nIn the end, in the PowerShell file, I execute new-avdaadsessionhost with these parameters. Also, I set the context and installing the Az.Avd module again. Install modules must be executed in every new PowerShell context (DevOps tasks)\nInstall-Module -Name Az.Avd -AllowPrerelease -Force -Verbose -Scope CurrentUser Import-Module -Name Az.Avd Set-AzContext -Subscription $subscriptionId $avdHostParameters = @{ HostpoolName = $HostpoolName HostpoolResourceGroup = $HostpoolResourceGroup ResourceGroupName = $ResourceGroupName sessionHostCount = $SessionHostCount Location = $Location Publisher = $Publisher Offer = $Offer Sku = $Sku Prefix = $Prefix DiskType = $DiskType LocalAdmin = $LocalAdmin LocalPass = $LocalPass SubNetId = $SubNetId VmSize = $VmSize } New-AvdAadSessionHost @avdHostParameters This is the end of the AVD deployment automated with Azure DevOps and the Az.Avd PowerShell module. For all the please check my [GitHub ](https://github.com/srozemuller/AVD/tree/main/Deployment/StrawberryBanana)[repository](https://github.com/srozemuller/AVD/tree/main/Deployment/SantasRide). Thank you! I hope you liked Santa‚Äôs üéÖ Tree üéÑ Ride üõ∑ and got a bit inspired. Now you know deploy an AVD based on AzureAD with DevOps and the Az.Avd PowerShell module is one of the options. If you like another cocktail feel free to stay and check the AVD Automation Cocktail menu.\nThank you for reading my blog avd automation cocktail ‚Äì deploy avd azuread with devops and az.avd. I hope you got a bit inspired. Enjoy your day and happy automating üëã ","date":"December 16, 2021","image":"http://localhost:1313/avd-automation-cocktail-deploy-avd-azuread-with-devops-and-az-avd/cocktail-bar-automation-avd-santas-ride.png","permalink":"/avd-automation-cocktail-deploy-avd-azuread-with-devops-and-az-avd/","title":"AVD Automation Cocktail ‚Äì Deploy AVD AzureAD with DevOps and Az.Avd"},{"categories":["Automation","Azure","Azure Virtual Desktop","Powershell"],"contents":"In this blog post, I explain how to configure FSLogix for Azure AD-joined machines automated. Also, I explain how to test the configuration and I rearranged the installation procedure in a more efficient way.\nMicrosoft announced the public preview of FSLogix profiles support for Azure AD-joined VMs for Azure Virtual Desktop. The preview allows you to create an Azure Files share to store the FSLogix profiles and configure it to support Azure AD authentication.\nTable Of Contents Before you start What is FSLogix Prerequisites AVD Fslogix best practices Script location Storage Account Resource group Create a storage account and share Assign share permissions Setting Azure AD authentication on the storage account Create an Azure AD application \u0026amp; set permissions Create a service principal Create a storage account key Create service principal \u0026amp; consent Add storage account password Assign directory permissions Configuring AVD Session hosts with FSLogix Testing Before you start What is FSLogix FSLogix is a set of solutions that enhance, enable, and simplify non-persistent Windows computing environments. FSLogix solutions are appropriate for virtual environments in both public and private clouds. For combining FSLogix with Azure Virtual Desktop as a desktop virtualization solution on Azure, I store profiles on Azure Files. In this blog post, I show how to configure Azure Files with Azure AD joined session hosts automated.\nIn addition, please check the documentation about FSLogix: https://docs.microsoft.com/en-us/azure/architecture/example-scenario/wvd/windows-virtual-desktop-fslogix\nPrerequisites Before you start deploying FSLogix with Azure AD joined automated machines make sure you met the prerequisites below.\nWindows 10 or 11 Enterprise single or multi-session, versions 2004 or later with the latest cumulative updates installed, especially the KB5007253 ‚Äì 2021-11 Cumulative Update Preview for Windows 10. Windows Server, version 2022 with the latest cumulative updates installed, especially the KB5007254 ‚Äì 2021-11 Cumulative Update Preview for Microsoft server operating system version 21H2. The user accounts must be hybrid identities. This means the user accounts are synchronized between the Active Directory and Azure AD with AD Sync. To assign Azure Role-Based Access Control (RBAC) permissions for the Azure file share to a user group, you must create the group in Active Directory and sync it to Azure AD. Following my blog post there is also an important thing.\nAt some time you need to assign permissions at the directory level. To prevent users from accessing the user profile of other users, you must also assign directory-level permissions. Microsoft provided a script to run on a local AD joined machine. When looking at the script closely it is only needed to gather some local domain information. So I decided to choose a different route.\nUsing this route saves time, and you don‚Äôt have an extra login moment. Which is key in automation.\nTo use this route make sure you have an Azure virtual machine that is local-domain joined. AVD Fslogix best practices During the configuration, I kept the Azure Files best practices in mind.\nhttps://docs.microsoft.com/en-us/azure/architecture/example-scenario/wvd/windows-virtual-desktop-fslogix#azure-files-best-practices\nIn the script, I use variables and snippets from a complete script. The complete script is stored on my GitHub repository.\nScript location In the past, I got some questions about functions or commands which didn‚Äôt work in this blog post. To clarify. I use snippets in this post. All the needed scripts are stored on my GitHub. You can check it over here.\nStorage Account FSLogix depends on storage. So at first, we will create a storage account and a file share. In the end, this file share is accessible from an Azure AD-joined sessionhost. This file share stores all the profiles. Before beginning, make sure you have installed and imported the Az.Storage PowerShell module.\nInstall-Module Az.Storage Import-Module Az.Storage Resource group Now we have all the commands available, I create a new resource group first. In this resource group, I create the storage account. I store the resource group deployment output into a variable so I can use it later.\n$resourceGroupName = \u0026#34;RG-ROZ-STOR-01\u0026#34; $location = \u0026#34;WestEurope\u0026#34; $rgParameters = @{ resourceGroupName = $resourceGroupName location = $location } $resourceGroup = New-AzResourceGroup @rgParameters Create a storage account and share In this resource group, I create a storage account and a share. Because I need the storage account creation output in the next step, I save them out in a variable. Next, I‚Äôm able to pipe the information into the share creation. An FSlogix best practice is a storage account name with a max of 15 characters. Because of that, I use a random number of max 4 numbers. The name FSlogix with 4 numbers makes 11, so that is fine.\n$storageAccountParameters = @{ Name = \u0026#34;fslogix$(Get-Random -max 10000)\u0026#34; SkuName = \u0026#34;Premium_LRS\u0026#34; Kind = \u0026#34;FileStorage\u0026#34; } $storageAccount = $resourceGroup | New-AzStorageAccount @storageAccountParameters $storageAccount $saShareParameters = @{ Name = \u0026#34;profiles\u0026#34; AccessTier = \u0026#34;Premium\u0026#34; QuotaGiB = 1024 } $saShare = $storageAccount | New-AzRMStorageShare @saShareParameters $saShare Why use New-AzRMStorageShare instead of New-AzStorageShare? First, because of its parameters. I‚Äôm not able to set a quota for example. Another point is the backend explained here: https://github.com/MicrosoftDocs/azure-docs/issues/60129.\nFor more information about creating a file share on an Azure Storage Account, check the documentation.\nAssign share permissions In the next step, the correct permissions are set. There are 2 main permission levels, all authenticated identities or users \u0026amp; groups. Because of security reasons I choose users and groups. With the code below, I‚Äôm assigning a synced Active Directory group with a Storage File Data SMB Share Contributor role to the storage account. The role is a build-in role with a fixed ID.\nThe principal ID is the group object ID gathered from my Azure AD.\n$guid = (new-guid).guid $smbShareContributorRoleId = \u0026#34;0c867c2a-1d8c-454a-a3db-ab2ea1bdc8bb\u0026#34; $roleDefinitionId = \u0026#34;/subscriptions/\u0026#34; + $(get-azcontext).Subscription.id + \u0026#34;/providers/Microsoft.Authorization/roleDefinitions/\u0026#34; + $smbShareContributorRoleId $roleUrl = $script:AzureUrl + $storageAccount.id + \u0026#34;/providers/Microsoft.Authorization/roleAssignments/$($guid)?api-version=2018-07-01\u0026#34; $roleBody = @{ properties = @{ roleDefinitionId = $roleDefinitionId principalId = \u0026#34;f119eef3-fee6-44c6-a692-4d761eccaf7e\u0026#34; # AD Group ID scope = $storageAccount.id } } $jsonRoleBody = $roleBody | ConvertTo-Json -Depth 6 Invoke-RestMethod -Uri $roleUrl -Method PUT -Body $jsonRoleBody -headers $script:AzureToken If you have an existing storage account, use $storageAccount = Get-AzStorageAccount -Name xx -ResourceGroupname xx to retrieve the resource‚Äôs ID.\nSetting Azure AD authentication on the storage account In this step, we enable the Azure AD authentication on the storage account. This option is only available through PowerShell. After executing this command, at the storage account, you will notice the following message in the portal.\n$Uri = $script:AzureUrl + $storageAccount.id + \u0026#34;?api-version=2021-04-01\u0026#34; $kerbBody = @{ properties = @{ azureFilesIdentityBasedAuthentication = @{ directoryServiceOptions = \u0026#34;AADKERB\u0026#34; } } } $kerbJsonBody = $kerbBody | ConvertTo-Json -Depth 99 try { Invoke-RestMethod -Uri $Uri -ContentType \u0026#39;application/json\u0026#39; -Method PATCH -Headers $script:AzureToken -Body $kerbJsonBody; } catch { Write-Host $_.Exception.ToString() Write-Error -Message \u0026#34;Caught exception setting Storage Account directoryServiceOptions=AADKERB: $_\u0026#34; -ErrorAction Stop } Next is configuring a new storage account key for Kerberos. This Kerberos key is a password shared between Azure AD and Azure Storage. Kerberos derives the password‚Äôs value from the first 32 bytes of the storage account‚Äôs kerb1 key. To set the password, run the cmdlets below. Also, this option is only available through PowerShell.\nCreate an Azure AD application \u0026amp; set permissions The next big step in configuring FSlogix for Azure AD joined session hosts automated, is to create an Azure AD application. This application is needed to set permissions. Resources like VMs, users, groups, function apps, etc, have a system identity option. Storage accounts don‚Äôt have this option. This is the reason we need an Azure AD application. This application is representing the storage account in Azure AD.\nTo create an application I use the graph.windows.net API. I know this API is deprecated but at the moment the only API which accepts the ‚Äòstrange‚Äô storage account URLs. I spoke with the Microsoft API product team and they are going to fix this issue in the Graph API. This is the only part where I‚Äôm using this API.\nIn the code below I create an array with the identifier URI and also, the application API permissions.\nopenid: 37f7f235-527c-4136-accd-4a02d197296e User.Read: e1fe6dd8-ba31-4d61-89e7-88639da4683d profile: 14dad69e-099b-42c9-810b-d002981feec1 In the end, I create the application at once.\n$identifierURIs = [System.Collections.Arraylist]::New() $identifierURIs.Add(\u0026#39;HTTP/{0}.file.core.windows.net\u0026#39; -f $storageAccount.StorageAccountName) | Out-Null $identifierURIs.Add(\u0026#39;CIFS/{0}.file.core.windows.net\u0026#39; -f $storageAccount.StorageAccountName) | Out-Null $identifierURIs.Add(\u0026#39;HOST/{0}.file.core.windows.net\u0026#39; -f $storageAccount.StorageAccountName) | Out-Null $url = $script:graphWindowsUrl + \u0026#34;/\u0026#34; + $(get-azcontext).Tenant.Id + \u0026#34;/applications?api-version=1.6\u0026#34; # assign permissions $permissions = @{ resourceAppId = \u0026#34;00000003-0000-0000-c000-000000000000\u0026#34; resourceAccess = @( @{ id = \u0026#34;37f7f235-527c-4136-accd-4a02d197296e\u0026#34; #openid type = \u0026#34;Scope\u0026#34; }, @{ id = \u0026#34;e1fe6dd8-ba31-4d61-89e7-88639da4683d\u0026#34; #User.Read type = \u0026#34;Scope\u0026#34; }, @{ id = \u0026#34;14dad69e-099b-42c9-810b-d002981feec1\u0026#34; #profile type = \u0026#34;Scope\u0026#34; } ) } $body = @{ displayName = $storageAccount.StorageAccountName GroupMembershipClaims = \u0026#34;All\u0026#34; identifierUris = $identifierURIs requiredResourceAccess = @( $permissions ) } $postBody = $body | ConvertTo-Json -Depth 4 $application = Invoke-RestMethod -Uri $url -Method POST -Body $postBody -Headers $script:graphWindowsToken -UseBasicParsing Small note: the granted consent in this picture is set in the next paragraph.\nCreate a service principal To access resources in Azure, we need a service principal. In this next part, I‚Äôm creating a service principal based on the just-created application. To create the service principal based on an application, we need the application ID. Also, I‚Äôm providing the service principal application type. To simplify the steps, I also provide the storage account key.\nCreate a storage account key I‚Äôm creating an extra storage account key (besides the default key1 and key2) with the name kerb1. This is the password used for communication between the storage account and the service principal. After creating, I‚Äôm converting the storage account key to a service principal key and assigning it to the service principal.\n$keyName = \u0026#34;kerb1\u0026#34; $storageAccount | New-AzStorageAccountKey -KeyName $keyName -ErrorAction Stop # Assign password to service principal $kerbKey1 = $storageAccount | Get-AzStorageAccountKey -ListKerbKey | Where-Object { $_.KeyName -eq $keyName } $aadPasswordBuffer = [System.Linq.Enumerable]::Take([System.Convert]::FromBase64String($kerbKey1.Value), 32); $password = \u0026#34;kk:\u0026#34; + [System.Convert]::ToBase64String($aadPasswordBuffer); As mentioned before, creating an extra access key is only available through PowerShell. In the portal, you only have key1 and key2.\nPowerShell output:\n$storageAccount | Get-AzStorageAccountKey -ListKerbKey Create service principal \u0026amp; consent Now we have an application and a storage account password. I provided these values into the body to create a service principal (aka Enterprise applications).\n$url = $Script:GraphApiUrl + \u0026#34;/Beta/servicePrincipals\u0026#34; $body = @{ appId = $application.appId ServicePrincipalType = \u0026#34;Application\u0026#34; } $postBody = $body | ConvertTo-Json $newSp = Invoke-RestMethod -Uri $url -Method POST -Body $postBody -Headers $script:graphApiToken After the service principal is created, we can consent to API permissions at the application level.\nAt first, I‚Äôm looking for the GraphAggregatorService to get the correct object ID. Then I consent at the OpenID, User.Read and profile permissions (must be exactly named, is case sensitive) at the scope part.\n$url = $Script:GraphApiUrl + \u0026#34;/Beta/servicePrincipals?`$filter=appId eq \u0026#39;00000003-0000-0000-c000-000000000000\u0026#39;\u0026#34; $graphAggregatorServiceObjectId = (Invoke-RestMethod -Uri $url -Headers $script:graphApiToken).Value.id $date = Get-Date $url = $($Script:GraphApiUrl) + \u0026#34;/Beta/oauth2PermissionGrants\u0026#34; $body = @{ clientId = $newSp.id consentType = \u0026#34;AllPrincipals\u0026#34; principalId = $null resourceId = $graphAggregatorServiceObjectId scope = \u0026#34;openid User.Read profile\u0026#34; startTime = $date expiryTime = $date } $postBody = $body | ConvertTo-Json Invoke-RestMethod -Uri $url -Method POST -Body $postBody -Headers $script:graphApiToken Add storage account password To add the storage account password, we need the graph.windows.net API again. This is because the password value field is read-only in the Graph API.\n$url = \u0026#34;https://graph.windows.net/\u0026#34; + $(get-azcontext).Tenant.id + \u0026#34;/servicePrincipals/\u0026#34; + $newSp.id + \u0026#34;?api-version=1.6\u0026#34; $body = @{ passwordCredentials = @( @{ customKeyIdentifier = $null startDate = [DateTime]::UtcNow.ToString(\u0026#34;s\u0026#34;) endDate = [DateTime]::UtcNow.AddDays(365).ToString(\u0026#34;s\u0026#34;) value = $password } ) } $postBody = $body | ConvertTo-Json -Depth 6 Invoke-RestMethod -Uri $url -Method PATCH -Body $postBody -Headers $script:graphWindowsToken Assign directory permissions In relation to Microsoft, I managed to skip the local Active Directory part. With the script below you can request the needed information and configure the storage account in one single session. The main reason for me was to skip the login part at a local domain-joined machine. This is because I‚Äôm already logging in and only need the domain information.\nThe main idea is to grab the local domain information within my current session. To make this work, you need an Active Directory domain-joined Azure VM. Otherwise use the script provided by Microsoft.\n$vm = Get-AzVM -name vmname -ResourceGroupName resourcegroup $output = $vm | invoke-azvmruncommand -CommandId \u0026#39;RunPowerShellScript\u0026#39; -ScriptPath \u0026#39;local-domaininfo.ps1\u0026#39; $domainGuid = ($output.Value[0].Message -replace \u0026#39;(?\u0026lt;!:.*):\u0026#39;, \u0026#39;=\u0026#39; | ConvertFrom-StringData).domainGuid $domainName = ($output.Value[0].Message -replace \u0026#39;(?\u0026lt;!:.*):\u0026#39;, \u0026#39;=\u0026#39; | ConvertFrom-StringData).domainName $domainSid = ($output.Value[0].Message -replace \u0026#39;(?\u0026lt;!:.*):\u0026#39;, \u0026#39;=\u0026#39; | ConvertFrom-StringData).domainSid $forestName = ($output.Value[0].Message -replace \u0026#39;(?\u0026lt;!:.*):\u0026#39;, \u0026#39;=\u0026#39; | ConvertFrom-StringData).forestName $netBiosDomainName = ($output.Value[0].Message -replace \u0026#39;(?\u0026lt;!:.*):\u0026#39;, \u0026#39;=\u0026#39; | ConvertFrom-StringData).netBiosDomainName $azureStorageSid = $domainSid + \u0026#34;-123454321\u0026#34; $body = @{ properties = @{ azureFilesIdentityBasedAuthentication = @{ directoryServiceOptions = \u0026#34;AADKERB\u0026#34;; activeDirectoryProperties = @{ domainName = $domainName netBiosDomainName = $netBiosDomainName forestName = $forestName domainGuid = $domainGuid domainSid = $domainSid azureStorageSid = $azureStorageSid } } } } $Uri = $script:AzureUrl + $storageAccount.Id + \u0026#34;?api-version=2021-04-01\u0026#34; $jsonBody = $body | ConvertTo-Json -Depth 99 Invoke-RestMethod -Uri $Uri -ContentType \u0026#39;application/json\u0026#39; -Method PATCH -Headers $script:token -Body $jsonBody Configuring AVD Session hosts with FSLogix The last step in configuring FSLogix for Azure AD joined session hosts automated, is to set the correct registry keys on the AVD session host. This should be a part of the automated session host deployment.\nTo set the correct FSLogix registry keys I wrote a PowerShell script. This script is stored in my GitHub repository. The idea is to run this script via the custom script extension. This type of extension allows me to execute public scripts on Azure virtual machines.\n# Configuring FSLogix $profileLocation = \u0026#34;\\\\$($storageAccount.StorageAccountName).file.core.windows.net\\profiles\u0026#34; $officeLocation = \u0026#34;\\\\$($storageAccount.StorageAccountName).file.core.windows.net\\$($saShare.Name)\u0026#34; $generalParameters = @{ ResourceGroupName = \u0026#34;RG_BPS_WE_AVD_MU\u0026#34; vmName = \u0026#34;AAD-VM-0\u0026#34; Name = \u0026#34;Configure.FSLogix.Keys\u0026#34; } $extensionParameters = @{ Location = \u0026#39;westeurope\u0026#39; FileUri = \u0026#34;https://raw.githubusercontent.com/srozemuller/AVD/main/FsLogix/deploy-fslogix-config.ps1\u0026#34; Run = \u0026#39;deploy-fslogix-config.ps1\u0026#39; Argument = \u0026#34;-profileLocation $profileLocation -officeLocation $officeLocation \u0026#34; ForceReRun = $true } Set-AzVMCustomScriptExtension @generalParameters @extensionParameters Because, it is possible to assign only one custom script extension to a virtual machine, I remove the extension after it has run successfully.\nGet-AzVMExtension @generalParameters | Remove-AzVMExtension -Force After login in again I got the following screen.\nAt the file share on the storage account, I got a folder with my profile in it.\nTesting When all the components are in place the final step is there. We need to test if the session host gets a Kerberos ticket from Microsoftonline.com. I wrote a quick test script that checks on the string Server: krbtgt/KERBEROS.MICROSOFTONLINE.COM @ KERBEROS.MICROSOFTONLINE.COM.\nTo test if a user gets a Kerberos ticket from Microsoft Online run the test script with the Invoke-AzVMRuncommand PowerShell command. In the first test, I ran the test script without the Azure AD Kerberos registry keys. In the second test, I ran the configure FSLogix script first. Thereafter I ran the test script again. Now I got a Kerberos ticket from MicrosoftOnline.com\nFirst I look for the session host. Then I run the Invoke-AzRMRunCommand.\n$vm = Get-azvm -name \u0026#34;AAD-avd-2\u0026#34; -ResourceGroupName \u0026#39;RG-roz-avd-01\u0026#39; $testParameters = @{ aadUserName = \u0026#34;AzureAD\\userprincipalName\u0026#34; azureADUserPwd = \u0026#34;thepasswordIsS3cret\u0026#34; psexecDownloadLocation = \u0026#34;https://download.sysinternals.com/files/PSTools.zip\u0026#34; } $testOutput = $vm | Invoke-AzVMRunCommand -CommandId \u0026#39;RunPowerShellScript\u0026#39; -ScriptPath .\\test-kbrt.ps1 -Parameter $testParameters $testOutput.Value[0].Message In the test script, I use the PSExec tool. Because the Invoke-AzVMRuncommand runs under SYSTEM context, I need a PowerShell box under the correct credentials. Running the ‚Äòklist get krbtgt‚Äô only works under the correct AzureAD user since SYSTEM has no Kerberos tickets.\nAs said, I ran the command first without the AAD Kerberos enabled keys.\nAfter enabling AAD with Kerberos on the session host, I got the output below.\nAt last, I can check the output if the Kerberos string is there.\nif ($testOutput2.Value[0].Message | Select-String -Pattern \u0026#34;Server: krbtgt/KERBEROS.MICROSOFTONLINE.COM @ KERBEROS.MICROSOFTONLINE.COM\u0026#34; -CaseSensitive -SimpleMatch){ Write-Output \u0026#34;Its OK!\u0026#34; } else { Write-Warning \u0026#34;NOT OK!\u0026#34; } Thank you for reading this blog about configuring FSLogix with Azure AD automated. Thank you for reading my blog configure fslogix profile for azure ad joined avd session host automated. I hope you got a bit inspired. Enjoy your day and happy automating üëã ","date":"December 14, 2021","image":"http://localhost:1313/configure-fslogix-profile-for-azure-ad-joined-avd-session-host-automated/fslogix-login.png","permalink":"/configure-fslogix-profile-for-azure-ad-joined-avd-session-host-automated/","title":"Configure FSLogix profile for Azure AD joined AVD session host automated"},{"categories":["Automation","Azure","Azure Virtual Desktop","Microsoft Intune","North Star","Powershell"],"contents":"In this article, I explain how to distribute OneDrive with Microsoft Endpoint Managed based on Winget. After deploying OneDrive I configure a settings catalog configuration policy. This policy has all the OneDrive best practices. At last, a sync device is triggered.\nTable Of Contents Operation North-Star General background info Install application OneDrive best practices Deploy OneDrive best practices automated Image management Sync devices Operation North-Star This blog post is a part of Operation North-Star within the Extend category. What is operation ‚ÄúNorth-Star‚Äù?\nThe main goal of operation ‚ÄúNorth-Star‚Äù is to show a way how to manage the modern workplace based on the North Star framework. Microsoft has released a framework for Microsoft Endpoint Management called ‚ÄòNorth Star‚Äô. This framework is designed to deploy, secure, optimize and manage Windows devices in the cloud.\nFor more information about this series please my blog about the North Star framework.\nGeneral background info In the picture below the red circles are in the scope of this part of the series. Before starting, I recommend start reading the kick-off blog about Operation North-Star.\n![](https://raw.githubusercontent.com/srozemuller/AVD/main/OperationNorthStar/Drawings/operation-northstar-onedrive-w.png)## OneDrive In the transition to the public cloud, OneDrive is very useful when migrating user data. OneDrive is a central place on the Microsoft cloud. It is accessible from anywhere and on any device. When logged in on these devices, data is synchronized to OneDrive. Using OneDrive also helps you keep away from local shared storage or removable storage.\nIn this paragraph, I show how to install and assign the OneDrive application to an AD group. Also, I show the OneDrive best practices and how to configure them in Mircosoft Endpoint Manager. Automated of course.\nInstall application A new way of installing applications is using the Windows Package Manager. Windows Package Manager is a CLI package installation tool, aka Winget. Winget is used for installing applications on Windows 10 and Windows 11. For installing OneDrive I also use this new way of application distribution.\nTo deploy OneDrive as a Win32 application in Microsoft Intune I used the commands below. I use the updated IntuneWin32App PowerShell module to log in with a service principal. Second, I created a PowerShell file. This file is used for configuring a Win32 application in MEM.\nI created a manifest file and stored it in my repository.\nAbout the intunewinfile, I created a general executable that is used with any manifest file. So, I did not create a specific file for OneDrive only. At last, I run the PowerShell script with these parameters.\nConnect-MSIntuneGraph -TenantID $tenantId -ClientID $clientId -ClientSecret $clientSecret $parameters = @{ yamlfile = \u0026#34;https://raw.githubusercontent.com/srozemuller/AVD/main/OperationNorthStar/Scripts/WinGet/manifests/m/Microsoft/OneDrive/21.220.1024.0005/Microsoft.OneDrive.installer.yaml\u0026#34; IntuneWinfile = \u0026#34;C:\\AVD\\OperationNorthStar\\Scripts\\WinGet\\Appinstaller\\Install-WinGetApplication.intunewin\u0026#34; assignToGroupName = \u0026#34;AVD - Session hosts\u0026#34; } .\\Deploy-Win32Apps.ps1 @parameters In addition to this subject, check my blog post about installing Winget applications in Microsoft Intune.\nFor more information about Windows Package Manager check the Microsoft Docs: https://devblogs.microsoft.com/commandline/windows-package-manager-1-1/\nOneDrive best practices In this paragraph, the OneDrive best practices, or ideal state, will pass. These settings are for the best performance, reliability, and user experience. The main goal is to show which settings are configured. And, how to deploy OneDrive best practices automated.\nTo deploy settings, we need configuration policies (again).\nThe needed settings are shown in the picture below within the red square.\nI have chosen to deploy a policy based on the settings catalog. A settings catalog allows me to configure single settings. To deploy a settings catalog policy, you need the /deviceManagement/configuationProfiles REST API URL. This is instead of /deviceManagement/deviceConfiguration URL where update rings are configured for example.\nWhen requesting the configuration policy URL the policies are returned. But you will notice there are no settings in it.\n$script:PostPolurl = \u0026#34;https://graph.microsoft.com/beta/deviceManagement/configurationPolicies\u0026#34; PS\u0026gt; (Invoke-RestMethod -Uri $script:PostPolurl -Method GET -Headers $script:token).value createdDateTime : 02-12-2021 19:21:44 creationSource : description : OneDrive Best Practices lastModifiedDateTime : 02-12-2021 19:21:44 name : Base OneDrive settings platforms : windows10 roleScopeTagIds : {0} settingCount : 9 technologies : mdm id : 7fe5f6e4-1378-4260-b11a-267057e5e3e3 templateReference : @{templateId=; templateFamily=none; templateDisplayName=; templateDisplayVersion=} To get the settings within the policy you need the request the policy settings based on the policy id (eg. 7fe5f6e4-1378-4260-b11a-267057e5e3e3). This with the /deviceManagement/configurationPolicies(‚ÄòpoliciyId‚Äô)/settings URL.\nIn the request below I requested the OneDrive best practices policy. This policy has 9 settings. (as you can see in the output above at the settingCount).\n$script:deviceConfigurl = \u0026#34;https://graph.microsoft.com/beta/deviceManagement/configurationPolicies(\u0026#39;7fe5f6e4-1378-4260-b11a-267057e5e3e3\u0026#39;)/settings\u0026#34; $script:token = GetAuthToken -resource \u0026#39;https://graph.microsoft.com\u0026#39; PS\u0026gt; (Invoke-RestMethod -Uri $script:deviceConfigurl -Method GET -Headers $script:token).value id settingInstance -- --------------- 0 @{@odata.type=#microsoft.graph.deviceManagementConfigurationChoiceSettingInstance; settingDefinitionId=user_vendor_msft_policy_config_onedrivengscv2~policy~onedrivengsc_enableallocsiclients; settingInstanceTemplateReference=; choiceSettingValue=} 1 @{@odata.type=#microsoft.graph.deviceManagementConfigurationChoiceSettingInstance; settingDefinitionId=device_vendor_msft_policy_config_onedrivengscv2~policy~onedrivengsc_diskspacecheckthresholdmb; settingInstanceTemplateReference=; choiceSettingValue=} 2 @{@odata.type=#microsoft.graph.deviceManagementConfigurationChoiceSettingInstance; settingDefinitionId=device_vendor_msft_policy_config_onedrivengscv2~policy~onedrivengsc_gposetupdatering; settingInstanceTemplateReference=; choiceSettingValue=} 3 @{@odata.type=#microsoft.graph.deviceManagementConfigurationChoiceSettingInstance; settingDefinitionId=device_vendor_msft_policy_config_onedrivengscv2~policy~onedrivengsc_kfmoptinnowizard; settingInstanceTemplateReference=; choiceSettingValue=} 4 @{@odata.type=#microsoft.graph.deviceManagementConfigurationChoiceSettingInstance; settingDefinitionId=device_vendor_msft_policy_config_onedrivengscv2~policy~onedrivengsc_silentaccountconfig; settingInstanceTemplateReference=; choiceSettingValue=} 5 @{@odata.type=#microsoft.graph.deviceManagementConfigurationChoiceSettingInstance; settingDefinitionId=device_vendor_msft_policy_config_onedrivengscv2~policy~onedrivengsc_filesondemandenabled; settingInstanceTemplateReference=; choiceSettingValue=} 6 @{@odata.type=#microsoft.graph.deviceManagementConfigurationChoiceSettingInstance; settingDefinitionId=device_vendor_msft_policy_config_storage_allowstoragesenseglobal; settingInstanceTemplateReference=; choiceSettingValue=} 7 @{@odata.type=#microsoft.graph.deviceManagementConfigurationChoiceSettingInstance; settingDefinitionId=device_vendor_msft_policy_config_storage_allowstoragesensetemporaryfilescleanup; settingInstanceTemplateReference=; choiceSettingValue=} 8 @{@odata.type=#microsoft.graph.deviceManagementConfigurationSimpleSettingInstance; settingDefinitionId=device_vendor_msft_policy_config_storage_configstoragesensedownloadscleanupthreshold; settingInstanceTemplateReference=; simpleSettingValue=} A settings catalog policy consists of settings, called a setting instance. Every instance has child settings like the picture below. This picture represents one instance. A red box represents a child setting.\nTo clarify settingsInstances under the hood, I picked the setting which represents the info shown above in the red boxes. I picked value 3 and converted the info to JSON. Every setting has its own block. When looking at the config file at my GitHub repo you will see the whole structure.\n$script:PostPolurl =\u0026#34;https://graph.microsoft.com/beta/deviceManagement/configurationPolicies(\u0026#39;7fe5f6e4-1378-4260-b11a-267057e5e3e3\u0026#39;)/settings\u0026#34; $policy = Invoke-RestMethod -Uri $script:PostPolurl -Method GET -Headers $script:token $policy.value[3] | ConvertTo-Json -Depth 99 { \u0026#34;id\u0026#34;: \u0026#34;3\u0026#34;, \u0026#34;settingInstance\u0026#34;: { \u0026#34;@odata.type\u0026#34;: \u0026#34;#microsoft.graph.deviceManagementConfigurationChoiceSettingInstance\u0026#34;, \u0026#34;settingDefinitionId\u0026#34;: \u0026#34;device_vendor_msft_policy_config_onedrivengscv2~policy~onedrivengsc_kfmoptinnowizard\u0026#34;, \u0026#34;settingInstanceTemplateReference\u0026#34;: null, \u0026#34;choiceSettingValue\u0026#34;: { \u0026#34;settingValueTemplateReference\u0026#34;: null, \u0026#34;value\u0026#34;: \u0026#34;device_vendor_msft_policy_config_onedrivengscv2~policy~onedrivengsc_kfmoptinnowizard_1\u0026#34;, \u0026#34;children\u0026#34;: [ { \u0026#34;@odata.type\u0026#34;: \u0026#34;#microsoft.graph.deviceManagementConfigurationChoiceSettingInstance\u0026#34;, \u0026#34;settingDefinitionId\u0026#34;: \u0026#34;device_vendor_msft_policy_config_onedrivengscv2~policy~onedrivengsc_kfmoptinnowizard_kfmoptinnowizard_dropdown\u0026#34;, \u0026#34;settingInstanceTemplateReference\u0026#34;: null, \u0026#34;choiceSettingValue\u0026#34;: { \u0026#34;settingValueTemplateReference\u0026#34;: null, \u0026#34;value\u0026#34;: \u0026#34;device_vendor_msft_policy_config_onedrivengscv2~policy~onedrivengsc_kfmoptinnowizard_kfmoptinnowizard_dropdown_0\u0026#34;, \u0026#34;children\u0026#34;: [] } }, { \u0026#34;@odata.type\u0026#34;: \u0026#34;#microsoft.graph.deviceManagementConfigurationSimpleSettingInstance\u0026#34;, \u0026#34;settingDefinitionId\u0026#34;: \u0026#34;device_vendor_msft_policy_config_onedrivengscv2~policy~onedrivengsc_kfmoptinnowizard_kfmoptinnowizard_textbox\u0026#34;, \u0026#34;settingInstanceTemplateReference\u0026#34;: null, \u0026#34;simpleSettingValue\u0026#34;: { \u0026#34;@odata.type\u0026#34;: \u0026#34;#microsoft.graph.deviceManagementConfigurationStringSettingValue\u0026#34;, \u0026#34;settingValueTemplateReference\u0026#34;: null, \u0026#34;value\u0026#34;: \u0026#34;\u0026lt;--tenantId--\u0026gt;\u0026#34; } } ] } } } Deploy OneDrive best practices automated Now we have our settings, let‚Äôs create the policy. First, I create a POST body with a name, description, platform, and technologies. The platform and technologies accept a fixed set of values. When reading the docs about the request body, you will notice there is no settings parameter available. However, the URL is accepting this value. So, that allows me to put the OneDrive best practices settings into the policy directly. In my example, I created a OneDrive best practices configuration file. The file is used to input settings data.\nBecause the file is JSON and my body is a PowerShell hashtable, I first revert the content back to PowerShell objects. Then, the POST body is converted to JSON.\n$policyBody = @{ \u0026#34;@odata.type\u0026#34; = \u0026#34;#microsoft.graph.deviceManagementConfigurationPolicy\u0026#34; \u0026#34;name\u0026#34; = \u0026#34;Base OneDrive settings\u0026#34; \u0026#34;description\u0026#34; = \u0026#34;Description value\u0026#34; \u0026#34;platforms\u0026#34; = \u0026#34;windows10\u0026#34; \u0026#34;technologies\u0026#34; = \u0026#34;mdm\u0026#34; \u0026#34;settings\u0026#34; = @( Get-Content ./onedrive-settings.json | ConvertFrom-Json ) } $jsonBody = $policyBody | ConvertTo-Json -Depth 99 $script:PostPolurl = \u0026#34;https://graph.microsoft.com/beta/deviceManagement/configurationPolicies\u0026#34; $policy = Invoke-RestMethod -Uri $script:PostPolurl -Method POST -Headers $script:token -Body $jsonBody -ContentType \u0026#39;Application/json\u0026#39; $policy At last, the policy is made successful.\nImage management If you are deploying OneDrive and its policies within an image, make sure you take a note about the following.\nSilentAccountConfig creates a SilentBusinessConfigCompleted registry entry once SilentAccountConfig has successfully provisioned the user in OneDrive.exe. This prevents SilentAccountConfig from reprovisioning the user in OneDrive.exe if the user manually stops syncing.\nIf SilentAccountConfig has successfully completed on a computer you‚Äôre going to use as your master for building a Windows deployment image (for example, SysPrep), you need to ensure this registry key is removed before you prepare your image. You can do so by running the following command:\nreg delete HKCU\\Software\\Microsoft\\OneDrive /v SilentBusinessConfigCompleted /f https://docs.microsoft.com/en-us/onedrive/use-silent-account-configuration#windows-image-prep-requirements\nIn addition to OneDrive and best practices check the docs: https://docs.microsoft.com/en-us/onedrive/ideal-state-configuration.\nSync devices In the end, the AVD hosts must be synchronized with MEM. By default, a Windows machine syncs every 8 hours. Because I don‚Äôt want to wait for 8 hours, I wrote a simple loop to sync every AVD session host. To find AVD session hosts I‚Äôm filtering the devices list on the SKU family. For the AVD session hosts, the SKU family is EnterpriseMultisession\n$script:devicesUrl = \u0026#34;https://graph.microsoft.com/beta/deviceManagement/managedDevices/\u0026#34; $devices = Invoke-RestMethod -Uri $script:devicesUrl -Method GET -Headers $script:token $devices.value | Where-Object {$_.skuFamily -eq \u0026#39;EnterpriseMultisession\u0026#39;} | ForEach-Object { $devicesSyncUrl = \u0026#34;https://graph.microsoft.com/beta/deviceManagement/managedDevices/\u0026#34; + $_.id + \u0026#34;/syncDevice\u0026#34; Invoke-RestMethod -Uri $devicesSyncUrl -Method POST -Headers $script:token } Thank you for reading this blog about installing OneDrive with OneDrive best practices in Microsoft Intune.\nThank you for reading my blog distribute onedrive with winget and onedrive best practices automated. I hope you got a bit inspired. Enjoy your day and happy automating üëã ","date":"December 3, 2021","image":"http://localhost:1313/distribute-onedrive-with-winget-and-onedrive-best-practices-automated/onedrive.jpg","permalink":"/distribute-onedrive-with-winget-and-onedrive-best-practices-automated/","title":"Distribute OneDrive with Winget and OneDrive best practices automated"},{"categories":["Microsoft Intune"],"contents":"In this post, I will show how to install applications with Windows Package Manager (or Winget), within Microsft Endpoint Manager. This is all done by one single executable package which accepts the application name and version. Or providing a manifest location. Using one single file avoids creating single packages and intunewin-files every time.\nDuring writing the next part of my blog series Operation North Star, application management was one of the subjects. Because application management is one of the biggest investments in Endpoint management, I decided to write a blog about this subject only.\nCreating images for endpoints is can be done in many ways. We are using tools like System Center Configuration Manager (SCCM), Packer, or writing our own packages installed by Azure DevOps. All good of course. In this blog, I use Winget for application distribution. This is with the back of Microsoft Intune. Winget got major improvements in the past months. Major improvements in installation and many application additions. At the moment, there are around 2000 applications available. So in my opinion a good way to deploy applications.\nUnfortunately, there is no way to install Winget applications with MEM directly. This means we are not able (yet) to provide a Winget application with its version in the Apps wizard. However, this will not mean we are not able to install applications with the Winget command line.\nTable Of Contents Main idea What differs from other blogs Deployment strategy Creating an executable Creating the .intunewin file Upload the intunewin file Using Winget in automated image management Main idea So, what are we going to do?\nWell, the main idea is to install Winget applications as a Windows Application (Win32) via Microsoft Intune. Nothing new so far as there are a lot of other blog posts about this subject. So what differs from other blog posts?\nWhat differs from other blogs This thing is that other posts use install commands based on PowerShell.exe and create install packages per application. However these packages are created automated, and it is very time-consuming. I want to deploy applications with one single executable file which accepts the application name and version or a YAML manifest file location. Based on that location, the package downloads all files.\nAh location could be: https://github.com/srozemuller/AVD/tree/main/OperationNorthStar/Scripts/WinGet/manifests/g/Google/Chrome/96.0.4664.45\nUsing a single install file avoids creating packages every time which reduces application management. Which results in fewer mistakes and a more efficient way of application distribution.\nAnother difference is the authentication part. I managed to add a service principal login to Nickolaj‚Äôs IntuneWin32App Powershell module and pushed the changes into a pull request. Feel free to use the updated module in my repository till the moment the module is updated.\nDeployment strategy Now the main idea is clear, it is time to work out the automation sequence. In the upcoming paragraphs, I will show how to start creating your very own executable. Next, I will show how to create an application into Intune as a Win32App.\nCreating an executable Before creating an executable, I first wrote a PowerShell file. In the PowerShell file, I put all the intelligence. I have several reasons for using .exe files. First is a security thing. Changing an executable‚Äôs source is more difficult than changing plain text script files. Another thing is the long installation command like ‚ÄúPowershell.exe -ExecutionPolicy Bypass -File xxx‚Äù. At last, is just a feeling and a bit of experience. Because of running application installations under the System context, a .exe file is more stable than a PowerShell file. During tests, I experienced that PowerShell running under System wasn‚Äôt very stable or running under the wrong context within the PowerShell.\nBut, it all starts with a PowerShell file which I convert to a .exe file with the PS2EXE PowerShell module.\nFor now, I will skip the PowerShell script part, you can find the script at my GitHub repository.\nAssuming you already have a PowerShell file or downloaded the file from my repo, let‚Äôs start creating an executable from a PowerShell file automated.\nAs mentioned you need the PS2EXE Powershell Module. Install this module with the command below.\nInstall-Module PS2EXE Import-Module PS2EXE We have several ways to create an executable from a PowerShell file. Run the ps2exe command only, to show all options you have in the console.\nInvoke-ps2exe -inputFile .\\Install-WinGetApplication.ps1 -outputFile .\\Install-WinGetApplication.exe -iconFile .\\exe.ico -title \u0026#34;WinGet App Installer\u0026#34; -description \u0026#34;This file helps you installing applications with WinGet\u0026#34; If you have errors during compiling about the fact that the Invoke-Ps2exe command is not found or something, try to run the module in a PowerShell 5.1 environment.\nWhen compiling is finished, it is time to create an intunewin file.\nCreating the .intunewin file Before you can add executables as a Win32 application in Intune, we need to prepare the file. During preparation, all files and folders will be encrypted and zipped into a single .intunewin file. The file is pushed as a Win32 app to devices. During deployment at the site of the machine, the Intune Management Extension decrypts and unpacks the intunewin file into its original state.\nCreating an intunewin file from the just-created executable is very easy. Just run the IntuneWinAppUtil.exe and provide the setup folder (-c), the source file (-s), and output folder (-o).\n.\\IntuneWinAppUtil.exe -c .\\AppInstaller -s .\\AppInstaller\\Install-WinGetApplication.exe -o .\\AppInstaller\\ If you need to troubleshoot issues during deployment check the logs at C:\\\\ProgramData\\\\Microsoft\\\\IntuneManagementExtension\\\\Logs on the end user‚Äôs device.\nIn addition to the above check the documentation about preparing the Win32 app for upload.\nFor downloading the preparation tool check the official repository. It is also stored in my repo combined with all other files of this blog.\nUpload the intunewin file After the .intunewin file is ready it is time to create a Win32 application in Microsoft Intune. To do that, I use Nickolaj‚Äôs IntuneWin32App PowerShell module. The module has now also a service principal login option. As mentioned at the start of this post I was able to add an extra feature into the module for authenticating to Intune with a service principal. Because this feature isn‚Äôt added to the module yet (the is a pull request waiting) I would suggest cloning the module from my repository. Make sure you select the correct branch first. After you stored the module on your own device, import the module with the command below in the folder itself. I use the -Force to make sure I have loaded the module with my changes. This is in case you already have loaded the original module.\nImport-Module .\\IntuneWin32App.psd1 -Force After importing the module it is time to log in.\n$tenantId = \u0026#34;\u0026#34; $clientId = \u0026#34;\u0026#34; $clientSecret = \u0026#34;\u0026#34; Connect-MSIntuneGraph -ClientID $clientId -ClientSecret $clientSecret -TenantID $tenantId After login, we are creating the Winget Win32 application in Microsoft Intune.\nI created a script that allows me to provide multiple YAML manifest files. In the script below, I configured Google Chrome as a Win32 application.\n$yamlFile = \u0026#34;https://raw.githubusercontent.com/srozemuller/AVD/main/OperationNorthStar/Scripts/WinGet/manifests/g/Google/Chrome/96.0.4664.45/Google.Chrome.installer.yaml\u0026#34; $yamlFile | ForEach-Object { Try { [string[]]$fileContent = (Invoke-WebRequest -Uri $_ -Headers @{\u0026#34;Cache-Control\u0026#34; = \u0026#34;no-cache\u0026#34; }).content $content = $null foreach ($line in $fileContent) { $content = $content + \u0026#34;`n\u0026#34; + $line } Try { $yamlContent = ConvertFrom-Yaml $content } Catch { Write-Error \u0026#34;Converting YAML not succesfull, $_\u0026#34; } } Catch { Write-Error \u0026#34;He! This location does not exist.\u0026#34; } $detectionRuleParameters = @{ Path = $yamlContent.InstallPath.Substring(0, $yamlContent.InstallPath.LastIndexOf(\u0026#34;\\\u0026#34;)) FileOrFolder = $yamlContent.InstallPath.Substring($yamlContent.InstallPath.LastIndexOf(\u0026#34;\\\u0026#34;) + 1) existence = $true check32BitOn64System = $false DetectionType = \u0026#34;exists\u0026#34; } # Create detection rule $DetectionRule = New-IntuneWin32AppDetectionRuleFile @detectionRuleParameters $manifestLocation = $_.Substring(0,$_.LastIndexOf(\u0026#39;/\u0026#39;)) $appDeployParameters = @{ filePath = $IntuneWinFile.FullName publisher = $yamlContent.PackageIdentifier.Substring(0, $yamlContent.PackageIdentifier.IndexOf(\u0026#39;.\u0026#39;)) displayName = $($yamlContent.PackageIdentifier.Substring($yamlContent.PackageIdentifier.IndexOf(\u0026#39;.\u0026#39;) + 1)).Replace(\u0026#39;.\u0026#39;, \u0026#39; \u0026#39;) description = $yamlContent.PackageDescription appversion = $yamlContent.PackageVersion InstallExperience = \u0026#34;system\u0026#34; RestartBehavior = \u0026#34;suppress\u0026#34; DetectionRule = $DetectionRule InstallCommandLine = \u0026#34;Install-WinGetApplication.exe install -manifestFileLocation $manifestLocation\u0026#34; UninstallCommandLine = \u0026#34;Install-WinGetApplication.exe uninstall -appname $($yamlContent.PackageIdentifier) -appversion $($yamlContent.PackageVersion)\u0026#34; } $appDeployment = Add-IntuneWin32App @appDeployParameters -Verbose $appDeployment Write-Verbose \u0026#34;Group name $groupName provided, looking for group in Azure AD\u0026#34; $graphUrl = \u0026#34;https://graph.microsoft.com\u0026#34; $requestUrl = $graphUrl + \u0026#34;/beta/groups?`$filter=displayName eq \u0026#39;All Users\u0026#39;\u0026#34; $identityInfo = (Invoke-RestMethod -Method GET -Uri $requestUrl -Headers $token).value.id Add-IntuneWin32AppAssignmentGroup -Include -ID $appDeployment.id -GroupID $identityInfo -Intent \u0026#34;available\u0026#34; -Notification \u0026#34;showAll\u0026#34; -Verbose } Using Winget in automated image management Another cool thing about this method is that there is also an option to use this in automated image management. Just provide the executable with the correct YAML file location in Azure DevOps for example. In the example below I created a PowerShell task in DevOps with the Install-WinGetApplication.ps1 file.\n- task: AzurePowerShell@5 name: deployApp displayName: Creating Temp ResourceGroup inputs: azureSubscription: ${{ parameters.serviceConnection }} ScriptType: \u0026#39;InlineScript\u0026#39; Inline: | Set-AzContext -Subscription ${{ parameters.subscriptionId }} $vm = Get-AzVm -Name imagevm $vm | Invoke-AzVMRunCommand -CommandId \u0026#39;RunPowerShellScript\u0026#39; -ScriptPath \u0026#34;$(Build.SourcesDirectory)\\Install-WinGetApplication.ps1\u0026#34; -Parameter @{task = \u0026#34;install\u0026#34;; manifestfilelocation = \u0026#34;\u0026lt;meta charset=\u0026#34;utf-8\u0026#34;\u0026gt;\u0026lt;/meta\u0026gt;$(Build.SourcesDirectory)\\local\\manifest\\path\u0026#34;} azurePowerShellVersion: \u0026#39;LatestVersion\u0026#39; Thank you for reading this blog about installing Winget applications with Microsoft Intune.\nThank you for reading my blog install winget applications using microsoft intune. I hope you got a bit inspired. Enjoy your day and happy automating üëã ","date":"November 22, 2021","image":"http://localhost:1313/install-winget-applications-using-microsoft-endpoint-manager/Image-2273.png","permalink":"/install-winget-applications-using-microsoft-endpoint-manager/","title":"Install Winget applications using Microsoft Intune"},{"categories":["AVD Cocktail","Azure","Azure Virtual Desktop"],"contents":"Welcome (back) to the AVD Automation Cocktail. In the past cocktail series, I showed different AVD deployment strategies and languages. During the time I got some requests about automation with Terraform. In this cocktail, The Dutch Bloody Harry, I‚Äôll show you how to deploy an AVD environment automated with Terraform Cloud and Github. I‚Äôm using the Terraform cloud combined with my GitHub repository.\nTable Of Contents Recipe Before to drink List of Terraform ingredients Aftertaste Start with Terraform Cloud Terraform Cloud organization Terraform Cloud Workspace Terraform Cloud Workspace Variables Terraform Cloud API Terraform syntax Deploying Azure resources with Terraform Cloud Create initial VM for image Resource Group Networking Shared Image Gallery Deploy AVD with Terraform AVD Azure AD join Automated Hostpool Application group Workspace Monitoring Deploy AVD Session hosts with Terraform AVD Extension System identity Create session host Conclusion Thank you! Recipe In this ‚ÄúBloodyHarry‚Äù-deployment recipe I will deploy an AVD environment automated with Terraform Cloud. This recipe slightly differs from the other cocktails. The biggest difference is the way of deployment. First, I create an initial virtual machine to use as the image version‚Äôs source.\nOnce the machine is generalized, I deploy a network and shared image gallery (with the initial version). At last the AVD environment. Within the AVD environment, I deploy diagnostics on Log Analytics. At last, the All Users group is assigned to the application group.\nBefore to drink To begin with this cocktail, it is good to know something about the background. In the picture below I have drawn a high-level schema about the situation. Assuming you already have an Azure tenant with an active subscription. Make sure you have a GitHub account with a repository.\nList of Terraform ingredients These are the main ingredients of this cocktail.\nhttps://www.terraform.io/docs/cloud/workspaces/variables.html#managing-variables-in-the-ui https://registry.terraform.io/providers/hashicorp/azurerm/latest/docs/resources/virtual_machine#identity https://registry.terraform.io/providers/hashicorp/azurerm/latest/docs/resources/virtual_desktop_host_pool https://registry.terraform.io/providers/hashicorp/azurerm/latest/docs/resources/shared_image https://registry.terraform.io/providers/hashicorp/azurerm/latest/docs/resources/windows_virtual_machine Aftertaste This cocktail reaches a sky-high level between two big clouds. It has a sweet taste of PowerShell with a strong Terraform taste on the side. In the end, you will have an AVD environment in Azure deployed with all the needed resources. These are a host pool, a workspace, an application group. Also, there are some session hosts. These hosts have an underlying image from a shared image gallery.\nStart with Terraform Cloud In this chapter, I will explain how to start with Terraform Cloud. I also explain how to create a Terraform workspace and connect it to GitHub. I will recommend cloning my AVD GitHub repository to get all the needed files, also for the other cocktails. The Terraform files are stored at this location First, we are going to set up a Terraform Cloud environment. In basics, Terraform Cloud looks a bit like Azure DevOps. We need an organization, connection to a tenant (in DevOps service connection) and, workbooks (in DevOps pipelines). One big difference between both is that Terraform is only an infrastructure-as-a-code deployment platform. Where in Azure DevOps you also have the ability to run PowerShell tasks for example.\nTerraform Cloud organization To use the Terraform Cloud, we need an organization. Creating an organization is very simple. First, we need to create an account at https://app.terraform.io/. After login, at the top of the page, you are able to create a new organization.\nTerraform Cloud Workspace Before deploying to Azure, we need a workspace. Workspaces represent a collection of infrastructure resources. Each new workspace needs a unique name and needs to know where its Terraform configuration will come from. In this cocktail, I created a workspace to create an AVD infrastructure. When creating a workspace, the wizards ask where to find the configuration.\nAfter choosing version control, several options appear. In my case I choose GitHub. In the case of GitHub, the wizard asks for the repository where the config is at. This can be every repository, private or public. In an empty repository, the workspace is waiting for a configuration.\nTo provide a configuration, make sure you have a correct Terraform syntax file with the .tf or .terraform file extension in the GitHub repository. Use *.auto.tfvars as a variable file. Finally, the workspace will notice the configuration and tells what is in it.\nTerraform Cloud Workspace Variables Due to security reasons, it is not recommended to store provider credentials in a Terraform file. To connect to an Azure tenant, it is recommended to use environment variables. Every cloud provider has its own variable names. Make sure you find the correct names. Setting these correct names as a variable allows Terraform to pick up the correct values. In the case of Azure, I need the following variable names.\nARM_TENANT_ID ARM_SUBSCRIPTION_ID ARM_CLIENT_ID ARM_CLIENT_SECRET In the Terraform configuration file, you only need to set the correct provider.\nterraform { required_providers { azurerm = { source = \u0026#34;hashicorp/azurerm\u0026#34; version = \u0026#34;=2.46.0\u0026#34; } } } # Configure the Microsoft Azure Provider provider \u0026#34;azurerm\u0026#34; { features {} } For additional information about providers and variables, check the Terraform documentation.\nhttps://registry.terraform.io/providers/hashicorp/azurerm/latest/docs#argument-reference\nhttps://www.terraform.io/docs/cloud/workspaces/variables.html#managing-variables-in-the-ui\nTerraform Cloud API As mentioned earlier, Terraform is a deploy infrastructure-as-a-code platform. It is meant for deploying infrastructure resources in Azure. A thing that it does not have is the execution of scripts. To fill in that gap, I have chosen to start these workspaces with a REST API call. That allows me to run PowerShell additionally and, deploy resources with the use of Terraform.\nIn Terraform Cloud means that I need to create an API token. To get a token, go to User Settings -\u0026gt; Tokens and create an API token.\nAs a result, we have set up an organization, a workspace, and a token to get in remotely.\nIn addition, check the URL for more information: https://www.terraform.io/docs/cloud/index.html\nTerraform syntax Before I start deploying resources it is good to know a bit about the syntax. At first sight, it looks very simple. In fact, it is very simple. Every configuration item is a separate code block. Within that block, we have attributes with values, like name = ‚Äòhi\\_my\\_name\\_is‚Äô.\nBlocks can have different purposes. For resource deployment, we need a resource block. To get data out of a tenant, I need a data block. At last, if we need to connect to an Azure tenant, the provider block is needed.\nBesides the block proposal, every block has its own unique name. After a deployment or data query, the output is assigned to that name. In fact, a block name is like using a PowerShell variable to store the output. powershell $hostpool = New-AvdHostpool -xx, for example.\nimage-37 Suggestion: Use the VSCode Terraform extension. It helps you with providing the correct options.\nFor more about Terraform syntax check the documentation: https://www.terraform.io/docs/language/syntax/configuration.html\nDeploying Azure resources with Terraform Cloud This sequence is different from other cocktails. Because there is no option running PowerShell within a Terraform deployment, I create an image version first. Whereafter I deploy the rest of the AVD resources. Because I need two workspaces I create two GitHub repositories. Both fed from those repositories. Another thing is the use of variables. I created three files. Firstly a config file, and a variable file. At last, the third file, I have a x.auto.tfvars file. In this file, all the values are stored. The main idea is only to change the variable file in case of a new deployment.\n![](dutch-bloody-harry-background-workspace.png) ### Authentication Before deploying resources with Terraform, we need a tenant login. Also, we need the correct permissions at the subscription level. At first, we need an application registration in the Azure tenant. The application needs custom role permissions at the subscription level. Also, this application need group read API permissions for assigning roles.\nhttps://registry.terraform.io/providers/hashicorp/azuread/latest/docs/data-sources/group#api-permissions\nThe minimal API needed permissions are:\nDirectory.Read.All or Group.Read.All For creating an application registration automated I used PowerShell. The application has a custom role at the subscription level. I copied the contributor role and added the ‚ÄúMicrosoft.Authorization/*/Write‚Äù action. Actually, I removed the write rule from the notActions. Only a subscription owner has these permissions. Due to security reasons, I created a Contributor Plus role. It is almost the same but then also with Authorization Write permissions.\nThe application also has Group.ReadWrite.All permissions for reading groups via Terraform.\nThe code which is I used is altogether stored in my GitHub repository.\nSuggestion: Make sure you first connect to Azure with Connect-AzAccount.\nCreate initial VM for image Before creating resources with Terraform Cloud, I first created a virtual machine that is sysprepped and generalized. First I create a virtual machine with Terraform. Thereafter I ran a PowerShell command which will Sysprep the machine and generalize it. In the next Terraform workspace, I pick up that machine to create an image version.\n![](dutch-bloody-harry-background-createimage-1.png) To start the initial VM workspace in Terraform, I used the following commands. I would suggest using the auto-apply value (default false). The auto-apply takes care of starting the workbook without user interaction. Besides the token, you also need a workspaceId. To find the workspaceId, check the workspace general settings. $token = \u0026#34;YZLao5Pt6FM6gw.xxxxx\u0026#34; $workspaceId = \u0026#34;ws-Pv8p\u0026#34; $body = @{ data = @{ attributes = @{ message = \u0026#34;From PowerShell\u0026#34; \u0026#34;auto-apply\u0026#34; = $true } type = \u0026#34;runs\u0026#34; relationships = @{ workspace = @{ data = @{ type = \u0026#34;workspaces\u0026#34; id = $workspaceId } } } } } $jsonBody = $body | ConvertTo-Json -Depth 5 $header = @{ Authorization = \u0026#34;Bearer $token\u0026#34; \u0026#34;Content-Type\u0026#34; = \u0026#34;application/vnd.api+json\u0026#34; } $parameters = @{ header = $header method = \u0026#34;POST\u0026#34; body = $jsonBody } $url = \u0026#34;https://app.terraform.io/api/v2/runs\u0026#34; Invoke-RestMethod -Uri $url @parameters The Terraform configuration files are stored in my TF-Image GitHub repository.\nAfter the VM is deployed with Terraform, I use PowerShell. The commands I used to run a Sysprep and generalize the VM are below.\n$vmName = \u0026#39;vm-init\u0026#39; $vm = get-azvm -name $vmName -Status $vm | Invoke-AzVMRunCommand -CommandId \u0026#34;RunPowerShellScript\u0026#34; -ScriptPath .\\sysprep.ps1 do { $vm = get-azvm -name $vmName -Status Write-Host $vm.Status } while($vm.PowerState -ne \u0026#39;VM stopped\u0026#39;) $vm | Set-AzVm -Generalized Resource Group As soon as the image is created, it is time to start the rest of the sequence. This is the part where we need the TF-AVD workspace. Before deploying anything we need to log in and create a new resource group. In the variable file, the content looks like below.\nvariable \u0026#34;avd_rg_name\u0026#34; { type = string description = \u0026#34;This is the AVD resource group\u0026#34; } variable \u0026#34;avd_rg_location\u0026#34; { type = string description = \u0026#34;This is the AVD resource groups location\u0026#34; } The values are stored in the \\*.auto.tfvars file and speaks for itself.\n# In this file all variable values are stored. # Initial Image settings init_vm_name = \u0026#34;vm-init\u0026#34; init_rg_name = \u0026#34;rg-temp-init\u0026#34; At last, the config file.\nresource \u0026#34;azurerm_resource_group\u0026#34; \u0026#34;rg-avd\u0026#34; { name = var.avd_rg_name location = var.avd_rg_location } Networking Networking is a good resource, to begin with. In this step, a new virtual network with a default subnet is deployed. Thereafter a Network Security Group is deployed.\n# Variable part variable \u0026#34;vnet_name\u0026#34; { type = string default = \u0026#34;vnet-roz-bh-001\u0026#34; } variable \u0026#34;vnet_nsg_name\u0026#34; { type = string default = \u0026#34;nsg-roz-bh-001\u0026#34; } variable \u0026#34;vnet_address_space\u0026#34; { type = list default = [\u0026#34;10.0.0.0/16\u0026#34;] } # Configuration part resource \u0026#34;azurerm_virtual_network\u0026#34; \u0026#34;vnet\u0026#34; { name = var.vnet_name location = azurerm_resource_group.rg-avd.location resource_group_name = azurerm_resource_group.rg-avd.name address_space = var.vnet_address_space tags = { environment = \u0026#34;Terraform test\u0026#34; } } resource \u0026#34;azurerm_subnet\u0026#34; \u0026#34;defaultSubnet\u0026#34; { name = \u0026#34;defaultSubnet\u0026#34; resource_group_name = azurerm_resource_group.rg-avd.name virtual_network_name = azurerm_virtual_network.vnet.name address_prefixes = [\u0026#34;10.0.1.0/24\u0026#34;] } resource \u0026#34;azurerm_network_security_group\u0026#34; \u0026#34;nsg\u0026#34; { name = var.vnet_nsg_name location = azurerm_resource_group.rg-avd.location resource_group_name = azurerm_resource_group.rg-avd.name security_rule { name = \u0026#34;allow-rdp\u0026#34; priority = 100 direction = \u0026#34;Inbound\u0026#34; access = \u0026#34;Allow\u0026#34; protocol = \u0026#34;Tcp\u0026#34; source_port_range = \u0026#34;*\u0026#34; destination_port_range = 3389 source_address_prefix = \u0026#34;*\u0026#34; destination_address_prefix = \u0026#34;*\u0026#34; } } resource \u0026#34;azurerm_subnet_network_security_group_association\u0026#34; \u0026#34;nsg_association\u0026#34; { subnet_id = azurerm_subnet.defaultSubnet.id network_security_group_id = azurerm_network_security_group.nsg.id } Shared Image Gallery In fact, the Shared Image Gallery actually consists of three resource types. The gallery, an image definition, and an image version. In the config below I deploy these three at once. Because most resources are quite simple I like to point to the image version.\nThe image version depends on the earlier created virtual machine (in the TF-Image workspace). In this workspace, I look at the data block for the initial VM. The data output is used as an input variable in the resource block.\nThe whole config is stored altogether in the TF-AVD GitHub repository. As you may know, an image version can have different sources. It could be a virtual machine, disk, or a snapshot. In the beginning, I deploy an initial virtual machine. Whereafter the virtual machine is sysprepped and generalized. As a result of generalizing, it is time to use the vm as an image source.\nTo search for resources without deploying them, we need the Terraform data module. To clarify, I stored a snippet below. I search for an Azure resource-type virtual machine. I also search for the resource with a specific name in a specific resource group. The last two values are stored as a variable in the variables file. As a result of the data module, I got VM resource ID. The resource ID is used in the managed_image_id value.\ndata \u0026#34;azurerm_resources\u0026#34; \u0026#34;initVM\u0026#34; { type = \u0026#34;Microsoft.Compute/virtualMachines\u0026#34; name = var.init_vm_name resource_group_name = var.init_rg_name } resource \u0026#34;azurerm_shared_image_version\u0026#34; \u0026#34;sig_version\u0026#34; { name = formatdate(\u0026#34;YYYY.MM.DD\u0026#34;, timestamp()) gallery_name = azurerm_shared_image_gallery.sig.name image_name = azurerm_shared_image.sig_def.name resource_group_name = azurerm_shared_image.sig_def.resource_group_name location = azurerm_shared_image.sig_def.location managed_image_id = data.azurerm_resources.initVM.resources[0].id target_region { name = azurerm_shared_image_gallery.sig.location regional_replica_count = 5 storage_account_type = \u0026#34;Standard_LRS\u0026#34; } } Deploy AVD with Terraform At last, it is time to deploy the Azure Virtual Desktop environment. I‚Äôm deploying a host pool, an application group, and a workspace. I have chosen to use Azure AD joined session hosts. Because of Azure AD join, we need extra settings. In the next paragraph, I clarify these settings.\nAVD Azure AD join Automated Firstly, I deploy a hostpool by adding an extra RDP property to the host pool custom RDP properties, targetisaadjoined:i:1. The host pool also needs the validation environment set. I also set the registration token with the timeAdd function.\nresource \u0026#34;azurerm_virtual_desktop_host_pool\u0026#34; \u0026#34;avd-hp\u0026#34; { location = azurerm_resource_group.rg-avd.location resource_group_name = azurerm_resource_group.rg-avd.name name = var.avd_hostpool_name friendly_name = var.avd_hostpool_friendly_name validate_environment = true start_vm_on_connect = true custom_rdp_properties = \u0026#34;audiocapturemode:i:1;audiomode:i:0;targetisaadjoined:i:1;\u0026#34; description = var.avd_hostpool_description type = var.avd_hostpool_type maximum_sessions_allowed = 1 load_balancer_type = \u0026#34;Persistent\u0026#34; personal_desktop_assignment_type = \u0026#34;Automatic\u0026#34; registration_info { expiration_date = timeadd(timestamp(), \u0026#34;24h\u0026#34;) } } Secondly, we need to assign a Virtual Machine User Login or Virtual Machine Administrator Login role. This role should be assigned at the resource, resource group, or subscription level. For now, I choose the resource group scope. I use the All Users AD group for example.\ndata \u0026#34;azuread_group\u0026#34; \u0026#34;aad_group\u0026#34; { display_name = var.aad_group_name security_enabled = true } data \u0026#34;azurerm_role_definition\u0026#34; \u0026#34;vm_user_login\u0026#34; { name = \u0026#34;Virtual Machine User Login\u0026#34; } resource \u0026#34;azurerm_role_assignment\u0026#34; \u0026#34;vm_user_role\u0026#34; { scope = azurerm_resource_group.rg-avd.id role_definition_id = data.azurerm_role_definition.vm_user_login.id principal_id = data.azuread_group.aad_group.id } Now we have the correct Id‚Äôs it is time to assign the role to the resource group for All Users\nAt last, we need to install the correct VM extension. I will discuss that point later in the session host deployment.\nIn addition to Azure AD join, check the Microsoft docs for more information: https://docs.microsoft.com/en-us/azure/virtual-desktop/deploy-azure-ad-joined-vm\nHostpool To begin with AVD, we need a hostpool first. Besides the well-known parameters, I like to shine a light on the registration info. To illustrate, I create a new registration token for two hours from now.\nresource \u0026#34;azurerm_virtual_desktop_host_pool\u0026#34; \u0026#34;avd-hp\u0026#34; { location = azurerm_resource_group.rg-avd.location resource_group_name = azurerm_resource_group.rg-avd.name name = var.avd_hostpool_name friendly_name = var.avd_hostpool_friendly_name validate_environment = true start_vm_on_connect = true custom_rdp_properties = \u0026#34;audiocapturemode:i:1;audiomode:i:0;targetisaadjoined:i:1;\u0026#34; description = var.avd_hostpool_description type = var.avd_hostpool_type maximum_sessions_allowed = 1 load_balancer_type = \u0026#34;Persistent\u0026#34; personal_desktop_assignment_type = \u0026#34;Automatic\u0026#34; registration_info { expiration_date = timeadd(timestamp(), \u0026#34;2h\u0026#34;) } } Application group Secondly, we need an application group. I used the code below to deploy the application group. The application group is the place where to assign users/groups to the AVD environment.\nresource \u0026#34;azurerm_virtual_desktop_application_group\u0026#34; \u0026#34;desktopapp\u0026#34; { name = var.avd_applicationgroup_name location = azurerm_resource_group.rg-avd.location resource_group_name = azurerm_resource_group.rg-avd.name type = var.avd_applicationgroup_type host_pool_id = azurerm_virtual_desktop_host_pool.avd-hp.id friendly_name = var.avd_applicationgroup_friendly_name description = var.avd_applicationgroup_description } Workspace At last, we have the workspace. This is the place where people authenticate and subscribe at. I‚Äôm also referring to the application output. The result of that output is used to create a workspace name.\nresource \u0026#34;azurerm_virtual_desktop_workspace\u0026#34; \u0026#34;workspace\u0026#34; { name = var.avd_workspace_name location = azurerm_resource_group.rg-avd.location resource_group_name = azurerm_resource_group.rg-avd.name friendly_name = var.avd_workspace_friendly_name description = var.avd_workspace_description } resource \u0026#34;azurerm_virtual_desktop_workspace_application_group_association\u0026#34; \u0026#34;workspaceremoteapp\u0026#34; { workspace_id = azurerm_virtual_desktop_workspace.workspace.id application_group_id = azurerm_virtual_desktop_application_group.desktopapp.id } Monitoring As for every environment, we also like to monitor this environment. To monitor this environment we are going to use Log Analytics. Log Analytics workspaces names are unique. Because of that, I used the random_integer module to create a random number.\nresource \u0026#34;random_integer\u0026#34; \u0026#34;random\u0026#34; { min = 1 max = 50000 } resource \u0026#34;azurerm_log_analytics_workspace\u0026#34; \u0026#34;laws\u0026#34; { name = \u0026#34;${var.laws_name-prefix}-${random_integer.random.result}\u0026#34; location = azurerm_resource_group.rg-avd.location resource_group_name = azurerm_resource_group.rg-avd.name sku = \u0026#34;PerGB2018\u0026#34; retention_in_days = 30 } The random integer created a result 45014. This in combination with the name prefix I configured in the variable file makes it a unique name.\nNext, we need to configure the diagnostic settings for the AVD host pool. In the command below I configure the error logs. The logs are sent to the just-created Log Analytics Workspace.\nresource \u0026#34;azurerm_monitor_diagnostic_setting\u0026#34; \u0026#34;avd-hostpool\u0026#34; { name = \u0026#34;AVD - Diagnostics\u0026#34; target_resource_id = azurerm_virtual_desktop_host_pool.avd-hp.id log_analytics_workspace_id = azurerm_log_analytics_workspace.laws.id log { category = \u0026#34;Error\u0026#34; enabled = true retention_policy { enabled = false } } } Deploy AVD Session hosts with Terraform Last is deploying the session hosts into the AVD hostpool with an Azure AD join. Also, deploy an AVD sessionhost is done with Terraform. I have chosen to join the AVD session hosts to Azure AD. To join Azure AD different settings must be set. The first thing to remember is the Desired State Config extension. The second thing which is different is the VM identity. A not AAD joined VM has no system assigned identity in the Azure AD.\nAVD Extension The AVD module is a Desired State Config (DSC) extension. DSC is a management platform in PowerShell that enables you to ‚Äòpush‚Äô configuration settings as code to a resource. In the case of AVD, means that the extension installs the AVD software with specific AVD environment settings. For example the registration token and the host pool name. Now AADJoin is part of the family a new module came up. This new module accepts the aadJoin parameter, which the native module does not.\nIf you install the native module with the AADJoin parameter you will get a message like below.\n(ArtifactNotFound) The VM extension with publisher ‚ÄòMicrosoft.Azure.ActiveDirectory‚Äô and type ‚ÄòActiveDirectory‚Äô could not be found.\n(VMExtensionProvisioningError) VM has reported a failure when processing extension ‚ÄòDSC‚Äô. Error message: ‚ÄúThe DSC Extension received an incorrect input: A parameter cannot be found that matches parameter name ‚ÄòaadJoin‚Äô.\nAfter digging into the deployment I found the correct artifact URL.\nhttps://wvdportalstorageblob.blob.core.windows.net/galleryartifacts/Configuration_01-19-2023.zip To deploy the AVD DSC extension in Terraform I used the resource block below. Make the settings part in JSON string format. And, make sure the token is provided in the protected settings part. Otherwise will get this error:\nError: Provider produced inconsistent final plan\nWhen expanding the plan for azurerm_virtual_machine_extension.AVDModule[1] to include new values learned so far during apply, provider ‚Äúregistry.terraform.io/hashicorp/azurerm‚Äù produced an invalid new value for .settings: inconsistent values for sensitive attribute. This is a bug in the provider, which should be reported in the provider‚Äôs own issue tracker.\nresource \u0026#34;azurerm_virtual_machine_extension\u0026#34; \u0026#34;AADLoginForWindows\u0026#34; { count = var.avd_sessionhost_count depends_on = [ azurerm_windows_virtual_machine.avd_sessionhost ] name = \u0026#34;AADLoginForWindows\u0026#34; virtual_machine_id = \u0026#34;${azurerm_resource_group.rg-avd.id}/providers/Microsoft.Compute/virtualMachines/${var.avd_sessionhost_prefix}-${count.index}\u0026#34; publisher = \u0026#34;Microsoft.Azure.ActiveDirectory\u0026#34; type = \u0026#34;AADLoginForWindows\u0026#34; type_handler_version = \u0026#34;1.0\u0026#34; settings = \u0026lt;\u0026lt;SETTINGS { \u0026#34;mdmId\u0026#34;: \u0026#34;0000000a-0000-0000-c000-000000000000\u0026#34; } SETTINGS } locals { registration_token = azurerm_virtual_desktop_host_pool.avd-hp.registration_info[0].token } resource \u0026#34;azurerm_virtual_machine_extension\u0026#34; \u0026#34;AVDModule\u0026#34; { count = var.avd_sessionhost_count depends_on = [ azurerm_windows_virtual_machine.avd_sessionhost, azurerm_virtual_machine_extension.AADLoginForWindows ] name = \u0026#34;Microsoft.PowerShell.DSC\u0026#34; virtual_machine_id = \u0026#34;${azurerm_resource_group.rg-avd.id}/providers/Microsoft.Compute/virtualMachines/${var.avd_sessionhost_prefix}-${count.index}\u0026#34; publisher = \u0026#34;Microsoft.Powershell\u0026#34; type = \u0026#34;DSC\u0026#34; type_handler_version = \u0026#34;2.73\u0026#34; settings = \u0026lt;\u0026lt;SETTINGS { \u0026#34;modulesUrl\u0026#34;: \u0026#34;https://wvdportalstorageblob.blob.core.windows.net/galleryartifacts/Configuration_6-1-2021.zip\u0026#34;, \u0026#34;ConfigurationFunction\u0026#34;: \u0026#34;Configuration.ps1\\\\AddSessionHost\u0026#34;, \u0026#34;Properties\u0026#34; : { \u0026#34;hostPoolName\u0026#34; : \u0026#34;${azurerm_virtual_desktop_host_pool.avd-hp.name}\u0026#34;, \u0026#34;aadJoin\u0026#34;: true } } SETTINGS protected_settings = \u0026lt;\u0026lt;PROTECTED_SETTINGS { \u0026#34;properties\u0026#34;: { \u0026#34;registrationInfoToken\u0026#34;: \u0026#34;${local.registration_token}\u0026#34; } } PROTECTED_SETTINGS } System identity The next difference between a native domain joined VM is the resource identity.\nFrom Microsoft: System-assigned Some Azure services allow you to enable a managed identity directly on a service instance. When you enable a system-assigned managed identity, an identity is created in Azure AD that is tied to the lifecycle of that service instance. So when the resource is deleted, Azure automatically deletes the identity for you. By design, only that Azure resource can use this identity to request tokens from Azure AD.\nUse the attribute below for assigning the system identity.\nidentity { type = \u0026#34;SystemAssigned\u0026#34; } In addition to this context please check the following URL for more information: https://docs.microsoft.com/en-us/azure/active-directory/managed-identities-azure-resources/overview\nCreate session host Now it is time to deploy AVD session hosts with Terraform. I decided to deploy two session hosts. This means we have to loop with Terraform. To loop with Terraform, I use the count function based on a variable.\nvariable \u0026#34;avd_sessionhost_count\u0026#34; { type = number default = 2 description = \u0026#34;Number of session host to deploy at first time\u0026#34; } You are able to use the count function in resource deployment.\nBased on the counter index, the code determines the session hosts number. As you can see, there is a dependence on the NIC. Within the network interface block, I‚Äôm doing the same trick. I create 2 NIC‚Äôs whereafter I attach them to a virtual machine. With virtual machine extensions, I also do the same trick.\nFor the whole overview please check my GitHub repository for all the files.\nFinally, in the end, we have Azure AD Join session hosts.\nI have the automated assignment of users to the application group under investigation.\nFor now, everything is in place you only have to assign a user group to the application group and that‚Äôs it.\nConclusion Deploying AVD automated with Terraform Cloud was really fun. I haven‚Äôt used Terraform in the past. The first steps were a bit wonky but after walking a while, it was not that hard to get something running. I was very surprised about the deployment speed. Within minutes a whole environment came up.\nA big advantage of the cloud environment is I don‚Äôt need to build a Terraform myself. The logging is quite well and clear in the overview.\nFor me, as a full automation guy, Terraform fill in my needs for around 80%. As mentioned at the top of this article, Terraform is an infrastructure-as-a-code platform. I also needed PowerShell to achieve my goal. For me, that was a reason to trigger workspaces with PowerShell as well.\nIn fact, this whole sequence is able to run with Azure DevOps for example.\nThank you! I hope you liked the Dutch Bloody Harry and got a bit inspired. Now you know how to deploy AVD automated with Terraform Cloud. If you like another cocktail feel free to stay and check the AVD Automation Cocktail menu.\nEnjoy your day and happy automating üëã\n","date":"October 27, 2021","image":"http://localhost:1313/avd-automation-cocktail-avd-automated-with-terraform-cloud/cocktail-bar-automation-avd-bloody-harry.png","permalink":"/avd-automation-cocktail-avd-automated-with-terraform-cloud/","title":"AVD Automation Cocktail ‚Äì Deploy AVD with Terraform Cloud"},{"categories":["Automation","Azure","Azure Virtual Desktop","Powershell"],"contents":"In this article, I explain how to configure AVD scaling plans automated with the Az.Avd module. This is one of the options to save costs on AVD. A plan which helps you start VM‚Äôs when needed. And stopping VM‚Äôs when not needed. Microsoft announced the public preview of AVD auto-scaling.\nIn Azure the best way is to save costs is to make sure you don‚Äôt ‚Äòclaim‚Äô resources when you don‚Äôt need them. For Azure VM‚Äôs, it means they must be deallocated. This results in session hosts which are unavailable. AVD already knows the ability to start VM‚Äôs on connect at the moment a user connects to it. Start VM on connect works fine at the moment the environment isn‚Äôt used (mostly at night) and every host is down.\nNow there is also AVD Auto Scaling which came in public preview. Scaling plans help you control your AVD environment automatically within a specific time window. If hosts are unused it shutdown hosts (automatically) and start them if the environment needs one.\nGood to know is that scaling plans only work for existing hosts. It is not creating new hosts.\nTable Of Contents Scaling Plans Preview symptoms Create RBAC role automated Create scaling plan Ramp-up Peak hours Ramp down Off-peak hours Create scaling plan automated Scaling Plans As mentioned in the introduction, scaling plans are in public preview. https://docs.microsoft.com/en-us/azure/virtual-desktop/autoscale-scaling-plan?WT.mc_id=EM-MVP-5004578. However this is a nice feature, there are some requirements and some restrictions which need some attention.\nPreview symptoms At this time it is only possible to assign pooled host pools to a scaling plan. I‚Äôve tried it and got an error message like below.\n{‚Äúcode‚Äù:‚Äù400‚Ä≥,‚Äùmessage‚Äù:‚ÄùActivityId: 97ba26e2-b679-4744-946a-936df75ea57e Error: ‚â§{\\‚Äùcode\\‚Äù:\\‚ÄùBadRequest\\‚Äù,\\‚Äùmessage\\‚Äù:\\‚ÄùOnly Pooled Host Pools are supported. Personal Host Pools will be included in a future release. Go to: https://go.microsoft.com/fwlink/?linkid=2146741\\‚Äù,\\‚Äùtarget\\‚Äù:\\‚Äù/xxx.\\‚Äù,\\‚Äùdetails\\‚Äù:null,\\‚ÄùinnerError\\‚Äù:null}‚â•‚Äù}\nTo calculate the host pool‚Äôs total capacity, the maxSessionLimit must be set without using the default 99999 setting.\nUpdate the maxSessionLimit with the code below.\nUpdate-AvdHostpool -HostpoolName avd-hostpool -ResourceGroupName rg-avd-01 -maxSessionLimit 15 Create RBAC role automated The last requirement is granting access to manage power on the VM compute resources.\nAs mentioned earlier, a scaling plan will take care of starting and stopping existing AVD session hosts. Before a plan will be able to do that it needs the correct permissions. These permissions are set on the host pool and the virtual machines, and needed to read the session count for example. If the sessions are zero a host will shut down. Also for the shutdown part, the Windows Virtual Desktop service (yes the old name) needs the correct permissions.\nIn the code below we create a new role and assign it directly to the session host resource group.\nIf you only like to create a role, skip the -Assign parameter.\n$roleParameters = @{ RoleName = \u0026#34;AVD Autoscale Role\u0026#34; RoleDescription = \u0026#34;An autoscale role for AVD\u0026#34; ResourceGroup = \u0026#34;RG-ROZ-AVD-01\u0026#34; } New-AutoScaleRole @roleParameters -Assign image-4\nCreate scaling plan Creating a scaling plan is very easy. But an AVD scaling plan has a lot of options. Some of these options need some extra context. In the upcoming paragraphs, I explain these options.\nimage-2\nRamp-up image-1 Ramp-up means the start time of the autoscale process. In this screenshot, the process will start at 08:00 AM and will balance the available session hosts.\nCapacity threshold\nImagine you have 5 session hosts with a max limit of 10. Then the total WVD hostpool can handle up to 50 sessions. If you have configured a 60% threshold the autoscale will kick in when 60% of 50 sessions has been reached. As a result that at session 53 a new host will be started.\nPeak hours Peaks hours means the time where the maximum of session host should be present for usage.\nRamp down Ramp down is the time where the process starts shutting down machines. The minimum percentage of hosts means the percentage of available hosts after the shutdown process. In this example, the process starts at 18:00. It will look for the host pool‚Äôs capacity. It will look from 18:00 if the total capacity is 50% or less. After that point, the ramp-down process starts.\nOff-peak hours This is the time where minimal hosts are available. If the total hostpool capacity is 50% or less, the system will start a new host.\nCreate scaling plan automated In the commands below I create an AVD-ScalingPlan and assign it to two host pools directly. I also add two scheduled days, Monday and Wednesday.\n$parameters = @{ ScalingPlanName = \u0026#34;AVD-ScalingPlan\u0026#34; ResourceGroupName = \u0026#34;RG-ROZ-AVD-01\u0026#34; location = \u0026#34;WestEurope\u0026#34; HostpoolType = \u0026#34;Pooled\u0026#34; Description = \u0026#34;TestDesc\u0026#34; FriendlyName = \u0026#34;MegaFriendly\u0026#34; AssignToHostPool = @{\u0026#34;Rozemuller-Hostpool\u0026#34; = \u0026#34;RG-ROZ-AVD-01\u0026#34;; \u0026#34;Rozemuller-Hostpool-2\u0026#34; = \u0026#34;RG-ROZ-AVD-01\u0026#34; } ScheduleName = \u0026#34;Schedule\u0026#34; ScheduleDays = @(\u0026#34;Monday\u0026#34;, \u0026#34;WednesDay\u0026#34;) rampUpStartTime = \u0026#34;06:00\u0026#34; rampUpLoadBalancingAlgorithm = \u0026#34;DepthFirst\u0026#34; rampUpMinimumHostsPct = 20 rampUpCapacityThresholdPct = 80 peakStartTime = \u0026#34;08:00\u0026#34; peakLoadBalancingAlgorithm = \u0026#34;BreadthFirst\u0026#34; rampDownStartTime = \u0026#34;18:00\u0026#34; rampDownLoadBalancingAlgorithm = \u0026#34;DepthFirst\u0026#34; rampDownMinimumHostsPct = 20 rampDownCapacityThresholdPct = 50 rampDownForceLogoffUsers = $true rampDownWaitTimeMinutes = 30 rampDownNotificationMessage = \u0026#34;message\u0026#34; offPeakStartTime = \u0026#34;20:00\u0026#34; offPeakLoadBalancingAlgorithm = \u0026#34;DepthFirst\u0026#34; } New-AvdScalingPlan @parameters image-6 The Az.Avd PowerShell module is updated with these two commandlets. Feel free to download and test it.\nInstall-Module Az.Avd Import-Module Az.Avd More information about scaling plans, check the URL: https://docs.microsoft.com/en-us/azure/virtual-desktop/autoscale-scaling-plan?WT.mc_id=EM-MVP-5004578\nThank you for reading this blog on how to enable AVD scaling plans automated.\nThank you for reading my blog how to enable avd scaling plans automated. I hope you got a bit inspired. Enjoy your day and happy automating üëã ","date":"October 20, 2021","image":"http://localhost:1313/how-to-enable-avd-scaling-plans-automated/avd-scaling-startup.png","permalink":"/how-to-enable-avd-scaling-plans-automated/","title":"How to enable AVD scaling plans automated"},{"categories":["Automation","Azure Virtual Desktop","Microsoft Intune","North Star","Powershell"],"contents":"In this article, I explain a way how to manage Windows Updates for Azure Virtual Desktop (AVD). To achieve this goal I use update rings in Microsoft Intune. These update rings will be assigned to a dynamic Azure AD group. I also show how to create and assign this configuration automated with the use of PowerShell.\nTable Of Contents Operation North-Star General background info Device configuration odata types @odata examples Deploy MEM configuration profiles automated Log in Windows Update policy Delivery Optimization Configuration Windows Health Monitoring Assignment Filters Dynamic groups Membership rules Assign Operation North-Star This blog post is a part of Operation North-Star. What is operation ‚ÄúNorth-Star‚Äù?\nThe main goal of operation ‚ÄúNorth-Star‚Äù is showing a way how to manage the modern workplace based on the North Star framework. Microsoft has released a framework for Microsoft Endpoint Management called ‚ÄòNorth Star‚Äô. This framework is designed to deploy, secure, optimize and manage Windows devices in the cloud.\nFor more information about this series please my blog about the North Star framework.\nGeneral background info In the picture below the red circles are in the scope of this part of the series. To get started I recommend start reading the kick-off blog about Operation North-Star.\noperation-northstar-updates\nDevice configuration To manage Windows Updates on the AVD session host automated, we need to create and assign an Update Ring policy. But there is more. Updating machines is only one part of all. After the update, we also need to check the device‚Äôs health. And what about update delivery. You don‚Äôt want your network down because of Windows Updates. All these aspects are managed with device configuration in Microsoft Intune (MEM).\nodata types To get started is good to know how device management works in MEM from an automation perspective. Actually, it all starts at the top of MEM. When looking at device policies, you may have noticed there are different kinds of policies. We have compliance- and configuration policies. And also update rings. In the portal, it all looks different.\nHowever, looking closer to devices and policies, under the hood, they are all at the same place. These policies are requested by all the same REST API URL. This URL is https://graph.microsoft.com/v1.0/deviceManagement/deviceConfigurations. When requesting that URL I got all policies and update rings in one request.\nSo, how is it possible to create different policies with the same REST API URL?\nWell, that‚Äôs the moment where the @odata.type value is popping up. I requested all the policies and selected the display name and the @odata.type. As you can see the types are different. Based on these types you are able to configure the correct policy.\nIn case of creating an Update Ring policy automated, we need the #microsoft.graph.windowsUpdateForBusinessConfiguration type.\nNow we know the correct data type we can look closer into policies. Also there we have to deal with different data types. In the basics, they all look the same with different settings. However, policies do have conditional settings. This means some settings are only available if you select a specific option. Knowing these differences helps you a lot in configuring them the automated way.\n@odata examples To clarify, I created two in basics the same update rings. Based on those two policies I show you the difference. The difference between those rings is the automatic update behavior. ,\nFirst, I created an update ring policy configured with automatic installation of updates. Also, it has a restart moment on any day at 03:00 AM in the morning.\nSecond, I created an update ring policy configured with auto-install at maintenance time. I configured this time between 8 AM and 5 PM.\nAs you can see these settings differ at the @odata.type.\nDeploy MEM configuration profiles automated In the upcoming paragraphs, I show how to deploy configuration profiles automated in MEM. With odata.types in mind, I will configure a Windows Update Ring. And a policy for Delivery Optimization, and Desktop Analytics. All via the M365 REST API and the configuration profiles URL.\nLog in To configure policies, we need to log in. As shown in the operation briefing, I created an application registration with the correct permissions. This is the first time we use the application. To log in I use the command below. After log-in, request a token for the Graph API. I also set the device configuration URL.\n$appId = \u0026#34;\u0026#34; $appPss = \u0026#34;\u0026#34; $sub = \u0026#34;\u0026#34; $ten = \u0026#34;\u0026#34; $PWord = ConvertTo-SecureString -String $appPss -AsPlainText -Force $Credential = New-Object -TypeName \u0026#34;System.Management.Automation.PSCredential\u0026#34; -ArgumentList $appId,$Pword Connect-AzAccount -Credential $Credential -Tenant $ten -Subscription $sub -ServicePrincipal $script:token = GetAuthToken -resource \u0026#39;https://graph.microsoft.com\u0026#39; $script:deviceConfigurl = \u0026#34;https://graph.microsoft.com/beta/deviceManagement/deviceConfigurations\u0026#34;$script:token = GetAuthToken -resource \u0026#39;https://graph.microsoft.com\u0026#39; Windows Update policy These kinds of policies had a lot of options. It comes very precisely from the way how you create a policy automated. I have chosen to follow Microsoft‚Äôs default settings. Also within these policies, we have to deal with odata.types I stored all these settings in the configuration below.\nI picked the auto-install at maintenance example from above. So you can see the whole configuration in its full glory.\n$updateRingBody = @{ \u0026#34;@odata.type\u0026#34; = \u0026#34;#microsoft.graph.windowsUpdateForBusinessConfiguration\u0026#34; description = \u0026#34;This update policy is for AVD hosts only.\u0026#34; displayName = \u0026#34;Update Ring - Azure Virtual Desktop\u0026#34; version = 2 deliveryOptimizationMode = \u0026#34;httpOnly\u0026#34; prereleaseFeatures = \u0026#34;settingsOnly\u0026#34; automaticUpdateMode = \u0026#34;autoInstallAtMaintenanceTime\u0026#34; microsoftUpdateServiceAllowed = $true driversExcluded = $true qualityUpdatesDeferralPeriodInDays = 14 featureUpdatesDeferralPeriodInDays = 90 qualityUpdatesPaused = $false featureUpdatesPaused = $false qualityUpdatesPauseExpiryDateTime = (get-date).AddDays(7) featureUpdatesPauseExpiryDateTime = (get-date).AddDays(14) businessReadyUpdatesOnly = \u0026#34;businessReadyOnly\u0026#34; skipChecksBeforeRestart = $false featureUpdatesRollbackWindowInDays = 60 deadlineForFeatureUpdatesInDays = 14 deadlineForQualityUpdatesInDays = 2 deadlineGracePeriodInDays = 1 postponeRebootUntilAfterDeadline = $false scheduleRestartWarningInHours = 2 scheduleImminentRestartWarningInMinutes = 15 userPauseAccess = \u0026#34;disabled\u0026#34; userWindowsUpdateScanAccess = \u0026#34;enabled\u0026#34; updateNotificationLevel = \u0026#34;defaultNotifications\u0026#34; installationSchedule = @{ \u0026#34;@odata.type\u0026#34; = \u0026#34;#microsoft.graph.windowsUpdateActiveHoursInstall\u0026#34; activeHoursStart = \u0026#34;08:00:00\u0026#34; activeHoursEnd = \u0026#34;17:00:00\u0026#34; } } $postBody = $updateRingBody | ConvertTo-Json -Depth 3 $deployUpdateRing = Invoke-RestMethod -Uri $script:deviceConfigurl -Method POST -Headers $script:token -Body $postBody image-1 In addition to the above please check the following URLs for more information about types.\nhttps://docs.microsoft.com/en-us/graph/api/resources/intune-deviceconfig-windowsupdatescheduledinstall?view=graph-rest-1.0\nhttps://docs.microsoft.com/en-us/graph/api/resources/intune-deviceconfig-windowsupdateinstallscheduletype?view=graph-rest-1.0\nDelivery Optimization Configuration Now the Update Ring is set, it is time to manage the updates to endpoints. As said before, you don‚Äôt want to get your network down because of updates. You can use Delivery Optimization to reduce bandwidth consumption by sharing the work of downloading these packages among multiple devices in your deployment.\nIn the case of a few devices, it may be a bit overkill. But, adding this policy into your automation sequence won‚Äôt cost extra effort. In the end, in enterprise environments, it will save a lot of issues.\nGood to know is that Delivery Optimization is a cloud-managed solution. Access to the Delivery Optimization cloud services is a requirement. This means that in order to use the peer-to-peer functionality of Delivery Optimization, devices must have access to the internet.\n$deliveryBody = @{ \u0026#34;@odata.type\u0026#34; = \u0026#34;#microsoft.graph.windowsDeliveryOptimizationConfiguration\u0026#34; description = \u0026#34;This update policy is for AVD hosts only.\u0026#34; displayName = \u0026#34;Delivery Optimization - Azure Virtual Desktop\u0026#34; version = 2 deliveryOptimizationMode = \u0026#34;httpOnly\u0026#34; restrictPeerSelectionBy = \u0026#34;notConfigured\u0026#34; backgroundDownloadFromHttpDelayInSeconds = 60 foregroundDownloadFromHttpDelayInSeconds = 60 minimumRamAllowedToPeerInGigabytes = 4 minimumDiskSizeAllowedToPeerInGigabytes = 32 minimumFileSizeToCacheInMegabytes = 10 minimumBatteryPercentageAllowedToUpload = 40 maximumCacheAgeInDays = 7 vpnPeerCaching = \u0026#34;notConfigured\u0026#34; cacheServerForegroundDownloadFallbackToHttpDelayInSeconds = 0 cacheServerBackgroundDownloadFallbackToHttpDelayInSeconds = 0 bandwidthMode = @{ \u0026#34;@odata.type\u0026#34; = \u0026#34;#microsoft.graph.deliveryOptimizationBandwidthHoursWithPercentage\u0026#34; bandwidthBackgroundPercentageHours = @{ bandwidthBeginBusinessHours = 8 bandwidthEndBusinessHours = 17 bandwidthPercentageDuringBusinessHours = 25 bandwidthPercentageOutsideBusinessHours = 75 } } } $deliveryPostBody = $deliveryBody | ConvertTo-Json -Depth 3 $deployDeliveryOptimization = Invoke-RestMethod -Uri $script:deviceConfigurl -Method POST -Headers $script:token -Body $deliveryPostBody image-2-2 In addition to the paragraph above, check the URL with a great overview. https://docs.microsoft.com/en-us/mem/intune/configuration/delivery-optimization-settings\nhttps://docs.microsoft.com/en-au/graph/api/resources/intune-deviceconfig-deliveryoptimizationbandwidthpercentage?view=graph-rest-beta\nWindows Health Monitoring The last step is Windows health monitoring. Now the updates are deployed without network outage it is time to check if the updates are installed correctly. This is the moment where Windows health monitoring shows up. The Windows health monitor collects event data and provides recommendations to improve performance on your Windows devices.\n$windowsHealthBody = @{ \u0026#34;@odata.type\u0026#34; = \u0026#34;#microsoft.graph.windowsHealthMonitoringConfiguration\u0026#34; description = \u0026#34;Shows the Windows Updates status\u0026#34; displayName = \u0026#34;Windows Health - Windows Updates\u0026#34; version = 1 allowDeviceHealthMonitoring = \u0026#34;enabled\u0026#34; configDeviceHealthMonitoringScope = \u0026#34;windowsUpdates\u0026#34; } $windowsHealthPostBody = $windowsHealthBody | ConvertTo-Json -Depth 3 $deployHealthMonitor = Invoke-RestMethod -Uri $script:deviceConfigurl -Method POST -Headers $script:token -Body $windowsHealthPostBody image-3 For more information about Windows health monitoring, please check this URL: https://docs.microsoft.com/en-us/mem/intune/configuration/windows-health-monitoring\nThe complete Device Configuration Rest API type list is available at the following location: https://docs.microsoft.com/en-us/graph/api/resources/intune-device-cfg-conceptual?view=graph-rest-1.0\nAssignment The last step to manage Windows Updates on an AVD host is to assign the policy. Because there are several ways to assign update policies to an object, let‚Äôs look at these options in detail. The highest level is users, groups, or devices. Because we are dealing with devices, we have two options left. Those options are groups and devices.\nIn addition, in the case of a user assignment, the policy starts working at the point when a user signs in.\n![image-14(image-14.png)\nFilters As said we have two options which both will work. However, in the case of All Devices, we need to do some extra work. For a while, Microsoft came up with the filters option. Filters help you to narrow the assignment scope of a policy.\nUsing filters can help you with standards. It avoids creating (dynamic) groups for every special situation. Assign at every ‚Äòdevice‚Äô-policy All Devices and enable the particular filter. I‚Äôm working on a piece of automation for creating and assigning filters.\nTo enable filters, go to Tenant Administration on the left, click Filters (Preview). Click on the switch to enable it.\nAfter enabling it the create filter option is available.\nIn the case of AVD, a filter could look like below. First, fill in a name and choose Windows 10 and later.\nimage-17 Second, choose the operatingSystemSKU property with the Equals operator.\nimage-18 Last select the ServerRdsh (Windows 10 Enterprise Multi-Session) value. This is the SKU for multi-session session hosts.\nimage-19 After creating the filter go to the created policy. Assign the policy to All Devices. You will notice a filter option is available. Select the correct filter.\nIn addition to filters check the following URL: https://docs.microsoft.com/en-us/mem/intune/fundamentals/filters\nAs mentioned, I‚Äôm working on filter automation. So for me now the only option left is the ‚ÄúAdd groups. Because I let Microsoft do the job, I have chosen to create a dynamic group.\nDynamic groups The base of dynamic groups is that the group members are added dynamically. Based on the membership rule an object will be a member or not. Creating a dynamic group is based on the membership type.\nIn the body below it is an object which accepts two different types. These types are Unified or dynamicMemberShip.\nMembership rules Dynamic groups depend on membership rules. If configured group types the body needs the memberShipRule. Otherwise, you will get an error like below.\n{\u0026#34;error\u0026#34;:{\u0026#34;code\u0026#34;:\u0026#34;Request_BadRequest\u0026#34;,\u0026#34;message\u0026#34;:\u0026#34;A value is required for property \u0026#39;membershipRule\u0026#39;.\u0026#34;,\u0026#34;innerError\u0026#34;:{\u0026#34;date\u0026#34;:\u0026#34;2021-09-10T11:12:05\u0026#34;,\u0026#34;request-id\u0026#34;:\u0026#34;cc8c3d81-686b-453a-9ad0-61e98ae8daf0\u0026#34;,\u0026#34;client-request-id\u0026#34;:\u0026#34;cc8c3d81-686b-453a-9ad0-61e98ae8daf0\u0026#34;}}} This is the point where I also need the Azure background. I‚Äôm still logged with the just created application (with Reader permissions on the subscription).\nI decided to fill the membership rule based on the computers OU with only AVD machines. To get this AVD machine I look at the machines which start with the AVD prefix.\nI use the Az.Avd PowerShell module to get the AVD prefix. Running the Get-AvdHostpoolInfo will get all the hostpool information. From that information, I‚Äôm searching for the VM template information. This VM template stores all the information for current and future session hosts in a hostpool. However, this value is not mandatory I should suggest using this parameter.\nFor more information about the use of VM templates, please check my blog post about this subject.\n$prefix = Get-AvdHostPool -HostpoolName AVD-Hostpool -ResourceGroupName rg-avd-demo-01 $namePrefix = $prefix.Properties.vmtemplate | ConvertFrom-Json $membershipRules = \u0026#39;(device.organizationalUnit -contains \u0026#34;OU=Computers,DC=domain,DC=local\u0026#34;) and (device.displayName -startsWith \u0026#34;\u0026#39;+$($namePrefix.namePrefix)+\u0026#39;\u0026#34;)\u0026#39; $groupBody = @{ description = \u0026#34;AVD - Session hosts\u0026#34; displayName = \u0026#34;AVD - Session hosts\u0026#34; groupTypes = @( \u0026#34;DynamicMemberShip\u0026#34; ) mailEnabled = $false mailNickname = \u0026#39;avdsh\u0026#39; isAssignableToRole = $false membershipRule = $membershipRules membershipRuleProcessingState = \u0026#39;On\u0026#39; securityEnabled = $true } $group = @{ method = \u0026#34;POST\u0026#34; body = $groupBody | ConvertTo-Json -Depth 2 uri = \u0026#34;https://graph.microsoft.com/v1.0/groups\u0026#34; Headers = $script:token } $groupvalues = Invoke-RestMethod @group After all, when PowerShell has deployed the group with the correct membership rules. Because I need this group later I stored the output into a variable.\nFor more info about groups check this URL: https://docs.microsoft.com/en-us/graph/api/group-post-groups?view=graph-rest-1.0\u0026amp;tabs=http\nAssign At last, this is the part where the puzzle completes. Now we put all the pieces together. The first needed piece is the $groupValues parameter.\nSecond is the update ring deployment output.\nTo assign the correct group to the update ring use the PowerShell below. You may have noticed I stored every deployment into a variable.\n$deployUpdateRing = Invoke-RestMethod -Uri $script:deviceConfigurl -Method POST -Headers $script:token -Body $postBody $deployDeliveryOptimization = Invoke-RestMethod -Uri $script:deviceConfigurl -Method POST -Headers $script:token -Body $deliveryPostBody $deployHealthMonitor = Invoke-RestMethod -Uri $script:deviceConfigurl -Method POST -Headers $script:token -Body $windowsHealthPostBody And that‚Äôs for a reason. Every deployment returns the deployment outcome. This means the policy values including an ID. That‚Äôs the ID I need for the assignment code below. Check the URI in the assignParameters where the $deployUpdateRing variable is used to get the ID.\n$assignBody = @{ \u0026#34;@odata.type\u0026#34; = \u0026#34;#microsoft.graph.deviceConfigurationAssignment\u0026#34; target = @{ \u0026#34;@odata.type\u0026#34; = \u0026#34;#microsoft.graph.groupAssignmentTarget\u0026#34; groupId = $groupvalues.id } } $assignParameters = @{ method = \u0026#34;POST\u0026#34; uri = \u0026#34;https://graph.microsoft.com/v1.0/deviceManagement/deviceConfigurations/$($\u0026lt;meta charset=\u0026#34;utf-8\u0026#34;\u0026gt;\u0026lt;/meta\u0026gt;deployUpdateRing.id)/assignments\u0026#34; Headers = $GraphHeader body = $assignBody | ConvertTo-Json } $assignValues = Invoke-RestMethod @assignParameters assignment-1 Thank you for reading this blog on how to manage Windows Updates for AVD automated.\nThank you for reading my blog manage windows updates for avd using microsoft intune. I hope you got a bit inspired. Enjoy your day and happy automating üëã ","date":"October 14, 2021","image":"http://localhost:1313/manage-windows-updates-for-avd-using-microsoft-endpoint-manager/update-progress.png","permalink":"/manage-windows-updates-for-avd-using-microsoft-endpoint-manager/","title":"Manage Windows Updates for AVD using Microsoft Intune"},{"categories":["Azure Virtual Desktop","Microsoft Intune"],"contents":"Welcome to a brand new series called Operation North Star. In the upcoming series, I will show how to manage AVD environments with Microsoft Intune. I talk about how to manage AVD environments and how to deploy these settings automated. At last, I try to show how to configure a new way of management.\nIn the AVD Automation Cocktail series, I showed how to deploy an AVD environment automated in several ways. For example with PowerShell, Bicep, or DevOps. As said in the upcoming series I will add a new layer for modern management with the main focus on Azure Virtual Desktop. What do you need? Why do you need it? How to deploy all these settings?\nI called this operation ‚ÄòNorth-Star‚Äô. With this purpose, I try to help you further in the world of managing a modern workplace.\nAt least the following subjects will pass:\nUpdate management Security Monitoring Application management Optimizing Altogether, with the back of Microsoft Intune. To complete the series I will show how to deploy all of this automated.\nBecause of overlapping subjects, I have chosen to use this article to explain the background. Information about used techniques and prerequisites. At last how to deploy them. All of this, to avoid extremely long articles which make them hard to read.\nTable Of Contents Why operation ‚ÄòNorth Star‚Äô Microsoft Intune General Operation Info Background Authentication Create Application Application permissions Admin consent Application secret Add application to Azure role Log-in and final check Azure log-in Microsoft 365 log-in Why operation ‚ÄòNorth Star‚Äô Microsoft has released a framework for Microsoft Endpoint Management called ‚ÄòNorth Star‚Äô. This framework is designed to deploy and secure Windows Devices in the cloud. Also, task to optimize and manage them. Because of all these settings, the framework is categorized into main blocks. To clarify more, these main blocks have four sub-blocks. In this blog series, I use the North Star framework to manage Azure Virtual Desktop after it is deployed.\nthumbnail In addition to the introduction, more information is available via the link below.\nhttps://techcommunity.microsoft.com/t5/windows-it-pro-blog/a-framework-for-windows-endpoint-management-transformation/ba-p/2460684\nMicrosoft Intune To manage modern workplaces we use Microsoft Intune (MEM). During this time, MEM has grown into a full management platform. In a Microsoft world a good way to manage client endpoints. Besides that, it also supports you in managing iOS and Android devices. In this series, the main focus is on Windows (10 and 11) devices from an AVD perspective. It helps you deliver a modern workplace and keeps your data secure.\nMicrosoft Intune combines services like Microsoft Intune, Configuration Manager, Desktop Analytics, and Windows Autopilot. These services are part of the Microsoft 365 stack. And, help to secure access, protect data, and responds to risks.\nThese Microsoft 365 services in combination with Microsoft Azure services as Azure Virtual Desktop makes it a really interesting one. From that point, I started building up a new environment.\nGeneral Operation Info In the upcoming paragraphs, I explain how to read this series. Also, I explain the background and techniques I used. At last, I explain what is needed to start and, how to configure all the components.\nBackground To start this series it is good to know a little bit about the background. The North-Star framework has a focus on Windows endpoints. With the management of MEM. This is the left side of the drawing below, Microsoft 365 (M365). Because we deal with Azure Virtual Desktop, I also adding the Azure part. I‚Äôm using Azure information to configure MEM.\nSo, we have Azure and we have MEM. The starting point of MEM is we have nothing configured. The only thing in MEM is the AVD session hosts which are joined in MEM.\nIn Azure, there is an AVD environment with domain-joined virtual machines. The domain-join is based on an on-premises Active Directory with AD sync to AzureAD. The local AD has some users with AVD permissions to log in later.\nFor additional information about hybrid join, please check the posts below. My AVD buddy Stefan Dingemanse has written some blogs about hybrid join. Also, we have recorded a YouTube video about this content.\nIn every operation, you will find a drawing like the above. The current situation and the new situation with a red dashed circle around it. In this operation start-up article, we will configure the M365 application registration in the Azure AD.\nAuthentication The first step in this operation is authentication. Good to know is we have to deal with two environments. These environments are Azure and Microsoft 365. Each environment has its management method. For Azure I use PowerShell. For Microsoft 365 I use the Graph API.\nIn the whole operation, we are connecting to Azure first. From there we also configure Microsoft Intune.\nHuh? But why do I need Azure for configuring Microsoft Intune? Yes, I know this sounds a bit weird but there is a philosophy behind this.\nThe reason why I connect to Azure first is because of the big picture. I like to achieve a full deployment and configuration of the whole AVD environment. That, including the Microsoft 365 part from one point. For example, DevOps with a service connection.\nTo manage one login for both sites we need an application registration in the Azure AD.\nIn the paragraphs below I explain how to authenticate with one single identity against both environments.\nCreate Application When talking about Active Directories we are common with user identities, group, and device objects. Actually, in Azure AD are more identity objects available. In this article, I use the application identity. This application has the correct permissions to log in and to get the needed token for Graph API.\nBecause I can write a blog about creating an application alone, I will only explain how to enroll. For now, I created an application with the correct permissions to write to the Graph API and a client secret.\nThe correct API permission for this context are:\nDeviceManagementManagedDevices.ReadWrite.All DeviceManagementConfiguration.ReadWrite.All Directory.ReadWrite.All Group.ReadWrite.All User.Read To help to create the application with the correct permissions, I wrote some PowerShell functions.\nFirst, I create the application with the function below. Before starting, make sure you are logged in Azure (with Connect-AzAccount). At the New-Application command, we use a display name. I recommend saving the command execution into a variable. This is because we need the returned ID in later steps. (An application display name is NOT unique).\nfunction GetAuthToken($resource) { $context = [Microsoft.Azure.Commands.Common.Authentication.Abstractions.AzureRmProfileProvider]::Instance.Profile.DefaultContext $Token = [Microsoft.Azure.Commands.Common.Authentication.AzureSession]::Instance.AuthenticationFactory.Authenticate($context.Account, $context.Environment, $context.Tenant.Id.ToString(), $null, [Microsoft.Azure.Commands.Common.Authentication.ShowDialog]::Never, $null, $resource).AccessToken $authHeader = @{ \u0026#39;Content-Type\u0026#39; = \u0026#39;application/json\u0026#39; Authorization = \u0026#39;Bearer \u0026#39; + $Token } return $authHeader } function New-Application { param ( [Parameter(Mandatory)] [ValidateNotNullOrEmpty()] [string]$AppDisplayName ) $url = $script:mainUrl + \u0026#34;/applications\u0026#34; $body = @{ displayName = $AppDisplayName } $postBody = $body | ConvertTo-Json $newApp = Invoke-RestMethod -Uri $url -Method POST -Body $postBody -Headers $script:token return $newApp } $script:token = GetAuthToken -resource \u0026#39;https://graph.microsoft.com\u0026#39; $script:mainUrl = \u0026#34;https://graph.microsoft.com/beta\u0026#34; $newApp = New-Application -AppDisplayName \u0026#34;MEM Configurator\u0026#34; # Sample response PS\u0026gt; $newApp @odata.context : https://graph.microsoft.com/beta/$metadata#applications/$entity @odata.id : https://graph.microsoft.com/v2/xxxxxxx-ae68-4a71-9c92-8508128134a2/directoryObjects/xxxxxx-91c7-497c-9175-xxxxxxxxxxxx/Microsoft.DirectoryServices.Application id : xxxx-91c7-497c-9175-xxxxxxxxxxxx deletedDateTime : appId : xxxx-2f68-4fa9-9bee-xxxxxxxxxxxx applicationTemplateId : identifierUris : {} createdDateTime : 01-10-2021 15:31:24 description : disabledByMicrosoftStatus : displayName : MEM Configurator isAuthorizationServiceEnabled : False Next, after finishing the task you will notice the application is empty. Which means that the application has no permissions at all.\nApplication permissions When adding permissions automated, we need IDs. From this point, it becomes a bit nasty. Finding IDs isn‚Äôt that simple. The old PowerShell module AzureAD and AzureADPreview had some commands which helps a bit. Because I don‚Äôt want to depend on those old modules I tried another way with REST API. Unfortunately, I couldn‚Äôt find the correct commands, yet. But I know they are somewhere.\nBecause I need those IDs once, I decided to use Azure CLI to get those IDs. I know all the permissions are under the Microsoft Graph application. The only thing I changed was the permission value. (It is case-sensitive). I searched in the Microsoft Graph application for all the needed permissions. The ID and value output are needed in the add permissions function.\naz ad sp list --query \u0026#34;[?appDisplayName==\u0026#39;Microsoft Graph\u0026#39;].{permissions:oauth2Permissions}[0].permissions[?value==\u0026#39;User.Read\u0026#39;].{id: id, value: value, adminConsentDisplayName: adminConsentDisplayName, adminConsentDescription: adminConsentDescription}[0]\u0026#34; --all { \u0026#34;adminConsentDescription\u0026#34;: \u0026#34;Allows users to sign-in to the app, and allows the app to read the profile of signed-in users. It also allows the app to read basic company information of signed-in users.\u0026#34;, \u0026#34;adminConsentDisplayName\u0026#34;: \u0026#34;Sign in and read user profile\u0026#34;, \u0026#34;id\u0026#34;: \u0026#34;e1fe6dd8-ba31-4d61-89e7-88639da4683d\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;User.Read\u0026#34; } After all, we have the main object ID and permission IDs.\nMicrosoft Graph : 00000003-0000-0000-c000-000000000000 User.Read : e1fe6dd8-ba31-4d61-89e7-88639da4683d Group.ReadWrite.All : 4e46008b-f24c-477d-8fff-7bb4ec7aafe0 Directory.ReadWrite.All : c5366453-9fb0-48a5-a156-24f0c49a4b84 DeviceManagementManagedDevices.ReadWrite.All : 243333ab-4d21-40cb-a475-36241daa0842 (Role) DeviceManagementConfiguration.ReadWrite.All : 9241abd9-d0e6-425a-bd4f-47ba86e767a4 (Role) I wrote some functions which allow me to add permissions to the created application.\nThe Get-Application function is missing in this example but you can find it on my GitHub repository.\nfunction Add-ApplicationPermissions { param ( [Parameter(Mandatory)] [ValidateNotNullOrEmpty()] [string]$appId, [Parameter(Mandatory)] [ValidateNotNullOrEmpty()] [object]$permissions ) $url = $($script:mainUrl) + \u0026#34;/applications/\u0026#34; + $appId $body = @{ requiredResourceAccess = @( $permissions ) } $postBody = $body | ConvertTo-Json -Depth 5 $appPermissions = Invoke-RestMethod -Uri $url -Method PATCH -Body $postBody -Headers $script:token -ContentType \u0026#34;application/json\u0026#34; return $appPermissions } $permissions = @{ resourceAppId = \u0026#34;00000003-0000-0000-c000-000000000000\u0026#34; resourceAccess = @( @{ id = \u0026#34;e1fe6dd8-ba31-4d61-89e7-88639da4683d\u0026#34; type = \u0026#34;Scope\u0026#34; }, @{ id = \u0026#34;4e46008b-f24c-477d-8fff-7bb4ec7aafe0\u0026#34; type = \u0026#34;Scope\u0026#34; }, @{ id = \u0026#34;c5366453-9fb0-48a5-a156-24f0c49a4b84\u0026#34; type = \u0026#34;Scope\u0026#34; }, @{ id = \u0026#34;9241abd9-d0e6-425a-bd4f-47ba86e767a4\u0026#34; type = \u0026#34;Role\u0026#34; }, @{ id = \u0026#34;243333ab-4d21-40cb-a475-36241daa0842\u0026#34; type = \u0026#34;Role\u0026#34; } ) } Add-ApplicationPermissions -AppId $newApp.appId -permissions $permissions After executing the function, the permissions are set. As shown in the screenshot below, we need more. The permissions need admin consent.\nAdmin consent Admin consent allows the application to execute tasks under the admin context. When consenting to an application, in the background a service principal is created. A service principal, or enterprise application, is used to manage how the registration behaves in the tenant. So, an enterprise application is representing the inner application in the Azure AD.\nIf you consent permissions through the portal, an enterprise application is created at the background automated. When automating, you need to create this application manual :). No not by hand but you have to create an enterprise application first.\nThe function below searches for the application registration and creates a service principal from that application. Also this time I save the output into a variable.\nfunction New-SPFromApp { param ( [Parameter(Mandatory)] [ValidateNotNullOrEmpty()] [string]$AppId ) $url = \u0026#34;$($script:mainUrl)/servicePrincipals\u0026#34; $body = @{ appId = $AppId } $postBody = $body | ConvertTo-Json $servicePrincipal = Invoke-RestMethod -Uri $url -Method POST -Body $postBody -Headers $script:token return $servicePrincipal } $newSp = New-SPFromApp -AppId $newApp.appId After the service principal is created, we can consent. This PowerShell function below allows you to consent admin-permissions to an application. The Get-ServicePrincipal PowerShell function is available in my GitHub repository.\nThe function asks for the application display name (for example, MEM Configurator). The resourceId and the scope. The resourceId is the id of the GraphAggregator service principal. The scope is a string value with all the permissions in it.\nFor example: ‚ÄúUser.Read Directory.ReadWrite.All Group.ReadWrite.All DeviceManagementConfiguration.ReadWrite.All‚Äù\nConsent-ApplicationPermissions -ServicePrincipalId $newSp.id -ResourceId \u0026#34;3f73b7e5-80b4-4ca8-9a77-8811bb27eb70\u0026#34; -Scope \u0026#34;User.Read Directory.ReadWrite.All Group.ReadWrite.All DeviceManagementConfiguration.ReadWrite.All DeviceManagementManagedDevices.ReadWrite.All\u0026#34; The Consent-ApplicationPermissions sets the permissions. Because we also have to deal with application roles, we need to add these roles to the service principal. The commands below set the app role for both applications. (The function is available in the script)\nAssign-AppRole -ServicePrincipalId $newSp.id -appRoleId \u0026#34;9241abd9-d0e6-425a-bd4f-47ba86e767a4\u0026#34; Assign-AppRole -ServicePrincipalId $newSp.id -appRoleId \u0026#34;243333ab-4d21-40cb-a475-36241daa0842\u0026#34; Application secret To log in with an application we need a password as well. This password is called a client secret. To create the secret I wrote the PowerShell function below, Create-ApplicationPassword. It only asks for the application name and returns the secret value.\nfunction New-ApplicationPassword { param ( [Parameter(Mandatory)] [ValidateNotNullOrEmpty()] [string]$AppId ) $url = $script:mainUrl + \u0026#34;/applications/\u0026#34;+ $AppId + \u0026#34;/addPassword\u0026#34; $body = @{ passwordCredential = @{ displayName = \u0026#39;AppPassword\u0026#39; } } $postBody = $body | ConvertTo-Json $appPass = Invoke-RestMethod -Uri $url -Method POST -Body $postBody -Headers $script:token return $appPass } $appPass = New-ApplicationPassword -AppId $newApp.id @odata.context : https://graph.microsoft.com/beta/$metadata#microsoft.graph.passwordCredential customKeyIdentifier : endDateTime : 23-09-2023 18:55:28 keyId : 5360bc7a-b3f5-479 startDateTime : 23-09-2021 18:55:28 secretText : f5h7Q~PJ5jYn22pELrz hint : f5h displayName : AppPassword Add application to Azure role At last, make sure the application has at least Reader permissions on the subscription level. This is because I log in via Connect-AzAccount. In the case of deploying resources, you need at least Contributor permissions. Make sure you request a new token for https://management.azure.com.\nFirst, request the service principal info. The ID is used in the assignment.\n$script:token = GetAuthToken -resource \u0026#39;https://graph.microsoft.com\u0026#39; $servicePrincipalInfo = Get-ServicePrincipal -appDisplayName $AppDisplayName Next, requesting a new token for management.azure.com. Assign the Reader role to the service principal at the subscription level scope.\nFor more information about scopes, check step 4 at the Microsoft documentation.\n$script:token = GetAuthToken -resource \u0026#39;https://management.azure.com\u0026#39; $guid = (new-guid).guid $roleDefinitionId = \u0026#34;/subscriptions/xxxxx/providers/Microsoft.Authorization/roleDefinitions/a795c7a0-d4a2-40c1-ae25-d81f01202912\u0026#34; $url = \u0026#34;https://management.azure.com/subscriptions/xxxxx/providers/Microsoft.Authorization/roleAssignments/$($guid)?api-version=2018-07-01\u0026#34; $body = @{ properties = @{ roleDefinitionId = $roleDefinitionId principalId = $servicePrincipalInfo.id } } $jsonBody = $body | ConvertTo-Json -Depth 6 Invoke-RestMethod -Uri $url -Method PUT -Body $jsonBody -headers $script:token Additional to built-in roles and IDs check the Microsoft documentation.\nLog-in and final check Finally, we have a registered application with the correct permissions and a secret. Now it is time to log in to the tenant again. This time with the application. Besides the fact, we have to log in to two environments, we need to log in once. After log-in, we requesting different authentication tokens. One for Azure, and later for Microsoft 365.\nAzure log-in We start the log-in at Azure with Connect-AzAccount. The use of the -ServicePrincpal flag makes it possible to connect to Azure with a service principal, the registered application. We log in with these credentials from now. To test the account, I requested the session hosts within the Azure context\n$tenantId = \u0026#34; \u0026#34; $subscriptionId = \u0026#34; \u0026#34; $clientId = $newApp.id $clientSecret = $appPass.secretText #Credentials $passwd = ConvertTo-SecureString $clientSecret -AsPlainText -Force $pscredential = New-Object System.Management.Automation.PSCredential($clientId, $passwd) Connect-AzAccount -ServicePrincipal -Credential $pscredential -Tenant $tenantID -Subscription $subscriptionId (Get-AvdSessionHost -HostPoolName Rozemuller-hostpool -ResourceGroupName rg-roz-avd-01).value name id ---- -- Rozemuller-Hostpool/rz-avd-0.rozemuller.local /subscriptions/xxx/resourcegroups/RG-ROZ-AVD-01 Rozemuller-Hostpool/rz-avd-1.rozemuller.local /subscriptions/xxx/resourcegroups/RG-ROZ-AVD-01 For more info about log-in with a service principal, check the Microsoft documentation.\nMicrosoft 365 log-in Second, I requested the endpoints in Microsoft Intune. This time I first requested a token for the Graph API. The Graph API is used for managing Microsoft 365 components.\n$token = GetAuthToken -resource \u0026#34;https://graph.microsoft.com\u0026#34; $url = \u0026#34;https://graph.microsoft.com/beta/deviceManagement/managedDevices\u0026#34; (Invoke-RestMethod -Method GET -Headers $token -Uri $url).value | select deviceName, joinType, skuFamily deviceName joinType skuFamily ---------- -------- --------- rz-avd-1 hybridAzureADJoined EnterpriseMultisession \u0026lt;meta charset=\u0026#34;utf-8\u0026#34;\u0026gt;\u0026lt;/meta\u0026gt;rz-avd-2 hybridAzureADJoined EnterpriseMultisession Thank you for reading this kick-off blog about managing AVD environments automated with Microsoft Intune.\nThank you for reading my blog manage avd automated with microsoft intune. I hope you got a bit inspired. Enjoy your day and happy automating üëã ","date":"October 7, 2021","image":"http://localhost:1313/manage-avd-automated-with-microsoft-endpoint-manager/Image-2029.png","permalink":"/manage-avd-automated-with-microsoft-endpoint-manager/","title":"Manage AVD automated with Microsoft Intune"},{"categories":["Automation","Azure","Azure Virtual Desktop","Powershell"],"contents":"In automation using templates is key! Using templates makes life easier. No new information I guess. If you run tasks in DevOps, Terraform, or other engines you want to reuse code blocks. When deploying resources in Azure we use ARM or Bicep templates. Good to know there is also is a sort of ‚Äòin place‚Äô-template available for AVD. This is the VM template and stored under the host pool information. In this blog post, I explain how to set and use VM templates. This will help you with the AVD sessionhost automation.\nTable Of Contents Templates Automation best practices (for me) VM-templates explained How to check VM templates Portal Code Enable VM templates Add a VM template Where does it help? Hostpool purpose and VM hardware Image location Custom objects Considerations Templates During my travel in the world of AVD, I tested lots of automation sequences. Some of them I wrote down in the AVD Automation Cocktail. All of these cocktails have one thing in common, the use of templates (Bicep, ARM, YAML). Actually, even PowerShell functions are a sort of template.\nIn the end, all ‚Äòdeployment‚Äô-templates having the same goal: make sure you don‚Äôt have to repeat code and can use them in generic.\nAll these kinds of templates have one thing in common, they are file-based. When using ARM or Bicep, we have .json or .bicep (parameter) files. Azure DevOps has YAML templates and PowerShell has modules (like Az.Avd) and functions.\nIn the case of Azure Resource Manager (ARM), we also have the ability to create template through the portal just before the deployment start.\nAutomation best practices (for me) To make my own automation life also a bit easier I created some automation best practices from where I start. It is not a list of rules but it helps me a lot working out automation strategies.\nDon‚Äô repeat yourself (use templates and, in scripts, functions) Get your sequence unattended as much as possible (use enviroment information, in case of an initial deployment use parameters) Make your scripts and automation task independed as much as possible (Avoid using extra files and modules) If needed, make sure templates can used in generic (avoid hardcoded customer specific sidesteps, in that case use parameters insertion) VM-templates explained In an Azure Virtual Desktop environment, the session hosts are the most dynamic part. We are building new images, adding/removing hosts all the time, and scale up and down. A good moment to look at AVD sessionhost automation.\nAt this point it becomes interesting. Yes, session hosts are Azure resources and so you can use ARM and Bicep templates. However, there is another way to create a session host. And that‚Äôs the use of VM templates. This VM template information is stored in the host pool information.\nThe good part about this template type is it solves two of my own automation best practices. Use environmental information (2) and be less independent (3).\nWhy is that?\nBelow I posted content inside a VM template. I got this information with the Get-AvdHostPoolInfo PowerShell command. This information is pretty useful.\ndomain : domain.local galleryImageOffer : galleryImagePublisher : galleryImageSKU : imageType : CustomImage imageUri : customImageId : /subscriptions/xxxx/resourceGroups/RG-AVD-SIG-001/providers/Microsoft.Compute /galleries/AVD_ImageGallery/images/Win10-MultiSession namePrefix : cm-avd osDiskType : StandardSSD_LRS useManagedDisks : True vmSize : @{id=Standard_D2s_v3; cores=2; ram=8} galleryItemId : In an AVD sessionhost automation scenario there are some considerations. A session host is pretty customer-specific. Think about the domain name and the session hostname prefix. Both are not recommended to be set in a generic template. As you can see the VM template information stores domain information, the name prefix, and the VM hardware for example. But some values could be used in generic like the imageType and the customImageId (Shared Image Gallery).\nThis information is needed when creating session hosts.\nIn earlier posts, I showed how to create session hosts based on an existing environment. I searched for the latest session host and grabbed all the needed information. However, a new environment has no session hosts. During the time I learned how to enroll session hosts always in the same way. Regardless of a new or existing environment. This with the help of VM templates.\nHow to check VM templates It is good to check if you are using VM templates. However, there is no kind of VM template blade available there is an option to check. As shown above, VM templates store important information. It is good to check if you are using it.\nPortal Some of this information is available in the portal when creating session hosts. To check the available information go to the session host blade. Within that blade, click add. In the Virtual Machines tab, you will notice some information is pre-filled. Check the name prefix and the image part. Also, the correct VM hardware is selected.\nCode When using PowerShell there is a single command to check if there is a template available. I use the Az.Avd PowerShell module.\nGet-AvdHostPoolInfo -HostPoolName avd-hostpool -ResourceGroupName rg-cm-avd-001 | select vmTemplate domain : domain.local galleryImageOffer : galleryImagePublisher : galleryImageSKU : imageType : CustomImage imageUri : customImageId : /subscriptions/xxxx/resourceGroups/RG-AVD-SIG-001/providers/Microsoft.Compute /galleries/AVD_ImageGallery/images/Win10-MultiSession namePrefix : cm-avd osDiskType : StandardSSD_LRS useManagedDisks : True vmSize : @{id=Standard_D2s_v3; cores=2; ram=8} galleryItemId : In case of no template, the portal parts are empty. When l searching with PowerShell, you get a null value returned. vmTemplate ---------- Enable VM templates So, creating session hosts starts with a VM template. This means we have to create a VM template at the initial deployment. Good to know is when deploying a new AVD environment through the portal the VM template is created automatically. However, when creating AVD environments with code the VM template is empty.\nAdd a VM template As mentioned earlier, when creating a session host through the portal a VM template is created automatically. If you use automation make sure you add this as an extra step in your sequence.\nTo help in AVD sessionhost automation I updated the Az.Avd PowerShell module. In the Az.Avd PowerShell module there is a command called Create-AvdVmTemplate. This command asks you for all the needed information to create a VM template and add it to the host pool.\ncreate-AvdVmTemplate -HostpoolName avd-hostpool -ResourceGroupName rg-demo-avd-01 -domain rozemuller.local -namePrefix rz-avd -vmSku \u0026#39;Standard_B2ms\u0026#39; -vmCores 2 -vmRam 8 -osDiskType \u0026#34;Premium_LRS\u0026#34; Where does it help? Now we know what VM templates mean, I can imagine there are questions about the added value. What is the difference between using ARM engine templates (ARM or Bicep) and the VM template?\nIn both cases we need templates but there are pros. The main difference is the fact that ARM engine templates are file-based where VM templates are not.\nBesides the fact that some values are prefilled in the Azure portal under the add session host section, there is more. As described in the VM templates explained paragraph a VM templates hold customer-specific and non-customer-specific values. With that fact, some really cool things can be automated.\nLet me explain based on some real-life scenarios.\nHostpool purpose and VM hardware Imagine you have several host pools with GPU-enabled session hosts. This kind of power host pool needs special hardware for the best performance. During this time more host pools are deployed at several customers. To deploy these session hosts you are used to deploying them with a customer-specific ARM template (with a domain name, prefix, etc).\nNow the time arrives that the VM hardware needs to be updated. And that‚Äôs the point where the use of a VM template could be a lifesaver. Instead of manually changing all those specific ARM templates you can search for those host pools (maybe based on a resource tag (eg. GPU host pool)) and update all those VM templates automatically.\nImage location Another situation is when using your own images, for example from out of a Shared Image Gallery. As you properly know, a Shared Image Gallery provides images across multiple subscriptions. When using Azure Lighthouse you are also able to share them even with other customers.\nIn the end, it means that you have one basic image with rules them all. Imagine you have this custom image location in customer-specific ARM templates and the image location changes (for example, a new image from a new image gallery). With the use of VM templates, you can search the host pools VM templates for the custom image location and changed it automated.\nCustom objects During testing, I noticed you can add custom properties. Since the VM template parameter accepts a string format there are no fixed objects. This means I was able to add the OU-path for example. These values are not present in the Azure Portal but can be used as PowerShell objects. This is extremely powerful.\nTo add these custom values, use the customObject parameter in the create-AvdTemplate command.\n$hostpoolName = \u0026#39;avd-hostpool\u0026#39; $ResourceGroupName = \u0026#39;rg-demo-avd-01\u0026#39; $customObjects = @{ OuPath = \u0026#39;OU=Computers,DC=rozemuller,DC=local\u0026#39; TestNonExisting = \u0026#39;rozemuller.com\u0026#39; } create-AvdVmTemplate -HostpoolName $hostpoolName -ResourceGroupName $ResourceGroupName -domain rozemuller.local -namePrefix rz-avd -vmSku \u0026#39;Standard_B2ms\u0026#39; -vmCores 2 -vmRam 8 -osDiskType \u0026#34;Premium_LRS\u0026#34; -CustomObject $customObjects $vmtemplateInfo = Get-AvdHostPoolInfo -HostPoolName $hostpoolName -ResourceGroupName $ResourceGroupName $vmtemplateInfo ![image-30(image-30.png)\nConsiderations Yes, there are some cons if you are not that familiar with PowerShell. For example, if configured a VM template you are not able to change the VM size within the portal.\nThe virtual machine size is greyed out.\nAnother thing is if you don‚Äôt fill in all the mandatory parameters you will break the template. During tests, I broke the template multiple times because of missing parameters. However, the docs aren‚Äôt telling you some parameters are extremely important.\nFor example, when only providing the VM CPU‚Äôs and memory you will get something like below where the VM type is missing.\nOr even worse, no VM at all.\nLuckily the Create-AvdVmTemplate in the Az.Avd PowerShell module helps you avoid this.\nThank you for reading this blog about using VM templates for AVD sessionhost automation.\nThank you for reading my blog using vm templates for avd sessionhost automation. I hope you got a bit inspired. Enjoy your day and happy automating üëã ","date":"September 21, 2021","image":"http://localhost:1313/using-vm-templates-for-avd-sessionhost-automation/old-hardware.png","permalink":"/using-vm-templates-for-avd-sessionhost-automation/","title":"Using VM templates for AVD sessionhost automation"},{"categories":["Automation","Azure","Azure Virtual Desktop","Image Management","Powershell"],"contents":"A Shared Image Gallery is a great way to distribute images over multiple environments. This great feature is also its disadvantage. When using a Shared Image Gallery in combination with several environments with even more host pools you will lose sight fast. At this moment there is no good overview of which host in which environment is using which version.\nIn this article, I will explain how to deal with AVD session hosts and version management. This will help to keep your environment clean and save costs.\nWe all were in a situation where image versioning wasn‚Äôt that necessary. But at this time we all know better and create versions over and over. And we all agree that versioning reduces downtime and gives us options to revert quickly.\nAlso, Microsoft thought it was a good idea to provide us with an image versioning option. Where Citrix is using Citrix Provisioning Microsoft has the Azure Shared Image gallery.\nI have worked a lot with both and still do (mostly on the Microsoft site). During the years I experienced it could be hard to manage all those environments with all of their versions. It becomes hard when you have multiple tenants with environments. And at last, it becomes almost impossible when you have set some AVD session hosts in drain mode (not accepting new connections) and deployed new session hosts with a newer version in the same host pool. It is just a matter of time becoming a mess.\nIn this article, I will show how to deal with AVD session hosts and version management. I will show you how to get all the session host information with the image versions, how to set drain mode, and how to clean up old session hosts.\nTable Of Contents Getting sight Install module Az.Avd Get AVD image version status Drain mode Clean up old resources Getting sight Let‚Äôs clear the air and make sure we are getting sight of our environment. The main idea is to get all the session host information in combination with the underlying image source. To get sight without writing a lot of code every time I will recommend using the Az.Avd PowerShell module. This module helps you manage AVD environments with PowerShell\nInstall module Az.Avd If you haven‚Äôt already installed the Az.Avd module then is time to do it right now. As I said it really helps you manage your AVD environments.\nBecause the module is available in the official PowerShell gallery it is very easy to install the module by the following command. After installing don‚Äôt forget to import the module as well.\nInstall-Module Az.Avd Import-Module Az.Avd So far so good.\nGet AVD image version status After the module is installed a whole new set of AVD PowerShell command become available. One of them is Get-AvdImageVersionStatus. This command accepts a few parameters and will look into the environment for all the needed information.\nGet-AvdImageVersionStatus -HostpoolName -ResourceGroupName [-NotLatest] Get-AvdImageVersionStatus -HostpoolName \u0026lt;String\u0026gt; -ResourceGroupName \u0026lt;String\u0026gt; [-SessionHostName \u0026lt;String\u0026gt;] [-NotLatest] [\u0026lt;CommonParameters\u0026gt;] For all help, information use the command Get-Help Get-AvdImageVersionStatus -Full\nNow let‚Äôs see the command in action. I‚Äôm searching in an AVD hostpool for all the session hosts with their image versions.\n$hostpoolInfo = @{ HostpoolName = \u0026#34;avd-hostpool\u0026#34; ResourceGroupName = \u0026#34;rg-avd-01\u0026#34; } Get-AvdImageVersionStatus @hostpoolInfo -Verbose The result of the command is shown in the picture below. If there are different versions active in one hostpool, you see differences between the latestVersion object and the currentImageVersion object. The latest version is picked from the Azure Compute Gallery from where the sessionhost got his image. The currentImageVersion is the version that has the sessionhost now.\nWhen using the -Latest flag in the command, the command searches at the isLatestVersion object.\nDrain mode To clean up the mess it is recommended to get the users to correct session hosts. Putting the old session hosts into drain mode is the first step. Drain mode avoids users logging in at one of the old hosts.\nPutting hosts into drain mode is simple with the Az.Avd PowerShell module. In the first step, I‚Äôm only requesting the old session hosts with the -NotLatest switch parameter. The results will be piped to another command Update-AvdSessionhostDrainMode.\n$avdParams = @{ hostpoolName = \u0026#34;AVD-Hostpool\u0026#34; resourceGroupName = \u0026#39;rg-roz-avd-01\u0026#39; } Get-AvdImageVersionStatus @avdParams -NotLatest | foreach { Update-AvdSessionhostDrainMode -InputObject $_ -AllowNewSession $true } Clean up old resources Now the session hosts are in drain mode it is a matter of time till the hosts are empty. At the moment the hosts are empty it is time to clean up the resources. First, we will delete the session host from the host pool. Later we are going to remove the virtual machine, the disk and the network card.\n$avdParams = @{ hostpoolName = \u0026#34;AVD-Hostpool\u0026#34; resourceGroupName = \u0026#39;rg-roz-avd-01\u0026#39; } Get-AvdImageVersionStatus @avdParams -NotLatest | foreach { $sessionHost = Get-AvdSessionHostResources -Id $_.sessionHostId Remove-AzResource -ResourceId $sessionHost.vmResources.Id -Force Remove-AzResource -ResourceId $sessionHost.vmResources.properties.StorageProfile.OsDisk.ManagedDisk.id -Force Remove-AzResource -ResourceId (Get-AvdNetworkInfo @avdParams -SessionHostName $sessionHost.name).NetworkCardInfo.nicId -Force } The next step is removing the session hosts from the AVD host pool.\nWith the command below we are deleting the AVD session host from the host pool. This command is also in the Az.Avd PowerShell module.\n$avdParams = @{ hostpoolName = \u0026#34;AVD-Hostpool\u0026#34; resourceGroupName = \u0026#39;rg-roz-avd-01\u0026#39; } Get-AvdImageVersionStatus @avdParams -NotLatest | foreach{ Remove-AvdSessionhost¬†-HostpoolName $_.Hostpoolname -ResourceGroupName $_.ResourceGroupName -Name $_.tName } Or based on AVD session host ID.\n$avdParams = @{ hostpoolName = \u0026#34;AVD-Hostpool\u0026#34; resourceGroupName = \u0026#39;rg-roz-avd-01\u0026#39; } Get-AvdImageVersionStatus @avdParams -NotLatest | foreach{ Remove-AvdSessionhost¬†-Id $_.id } And now the old session hosts are removed.\nThank you for reading my blog how to deal with avd session hosts version management. I hope you got a bit inspired. Enjoy your day and happy automating üëã ","date":"September 10, 2021","image":"http://localhost:1313/how-to-deal-with-avd-session-hosts-version-management/new-version.png","permalink":"/how-to-deal-with-avd-session-hosts-version-management/","title":"How to deal with AVD session hosts version management"},{"categories":["Azure","Azure Virtual Desktop"],"contents":"Azure Virtual Desktop is growing and becomes available at new (metadata) locations more and more. In this article, I will explain how to move AVD session hosts to a new AVD host pool. metadata location with REST API and PowerShell.\nIntroduction Azure Virtual Desktop becomes a more important Azure service every day and does not need any more introduction I think. AVD is a service that takes care of publishing in a remote working place. The working place with its resources is your own responsibility but Microsoft takes care of the session handling, the login website, and publishing the correct workspaces to end-users.\nThe Microsoft part also has stored his configuration somewhere. This location is called the metadata location. In the beginning, a few locations were available but, the metadata locations are growing. This is good news, but only for new AVD host pools. At this moment there is no option for moving a host pool to a new metadata location. Luckily we are able to create a new host pool and then move existing session hosts to the new host pool.\nIn this article, I will show you how to move the session hosts to a new host pool automated.\nTable Of Contents Introduction AVD metadata locations Move AVD session hosts to another host pool Remove session host from AVD host pool Authenticating Remove session host Generate token Add session host to a new AVD host pool PowerShell Module Az.Avd AVD metadata locations Azure Virtual Desktop consists of several Azure Resources. Think of the session hosts (virtual machines), networking, file servers, or other associated services. Another part of AVD is services resources. Think about the session broker, a gateway, and the web page.\nIn the graph below you see the three big Azure Virtual Desktop pieces. The right resources are the customer‚Äôs responsibility. These resources can be stored at every Azure geographical location. These are the resources that are your own responsibility.\nThe middle part is the AVD services resources. These resources are managed by Microsoft and also stored somewhere. That location is called the metadata location. At this location, Microsoft stores traffic patterns, health checks, usage log information, and more. This information is needed to control the infrastructure and can scale capacity if needed. Customer data is not stored at this location.\nCurrently, Microsoft has the following locations available.\nUnited States (US) Europe (EU) United Kingdom (UK) Canada (CA) Japan (JP) *in Public Preview Australia (AU) *in Public Preview For more information about data location please check the AVD data location documentation.\nMove AVD session hosts to another host pool Microsoft is planning more locations in the future. That will make it possible to get the metadata location at a place you like. However, at this moment it is not possible to move a host pool to a new location. Even when you move the host pool to a new resource group, with another location, the host pool location will not be changed.\nBut there are options to change the metadata location if you like. There is an option to move session hosts to a new host pool. This can be done by creating a new host pool at the correct location and moving the session hosts to the new host pool.\nIn the following chapters, I will explain the process how to move an AVD session host to a new host pool with REST API.\nIn my situation I have two AVD host pools:\nRozemuller-Hostpool is located in West-Europe US-Hostpool is located in East-US. The Rozemuller-Hostpool has one session host wvd-0.rozemuller.local. The US-Hostpool is empty.\nRemove session host from AVD host pool The first step in the process is removing the session host from its current host pool. Make sure there are no sessions associated with the host. Otherwise, you will get an error message like the one below.\nOnce the session host is empty you are able to remove it from the host pool. We are using the REST API for removing the session host.\nAuthenticating Before you are able to use the REST API you will need to authenticate to the API.\nBecause we are using PowerShell for executing the REST API calls we only need to authenticate in PowerShell. From that context, we are gathering a header token for the REST API.\nfunction¬†GetAuthToken($resource)¬†{ $context¬†=¬†[Microsoft.Azure.Commands.Common.Authentication.Abstractions.AzureRmProfileProvider]::Instance.Profile.DefaultContext $Token¬†=¬†[Microsoft.Azure.Commands.Common.Authentication.AzureSession]::Instance.AuthenticationFactory.Authenticate($context.Account,¬†$context.Environment,¬†$context.Tenant.Id.ToString(),¬†$null,¬†[Microsoft.Azure.Commands.Common.Authentication.ShowDialog]::Never,¬†$null,¬†$resource).AccessToken $authHeader¬†=¬†@{ \u0026#39;Content-Type\u0026#39;¬†=¬†\u0026#39;application/json\u0026#39; Authorization¬†=¬†\u0026#39;Bearer¬†\u0026#39;¬†+¬†$Token } return¬†$authHeader } $token¬†=¬†GetAuthToken¬†-resource¬†\u0026#34;https://management.azure.com\u0026#34; The authentication token will be returned to the $token variable. This is the token we need for requesting API calls to Azure.\nRemove session host If we have received a token we are able to call the REST API for removing the session host. I will save the outcome into the $DeleteSessionHost variable. The command returns the session host object. The object contains the virtual machine resourceId which is needed in the next step.\n$subscriptionId = \u0026#34;xxx\u0026#34; $ResourceGroupName¬†=¬†\u0026#34;rg-wvd-001\u0026#34; $hostpoolname¬†=¬†\u0026#34;Rozemuller-hostpool\u0026#34; $SessionHostName¬†=¬†\u0026#39;wvd-0.rozemuller.local\u0026#39; $SessionHostUrl¬†=¬†\u0026#34;https://management.azure.com/subscriptions/{0}/resourceGroups/{1}/providers/Microsoft.DesktopVirtualization/hostpools/{2}/sessionHosts/{3}?api-version=2021-03-09-preview\u0026#34; -f $subscriptionId, $ResourceGroupName, $HostpoolName, $SessionHostName $parameters¬†=¬†@{ uri¬†=¬†$SessionHostUrl Method¬†=¬†\u0026#34;DELETE\u0026#34; Headers¬†=¬†$token } $sessionHost¬†=¬†Invoke-RestMethod¬†@parameters Or, use the Az.Avd module (after login with Connect-AzAccount) and run the command below.\n$ResourceGroupName¬†=¬†\u0026#34;rg-wvd-001\u0026#34; $hostpoolname¬†=¬†\u0026#34;Rozemuller-hostpool\u0026#34; $SessionHostName¬†=¬†\u0026#39;wvd-0.rozemuller.local\u0026#39; Remove-AvdSessionHost -HostpoolName $hostpoolname -ResourceGroupName $ResourceGroupName -Name $SessionHostName The Rozemuller-Hostpool is now empty.\nGenerate token A session host will register itself to a host pool with a globally unique token. This very long string in combination with the host pool name will make it possible to join a host pool. The AVD RD Infra Agent will search in the Azure cloud for a host pool name with this token and register itself into that host pool.\nA token must be available between an hour and a maximum of 27 days.\nIn my code, I will generate a new token for 1 hour in the new host pool. This for the simple reason I don‚Äôt need more time to register the session host. Make a notice about the $hostpoolToken variable. I will store the output into this variable. I need the token which will be returned.\n$subscriptionId = \u0026#34;xxx\u0026#34; $NewResourceGroupName¬†=¬†\u0026#34;rg-wvd-001\u0026#34; $Newhostpoolname¬†=¬†\u0026#34;US-Hostpool\u0026#34; $baseUrl¬†=¬†\u0026#34;https://management.azure.com/subscriptions/{0}/resourceGroups/{1}/providers/Microsoft.DesktopVirtualization/hostpools/{2}?api-version=2021-03-09-preview\u0026#34; -f $subscriptionId, $NewResourceGroupName, $Newhostpoolname $Body¬†=¬†@{ properties¬†=¬†@{ registrationInfo¬†=¬†@{ expirationTime¬†=¬†\u0026#34;$($(Get-Date).AddHours(1))\u0026#34; registrationTokenOperation¬†=¬†\u0026#34;Update\u0026#34; } } } $parameters¬†=¬†@{ URI¬†=¬†$baseUrl¬†Method¬†=¬†\u0026#34;PATCH\u0026#34; Headers¬†=¬†$token Body¬†=¬†$Body¬†|¬†ConvertTo-Json } $hostpoolToken¬†=¬†Invoke-RestMethod¬†@parameters Use the code below in the next request body.\n$hostpoolToken.properties.registrationInfo.token Generating a host pool token is also possible with the Az.Avd PowerShell module. Use the command below.\nUpdate-AvdRegistrationToken -HostpoolName $Newhostpoolname -ResourceGroupName $NewResourceGroupName -HoursActive 4 Add session host to a new AVD host pool The last step generating a script that will be executed on the session host. We have stored the session host Id and the token in variables and are using these in the body.\nWe are also going to send an invoke-azruncommand via the REST API. To send a script via the REST API we need to generate an ArrayList with PowerShell. In the first part of the commands below, we are creating an ArrayList and adding commands with the.Add () function.\nIn the first command, I use the $hostpool token variable which is the registration token value in the registry.\nThe $sessionHost resource Id is used in the REST API URL.\n$script¬†=¬†[System.Collections.ArrayList]@() $script.Add(\u0026#39;Set-ItemProperty¬†-Path¬†Registry::HKEY_LOCAL_MACHINE\\SOFTWARE\\Microsoft\\RDInfraAgent¬†-Name¬†RegistrationToken¬†-Value¬†\u0026#39;+$($hostpoolToken.properties.registrationInfo.token)+\u0026#39;\u0026#39;) $script.Add(\u0026#39;Set-ItemProperty¬†-Path¬†Registry::HKEY_LOCAL_MACHINE\\SOFTWARE\\Microsoft\\RDInfraAgent¬†-Name¬†IsRegistered¬†-Value¬†0\u0026#39;) $script.Add(\u0026#39;Restart-Service¬†-Name¬†RDAgentBootLoader\u0026#39;) $MoveBody¬†=¬†@{ commandId¬†=¬†\u0026#34;RunPowerShellScript\u0026#34; script¬†=¬†$script } $url¬†=¬†\u0026#34;https://management.azure.com{0}runCommand?api-version=2021-03-01\u0026#34; -f $sessionHost.properties.resourceId $parameters¬†=¬†@{ URI¬†=¬†$url¬†Method¬†=¬†\u0026#34;POST\u0026#34; Body¬†=¬†$MoveBody¬†|¬†ConvertTo-Json Headers¬†=¬†$token } Invoke-RestMethod¬†@parameters Wait for a few minutes after running the command. The session host is moved to a new host pool.\nPowerShell Module Az.Avd If you are not that familiar with Rest API, no worries. I also updated the Az.Avd PowerShell module with a new command called Move-AvdSessionHost. This command helps you move session hosts to another host pool.\nUse the following syntax:\nMove-AvdSessionHost -FromHostpoolName \u0026lt;String\u0026gt; -FromResourceGroupName \u0026lt;String\u0026gt; -ToHostpoolName \u0026lt;String\u0026gt; -ToResourceGroupName \u0026lt;String\u0026gt; [-SessionHostName \u0026lt;String\u0026gt;] [\u0026lt;CommonParameters\u0026gt;] Thank you for reading about moving AVD session hosts to a new host pool via REST API.\nThank you for reading my blog move avd session hosts to a new host pool with rest api. I hope you got a bit inspired. Enjoy your day and happy automating üëã ","date":"August 30, 2021","image":"http://localhost:1313/move-avd-session-hosts-to-a-new-host-pool-with-rest-api/we-have-moved.png","permalink":"/move-avd-session-hosts-to-a-new-host-pool-with-rest-api/","title":"Move AVD session hosts to a new host pool with REST API"},{"categories":["Automation","Azure","Azure Virtual Desktop","Powershell"],"contents":"After the Windows 11 launch everyone was waiting till Windows 11 came available in Azure. Since the 19th of August Windows 11 became in preview. I decided to test it in a AVD environment. Because it is not a next-next-finish installation I will explain how to deploy Windows 11 in an AVD environment.\nAfter the release of Windows 11 Microsoft announced the preview status of Windows 11 for Azure Virtual Desktop. This means we are able to try Windows 11 in our AVD environment. After this announcement a lot of people tried to get it work. During my test I mentioned it wasn‚Äôt possible to create an AVD session host with Windows 11 directly. So I decided to write a small post about how to get Windows 11 work in an AVD environment.\nLet‚Äôs start.\nTable Of Contents The issue Create Windows 11 VM Install AVD Extensions Preview symptoms Get Windows 11 Image SKU with PowerShell Windows 10 display names Old AVD DSC extension not working The issue After the moment Windows 11 came available for AVD I started to test it out. I started in the portal at the Azure Virtual Desktop. Created a new registration token followed by adding a session host. That‚Äôs the point where the issue started.\nWhen creating a new session host you have to select an image from where to create a session host. This can be an image from the Azure marketplace or your own. In this case, I choose the marketplace. After searching for Windows 11 there are several images available.\nFor AVD I like to test the Windows 11 Enterprise multi-session (Preview) ‚Äì Gen2. After selecting that image I got a message from Microsoft.\nThe selected image is not available on your location.\nThe first thing I did was changing the virtual machines location but didn‚Äôt help. Also I changed the resource group with an other location. After some community contact we noticed we need to create a VM first and install the AVD extensions later.\nCreate Windows 11 VM The first step to get Windows 11 running in AVD is creating the virtual machine. This time we PowerShell to create the virtual machine.\n$VMLocalAdminUser = \u0026#34;LocalAdminUser\u0026#34; $VMLocalPassword = \u0026#34;V3rySecretP@ssw0rd\u0026#34; $VMLocalAdminSecurePassword = ConvertTo-SecureString $VMLocalPassword -AsPlainText -Force $location = \u0026#39;westeurope\u0026#39; $VMName = \u0026#34;avd-win11-0\u0026#34; $VMSize = \u0026#34;Standard_D2s_v3\u0026#34; $ImageSku = \u0026#34;win11-21h2-avd\u0026#34; $ImageOffer = \u0026#34;windows-11-preview\u0026#34; $ImagePublisher = \u0026#34;MicrosoftWindowsDesktop\u0026#34; $ComputerName = $VMName $DiskSizeGB = 128 $nicName = \u0026#34;nic-$vmName\u0026#34; $vnetResourceGroup = \u0026#39;rg-cm-vnet-01\u0026#39; $vnet = Get-AzVirtualNetwork -Name \u0026#39;cm-vnet-01\u0026#39; -ResourceGroupName $vnetResourceGroup $subnet = Get-AzVirtualNetworkSubnetConfig -Name \u0026#39;wvdSubnet\u0026#39; -VirtualNetwork $vnet $NIC = New-AzNetworkInterface -Name $nicName -ResourceGroupName $vnetResourceGroup -Location $location -Subnet $subnet $Credential = New-Object System.Management.Automation.PSCredential ($VMLocalAdminUser, $VMLocalAdminSecurePassword); $VirtualMachine = New-AzVMConfig -VMName $VMName -VMSize $VMSize $VirtualMachine = Set-AzVMOperatingSystem -VM $VirtualMachine -Windows -ComputerName $ComputerName -Credential $Credential -ProvisionVMAgent -EnableAutoUpdate $VirtualMachine = Add-AzVMNetworkInterface -VM $VirtualMachine -Id $NIC.Id $VirtualMachine = Set-AzVMOSDisk -Windows -VM $VirtualMachine -CreateOption FromImage -DiskSizeInGB $DiskSizeGB $VirtualMachine = Set-AzVMSourceImage -VM $VirtualMachine -PublisherName $ImagePublisher -Offer $ImageOffer -Skus $ImageSku -Version latest $newVm = New-AzVM -ResourceGroupName \u0026#39;rg-cm-wvd-cus\u0026#39; -Location $Location -VM $VirtualMachine The PowerShell commands above creating a VM profile, a network card and disk. At the end it puts all the pieces together into a new vm deployment. Make a note about the image information. How I got these information is discussed later.\nInstall AVD Extensions After the VM is created it is time to install the extensions. In this example, I have chosen to install the native AD join extension.\n$domainUser = \u0026#34;vmjoiner@domain.local\u0026#34; $domainPassword = \u0026#34;veryS3cret\u0026#34; $domain = $domainUser.Split(\u0026#34;@\u0026#34;)[-1] $ouPath = \u0026#34;OU=Computers,OU=AVD,DC=domain,DC=local\u0026#34; \u0026lt;meta charset=\u0026#34;utf-8\u0026#34;\u0026gt;\u0026lt;/meta\u0026gt;$domainPassword = \u0026#34;verySecretPass\u0026#34; $domainJoinSettings = @{ Name = \u0026#34;joindomain\u0026#34; Type = \u0026#34;JsonADDomainExtension\u0026#34; Publisher = \u0026#34;Microsoft.Compute\u0026#34; typeHandlerVersion = \u0026#34;1.3\u0026#34; SettingString = \u0026#39;{ \u0026#34;name\u0026#34;: \u0026#34;\u0026#39;+ $($domain) + \u0026#39;\u0026#34;, \u0026#34;ouPath\u0026#34;: \u0026#34;\u0026#39;+ $($ouPath) + \u0026#39;\u0026#34;, \u0026#34;user\u0026#34;: \u0026#34;\u0026#39;+ $($domainUser) + \u0026#39;\u0026#34;, \u0026#34;restart\u0026#34;: \u0026#34;\u0026#39;+ $true + \u0026#39;\u0026#34;, \u0026#34;options\u0026#34;: 3 }\u0026#39; ProtectedSettingString = \u0026#39;{ \u0026#34;password\u0026#34;:\u0026#34;\u0026#39; +$($domainPassword)+ \u0026#39;\u0026#34;}\u0026#39; VMName = $VMName ResourceGroupName = $resourceGroupName location = $Location } Set-AzVMExtension @domainJoinSettings After the domain join is finished make sure your VM is available under Devices in the Azure AD blade marked as Hybrid AD Joined.\nNext is installing the AVD extension. I tested a lot with the ‚Äúold‚Äù not AzureAD join DSC configuration. After all those tests I didn‚Äôt get it working fine. So I choose the new Azure AD join DCS configuration with no AAD join.\nTo get the AzureAD join configuration use the artifact URL below. https://wvdportalstorageblob.blob.core.windows.net/galleryartifacts/Configuration_01-19-2023.zip\nFor updating the AVD registration token, with Update-AvdRegistrationToken , I used the Az.Avd PowerShell module.\n$avdHostpool = \u0026#39;AVD-Hostpool-US\u0026#39; $avdResourceGroupName = \u0026#39;rg-demo-avd-01\u0026#39; $registrationToken = (Update-AvdRegistrationToken -HostpoolName $avdHostpool $avdResourceGroupName).Properties.registrationInfo.token $avdModuleLocation = \u0026#34;https://wvdportalstorageblob.blob.core.windows.net/galleryartifacts/Configuration_01-19-2023.zip\u0026#34; $avdDscSettings = @{ Name = \u0026#34;Microsoft.PowerShell.DSC\u0026#34; Type = \u0026#34;DSC\u0026#34; Publisher = \u0026#34;Microsoft.Powershell\u0026#34; typeHandlerVersion = \u0026#34;2.73\u0026#34; SettingString = \u0026#34;{ \u0026#34;\u0026#34;modulesUrl\u0026#34;\u0026#34;:\u0026#39;$avdModuleLocation\u0026#39;, \u0026#34;\u0026#34;ConfigurationFunction\u0026#34;\u0026#34;:\u0026#34;\u0026#34;Configuration.ps1\\\\AddSessionHost\u0026#34;\u0026#34;, \u0026#34;\u0026#34;Properties\u0026#34;\u0026#34;: { \u0026#34;\u0026#34;hostPoolName\u0026#34;\u0026#34;: \u0026#34;\u0026#34;AVD-Hostpool-US\u0026#34;\u0026#34;, \u0026#34;\u0026#34;registrationInfoToken\u0026#34;\u0026#34;: \u0026#34;\u0026#34;$registrationToken\u0026#34;\u0026#34;, \u0026#34;\u0026#34;aadJoin\u0026#34;\u0026#34;: false } }\u0026#34; VMName = $VMName ResourceGroupName = $resourceGroupName location = $Location } Set-AzVMExtension @avdDscSettings Preview symptoms During my test, I noticed some preview symptoms which deserve some attention.\nGet Windows 11 Image SKU with PowerShell Before creating a virtual machine from an image we need the image details. These details are the SKU, offer and publisher. When searching for the Windows 11 SKU‚Äôs with PowerShell you will notice no Windows 11 SKU is available. In the command below I‚Äôm searching for all MicrosoftWindowsDesktop images. As you can see there is no Windows 11 available.\nGet-AzVMImageOffer -Location \u0026#39;westeurope\u0026#39; -PublisherName \u0026#39;MicrosoftWindowsDesktop\u0026#39; Because I‚Äôm not satisfied with this I also tried to find the windows-11-preview SKU itself. Also there I got an error.\nGet-AzVMImageSku -Location \u0026#39;westeurope\u0026#39; -PublisherName \u0026#39;MicrosoftWindowsDesktop\u0026#39; -Offer \u0026#39;windows11preview\u0026#39; Get-AzVMImageSku: Artifact: VMImage was not found.\nErrorCode: NotFound\nErrorMessage: Artifact: VMImage was not found.\nErrorTarget:\nStatusCode: 404\nReasonPhrase: Not Found\nOperationID : 429a474d-c4d4-4791-98f2-f5abb42943a9\nSo how did I find the correct image information?\nTo get that answer I moved to the Azure portal. I simply created a new VM with an Windows 11 image from the marketplace. At the end of the wizard I clicked the download a template for automation link. In the ARM templates I found the correct Windows 11 image information.\nWindows 10 display names Other thing I noticed is the operating system display name. In some cases it is still Windows 10.\nOld AVD DSC extension not working I noticed after testing a lot the old AVD DSC virtual machine extension is not working. At least I didn‚Äôt get it stable. What happpens is that the extension will update to the latest version. After the update the I got errors the SXS Stack listener is not working.\nI also tried to disable auto update but for some reason it is still updating or gets stuck in the update.\nTo get the working DSC configuration package use the URL below.\nhttps://wvdportalstorageblob.blob.core.windows.net/galleryartifacts/Configuration_01-19-2023.zip Make sure in the PowerShell you also provide the AADJoin parameter. Because of a hybrid join, I set the parameter to false.\nThank you for reading my post about how to deploy Windows 11 in an AVD environment.\nThank you for reading my blog how to deploy windows 11 in an avd environment. I hope you got a bit inspired. Enjoy your day and happy automating üëã ","date":"August 23, 2021","image":"http://localhost:1313/how-to-deploy-windows-11-in-an-avd-environment/win-11.png","permalink":"/how-to-deploy-windows-11-in-an-avd-environment/","title":"How to deploy Windows 11 in an AVD environment"},{"categories":["Automation","Azure AD","Powershell"],"contents":"In this article, I show how to deploy and join an Azure VM to Azure AD join automated. In the deployment, I use two different ways of deployment. I use PowerShell and Azure CLI. Additionally, I show how to install the AVD extension with the Azure AD join.\nTable Of Contents Azure AD join Deploy an Azure AD joined VM System identity Deploy AAD VM-extension AAD join in Azure Virtual Desktop Prepare AVD host pool AVD RDP property Assign roles AVD AAD join settings AVD Extension Azure AD join We all heard the news that Azure AD join is now in public preview. Because of that, I decided to try that new feature in an Azure Virtual Desktop environment. While testing the feature I also want to do azure ad join automated. To make this feature work you‚Äôll need to walk through some steps. The first thing to remember is there are also some limitations and it is still in public preview.\nIn addition to the short introduction more info about Azure AD join: https://docs.microsoft.com/en-us/azure/virtual-desktop/deploy-azure-ad-joined-vm\nDeploy an Azure AD joined VM During joining a virtual machine several steps are executed in the background. The first is creating a system identity based on the virtual machine. The second is deploying the correct AAD VM extension. These two steps are explained below.\nSystem identity A difference between a native domain joined VM is the resource identity.\nFrom Microsoft: System-assigned Some Azure services allow you to enable a managed identity directly on a service instance. When you enable a system-assigned managed identity identity is created in Azure AD that is tied to the lifecycle of that service instance. So when the resource is deleted, Azure automatically deletes the identity for you. By design, only that Azure resource can use this identity to request tokens from Azure AD.\nThe PowerShell command below is creating a system identity for the VM.\n# VM Properties $vmName = \u0026#34;avd-1\u0026#34; $resourceGroupName = \u0026#34;rg-avd-demo\u0026#34; $location = \u0026#39;westeurope\u0026#39; $vm = Get-AzVM -Name $vmName -ResourceGroupName $resourceGroupName Update-AzVM -ResourceGroupName $resourceGroupName -VM $vm -IdentityType SystemAssigned Use the command below for assigning the system identity with Azure CLI.\naz vm identity assign --name $vmName --resource-group $resourceGroupName --identities System Deploy AAD VM-extension Joining a virtual machine to a domain is handled by an Azure VM-Extension. Since AAD join is possible two new extensions are available.\nIn addition to this context please check the following URL for more information: https://docs.microsoft.com/en-us/azure/active-directory/managed-identities-azure-resources/overview\nAADLoginForWindows AADLoginForWindowsWithIntune There are several options for installing the AAD extension to an Azure virtual machine. In the commands below I show how to deploy these extensions. To demonstrate several options I show a deployment with Azure CLI and PowerShell.\nGood to know is that only one extension can be deployed to a virtual machine.\nBefore adding an Azure VM extension with PowerShell make sure you imported the Az.Compute module. Next, configure the parameters.\n# VM Properties $vmName = \u0026#34;avd-1\u0026#34; $resourceGroupName = \u0026#34;rg-avd-demo\u0026#34; $location = \u0026#39;westeurope\u0026#39; # Azure AD Join domain extension $domainJoinName = \u0026#34;AADLoginForWindows\u0026#34; $domainJoinType = \u0026#34;AADLoginForWindows\u0026#34; $domainJoinPublisher = \u0026#34;Microsoft.Azure.ActiveDirectory\u0026#34; $domainJoinVersion = \u0026#34;1.0\u0026#34; Set-AzVMExtension -VMName $vmName -ResourceGroupName $resourceGroupName -Location $location -TypeHandlerVersion $domainJoinVersion -Publisher $domainJoinPublisher -ExtensionType $domainJoinType -Name $domainJoinName If you like to join the machine also in your Microsoft Intune (MEM) environment you need to use the AADLoginForWindowWithIntune extension. In that case, you need to provide the mdmId in the extension settings.\n# VM Properties $vmName = \u0026#34;avd-1\u0026#34; $resourceGroupName = \u0026#34;rg-avd-demo\u0026#34; $location = \u0026#39;westeurope\u0026#39; $domainJoinName = \u0026#34;AADLoginForWindows\u0026#34; $domainJoinSettings = @{ mdmId = \u0026#34;0000000a-0000-0000-c000-000000000000\u0026#34; } $domainJoinType = \u0026#34;AADLoginForWindows\u0026#34; $domainJoinPublisher = \u0026#34;Microsoft.Azure.ActiveDirectory\u0026#34; $domainJoinVersion = \u0026#34;1.0\u0026#34; Set-AzVMExtension -VMName $vmName -ResourceGroupName $resourceGroupName -Location $location -TypeHandlerVersion $domainJoinVersion -Publisher $domainJoinPublisher -ExtensionType $domainJoinType -Name $domainJoinName -Settings $domainJoinSettings For deploying extensions with Azure CLI use the command below. Use the parameters above.\n# Azure AD Join domain extension $domainJoinName = \u0026#34;AADLoginForWindows\u0026#34; $domainJoinPublisher = \u0026#34;Microsoft.Azure.ActiveDirectory\u0026#34; $domainJoinVersion = \u0026#34;1.0\u0026#34; $domainJoinSettings = \u0026#39;{\u0026#34;\u0026#34;mdmId\u0026#34;\u0026#34;: \u0026#34;\u0026#34;0000000a-0000-0000-c000-000000000000\u0026#34;\u0026#34;}\u0026#39; az vm identity assign --name $vmName --resource-group $resourceGroupName az vm extension set --vm-name $vmName --resource-group $resourceGroupName --name $domainJoinName --publisher $domainJoinPublisher --version $domainJoinVersion --settings $domainJoinSettings AAD join in Azure Virtual Desktop After creating the virtual machines (or session hosts) it is time to prepare the AVD environment. This is the host pool and installing the AVD extension. Also for AVD is a new VM extension available.\nPrepare AVD host pool To connect to an Azure AD joined virtual machine the client PC must meet one of the following conditions.\nAzure AD-joined to the same Azure AD tenant as the session host\nHybrid Azure AD-joined to the same Azure AD tenant as the session host\nRunning Windows 10, version 2004 and later and Azure AD registered to the same Azure AD tenant as the session host\n(Source: Microsoft)\nHowever, in an AVD situation, not every client PC has one of the conditions above. In fact, AVD is also available from the web (via https://rdweb.wvd.microsoft.com/arm/webclient/index.html). Till to the present day, a browser has no domain join :). Luckily there is a solution for accessing AVD with non-domain joined clients.\nAVD RDP property First is adding an extra RDP property to the host pool, targetisaadjoined:i:1. The second step is configuring the host pool as a validation environment set.\n# AVD Properties $resourceGroupName = \u0026#34;rg-avd-demo\u0026#34; $location = \u0026#39;westeurope\u0026#39; $HostPoolName = \u0026#39;avd-hostpool\u0026#39; $rdpProperties = \u0026#34;targetisaadjoined:i:1\u0026#34; New-AzWvdHostPool -ResourceGroupName -Name $HostPoolName -Location $location -HostPoolType \u0026#39;Personal\u0026#39; -LoadBalancerType \u0026#39;Persistent\u0026#39; -PreferredAppGroupType \u0026#34;Desktop\u0026#34; -MaxSessionLimit 5 -CustomRdpProperty $rdpProperties -ValidationEnvironment:$true Assign roles Next is assigning a Virtual Machine User Login or Virtual Machine Administrator Login role. This role must be assigned at VM, resource group, or at the subscription level. I choose the resource group scope. For example, I used the All Users AD group.\nIn the first place, we need to get the All Users objectId.\nNext is finding the correct role. I use the Virtual Machine User Login role. This role is a BuilldIn role.\nIn PowerShell use the following code. (PowerShell module: Az.Resources). I use the resource group scope.\nNow we have the correct Id‚Äôs it is time to assign the role to the resource group for All Users.\n$objectId = Get-AzADGroup -DisplayName \u0026#34;All Users\u0026#34; | Select Id $role = Get-AzRoleDefinition -Name \u0026#34;Virtual Machine User Login\u0026#34; New-AzRoleAssignment -ObjectId $objectId -RoleDefinitionName $role.name -resourceGroupName rg-avd-001 For Azure CLI use the code below.\n$objectId = az ad group show --group \u0026#34;AVD Users\u0026#34; --query \u0026#34;objectId\u0026#34; $role = az role definition list --name \u0026#34;Virtual Machine User Login\u0026#34; --query \u0026#34;name\u0026#34; $roleName = $role | Convertfrom-json az role assignment create --assignee $objectId --role $roleName.name --resource-group $resourceGroupName Role output from Azure CLI 71 At last, we need to install the correct VM extension. I will discuss that point later in the session host deployment.\nAVD AAD join settings To join an AVD session host to Azure AD you need different settings relating to native AD. The first thing to remember is the Desired State Config extension. The second different thing is the VM identity. A not AAD joined VM has no system-assigned identity in the Azure AD.\nAVD Extension The AVD module is a Desired State Config (DSC) extension. DSC is a management platform in PowerShell that enables you to ‚Äòpush‚Äô configuration settings as code to a resource. In the case of AVD, means that the extension installs the AVD software with specific AVD environment settings. For example the registration token and the host pool name. Now AADJoin is part of the family a new module came up. This new module accepts the aadJoin parameter, which the native module does not.\nWhile installing the native module with the AADJoin parameter you will get a message like below.\n(ArtifactNotFound) The VM extension with publisher ‚ÄòMicrosoft.Azure.ActiveDirectory‚Äô and type ‚ÄòActiveDirectory‚Äô could not be found.\n(VMExtensionProvisioningError) VM has reported a failure when processing extension ‚ÄòDSC‚Äô. Error message: ‚ÄúThe DSC Extension received an incorrect input: A parameter cannot be found that matches parameter name ‚ÄòaadJoin‚Äô.\nAfter digging into the deployment I found the correct AVD desired state config artifact URL.\nhttps://wvdportalstorageblob.blob.core.windows.net/galleryartifacts/Configuration_01-19-2023.zip When using PowerShell use the commands below. I used the Az.Avd PowerShell module and the Az.Compute.\n$hostpoolName = \u0026#34;avd-hostpool\u0026#34; $resourceGroupName = \u0026#34;rg-avd-demo\u0026#34; $token = Update-AvdRegistrationToken -HostpoolName $hostpoolName -ResourceGroupName $resourceGroupName -HoursActive 4 # AVD Azure AD Join domain extension $moduleLocation = \u0026#34;https://wvdportalstorageblob.blob.core.windows.net/galleryartifacts/Configuration_01-19-2023.zip\u0026#34; $avdExtensionName = \u0026#34;DSC\u0026#34; $avdExtensionPublisher = \u0026#34;Microsoft.Powershell\u0026#34; $avdExtensionVersion = \u0026#34;2.73\u0026#34; $avdExtensionSetting = @{ modulesUrl = $moduleLocation ConfigurationFunction = \u0026#34;Configuration.ps1\\\\AddSessionHost\u0026#34; Properties = @{ hostPoolName = $hostpoolName registrationInfoToken = $($token.token) aadJoin = $true } } Set-AzVMExtension -VMName $vmName -ResourceGroupName $resourceGroupName -Location $location -TypeHandlerVersion $avdExtensionVersion -Publisher $avdExtensionPublisher -ExtensionType $avdExtensionName -Name $avdExtensionName -Settings $avdExtensionSetting Use the code below to deploy the AVD DSC extension with Azure CLI. Make a notice about the aadJoin parameter in the settings. Important to realize when using Azure CLI, is the JSON input format for the extension settings.\n$token = az desktopvirtualization hostpool update --name $hostpoolName --resource-group $resourceGroupName --registration-info expiration-time=$date registration-token-operation=\u0026#34;Update\u0026#34; $registrationToken = $token | ConvertFrom-Json $hostpoolName = \u0026#34;avd-hostpool\u0026#34; $resourceGroupName = \u0026#34;rg-avd-demo\u0026#34; $date = (Get-Date).AddHours(4).ToString(\u0026#34;yyyy\u0026#39;-\u0026#39;MM\u0026#39;-\u0026#39;dd\u0026#39;T\u0026#39;HH\u0026#39;:\u0026#39;mm\u0026#39;:\u0026#39;ss.fffffffK\u0026#34;) $token = az desktopvirtualization hostpool update --name $hostpoolName --resourcegroup $resourceGroupName --registration-token-operation=\u0026#34;Update\u0026#34; --registration-info expiration-time=$date $moduleLocation = \u0026#34;https://wvdportalstorageblob.blob.core.windows.net/galleryartifacts/Configuration_01-19-2023.zip\u0026#34; $avdExtensionName = \u0026#34;DSC\u0026#34; $avdExtensionPublisher = \u0026#34;Microsoft.Powershell\u0026#34; $avdExtensionVersion = \u0026#34;2.73\u0026#34; $avdExtensionSetting = \u0026#39;{\u0026#34;\u0026#34;modulesUrl\u0026#34;\u0026#34;: \u0026#34;\u0026#34;\u0026#39;+$moduleLocation+\u0026#39;\u0026#34;\u0026#34;,\u0026#34;\u0026#34;ConfigurationFunction\u0026#34;\u0026#34;:\u0026#34;\u0026#34;Configuration.ps1\\\\AddSessionHost\u0026#34;\u0026#34;,\u0026#34;\u0026#34;Properties\u0026#34;\u0026#34;: {\u0026#34;\u0026#34;hostPoolName\u0026#34;\u0026#34;: \u0026#34;\u0026#34;\u0026#39;+ $($hostpoolName) + \u0026#39;\u0026#34;\u0026#34;,\u0026#34;\u0026#34;registrationInfoToken\u0026#34;\u0026#34;: \u0026#34;\u0026#34;\u0026#39;+ $($registrationToken .registrationInfo.token) + \u0026#39;\u0026#34;\u0026#34;, \u0026#34;\u0026#34;aadJoin\u0026#34;\u0026#34;: \u0026#34;\u0026#39;+ $true + \u0026#39;\u0026#34;}}\u0026#39; az vm extension set --vm-name $vmName --resource-group $resourceGroupName --name $avdExtensionName --publisher $avdExtensionPublisher --version $avdExtensionVersion --settings $avdExtensionSetting Thank you for reading this article I show how to deploy and an Azure VM to Azure AD join automated.\nThank you for reading my blog how to join azure ad automated. I hope you got a bit inspired. Enjoy your day and happy automating üëã ","date":"July 26, 2021","image":"http://localhost:1313/how-to-join-azure-ad-automated/cloud-identity.png","permalink":"/how-to-join-azure-ad-automated/","title":"How to join Azure AD  automated"},{"categories":["AVD Cocktail","Azure","Azure CLI","Azure Virtual Desktop"],"contents":"Welcome to the AVD Automation Cocktail. In this cocktail series I will show different AVD deployment strategies and languages. In this cocktail, the Sweet Orange Sunset, I will show you how to deploy an AVD environment automated with Azure CLI. Great new feature in this cocktail is the Azure AAD Join feature.\nTable Of Contents Recipe Before to drink List Azure CLI of ingredients Aftertaste AVD automated with Azure CLI Resource Group Networking Shared Image Gallery Initial Image Version Create a virtual machine SysPrep Generalize VM Create image version Azure Virtual Desktop AVD Azure AD join Automated Hostpool Application group Workspace Monitoring AVD Session hosts Azure Key vault Azure AAD Join Settings AVD Extension System identity Create session host Public IP and NSG Conclusion Thank you! Recipe In this ‚ÄúOrangeSunset ‚Äú-deployment recipe I will deploy an AVD environment automated with Azure CLI only. This cocktail looks a bit like the Fresh Minty Breeze. However this cocktail has no templates and using the specific deployment commands. This deployment also using the input parameters. This because to show all the needed Azure CLI commands and parameters. A great extra is the Azure AAD Join deployment.\nBefore to drink To start enrolling AVD automated with Azure CLI you need installing the software first. Download the Azure CLI software by clicking this URL: https://docs.microsoft.com/en-us/cli/azure/install-azure-cli\nMore information about Azure CLI please check the following URLs:\nhttps://docs.microsoft.com/en-us/cli/azure/ List Azure CLI of ingredients https://docs.microsoft.com/nl-nl/cli/azure/network/nsg?view=azure-cli-latest (NSG) https://docs.microsoft.com/nl-nl/cli/azure/network?view=azure-cli-latest (Network) https://docs.microsoft.com/en-us/cli/azure/sig?view=azure-cli-latest (Shared Image Gallery) https://docs.microsoft.com/en-us/cli/azure/vm/run-command?view=azure-cli-latest#az_vm_run_command_invoke (VM Run command) https://docs.microsoft.com/en-us/cli/azure/vm?view=azure-cli-latest#az_vm_capture (VM Capture) https://docs.microsoft.com/en-us/cli/azure/vm?view=azure-cli-latest#az_vm_generalize (VM Generalize) https://docs.microsoft.com/en-us/cli/azure/vm/identity?view=azure-cli-latest#az_vm_identity_assign (VM Identity) https://docs.microsoft.com/en-us/cli/azure/ad/signed-in-user?view=azure-cli-latest (AD Signed in User) https://docs.microsoft.com/en-us/cli/azure/vm/extension?view=azure-cli-latest#az_vm_extension_set (VM Extension) https://docs.microsoft.com/en-us/cli/azure/keyvault?view=azure-cli-latest (Key Vault) https://docs.microsoft.com/en-us/cli/azure/monitor/diagnostic-settings?view=azure-cli-latest (Azure Monitor) https://docs.microsoft.com/en-us/cli/azure/desktopvirtualization?view=azure-cli-latest (AVD) https://docs.microsoft.com/en-us/cli/azure/desktopvirtualization/hostpool?view=azure-cli-latest#az_desktopvirtualization_hostpool_update (AVD Hostpool token) Aftertaste This cocktail has a strong sweet Azure CLI taste with a little bit of PowerShell. At the end you will have an AVD environment in Azure deployed with all the needed resources. These are a hostpool, a workspace, an application group. Also there are some session hosts. These hosts have an underlying image from a shared image gallery.\nAVD automated with Azure CLI In this chapter I will explain how to deploy an AVD environment automated with Azure CLI. I will recommend to clone my AVD GitHub repository to get all the needed files, also for the other cocktails. I like to work with functions as much as I can. Using functions will avoid you are repeating yourself in lines of code. It also will help you keeping your code nice and clean. You will notice that when looking back in the file structure I‚Äôve setup in the repository.\nResource Group Before deploying anything we need to login and create a new resource group.\n$location=westeurope $resourceGroupName=RG-ROZ-ORANGESUNSET az login az group create --name $resourceGroupName --location $location Networking The base of every environment is networking. In this step the idea is to deploy a new virtual network. The VNET has two subnets, a default subnet and an AVD-orange-subnet. The deployment code looks like the following:\n\u0026lt;pre class=\u0026#34;wp-block-code\u0026#34;\u0026gt;```json $deployNsg = az network nsg create --name nsg-roz-orangesunset --resource-group $resourceGroupName Next, when the network is deployed, the subnets are created.\n$deployVnet = az network vnet create --name vnet-roz-orangesunset --resource-group $resourceGroupName --address-prefixes 10.4.0.0/22 --network-security-group $nsg.NewNSG.name $vnet = $deployVnet | ConvertFrom-Json $deployDefaultSubnet = az network vnet subnet create --name DefaultSubnet --address-prefixes 10.4.1.0/24 --resource-group $resourceGroupName --vnet-name $vnet.newVNet.name $deployAvdSubnet = az network vnet subnet create --name AVD-Orange-Subnet --address-prefixes 10.4.2.0/24 --resource-group $resourceGroupName --vnet-name $vnet.newVNet.name Shared Image Gallery First, we need the Shared Image Gallery itself. Because I want to reuse parameters as much as possible, I stored the deployment output in a variable. After deployment, I convert the deployment output from JSON to PowerShell objects\n$gallery = $deploySig | ConvertFrom-Json $deploySigDefinition = az sig image-definition create --gallery-image-definition Win-Orange-Definition --gallery-name $gallery.name ` --resource-group $resourceGroupName --os-type Windows --hyper-v-generation V2 ` --offer Sweet --publisher Orange --sku Sunset While the deployment is running you will notice the ‚ÄúRunning‚Äù state in the terminal.\nFinally the deployment is successful.\nInitial Image Version Next step in our OrangeSunset-deployment is creating a new virtual machine. This machine is used for creating a new image version. This version is deployed into the gallery.\nCreate a virtual machine Before I create a virtual machine through Azure CLI I need the correct image. By using the az vm image list command I get an complete image overview. Because of the long image list, I use a filter based on the publisher and SKU. After running the command I get some image versions for the 21h1 version. I picked the latest.\n$initialImage = az vm image list --publisher MicrosoftWindowsDesktop --sku 21h1-evd-g2 --all $lastImage = ($initialImage | ConvertFrom-Json)[-1] I use the ‚Äìimage option with the urn output.\n[ { \u0026#34;offer\u0026#34;: \u0026#34;Windows-10\u0026#34;, \u0026#34;publisher\u0026#34;: \u0026#34;MicrosoftWindowsDesktop\u0026#34;, \u0026#34;sku\u0026#34;: \u0026#34;21h1-evd-g2\u0026#34;, \u0026#34;urn\u0026#34;: \u0026#34;MicrosoftWindowsDesktop:Windows-10:21h1-evd-g2:19043.1083.2107060627\u0026#34;, \u0026#34;version\u0026#34;: \u0026#34;19043.1083.2107060627\u0026#34; }, { \u0026#34;offer\u0026#34;: \u0026#34;Windows-10\u0026#34;, \u0026#34;publisher\u0026#34;: \u0026#34;MicrosoftWindowsDesktop\u0026#34;, \u0026#34;sku\u0026#34;: \u0026#34;21h1-evd-g2\u0026#34;, \u0026#34;urn\u0026#34;: \u0026#34;MicrosoftWindowsDesktop:Windows-10:21h1-evd-g2:19043.1110.2107101729\u0026#34;, \u0026#34;version\u0026#34;: \u0026#34;19043.1110.2107101729\u0026#34; } ] Now we have all the information we can create the initial virtual machine.\n$subnet = $deployAvdSubnet | ConvertFrom-Json $deployVm = az vm create --name orange-vm --resource-group $resourceGroupName --image $lastImage.urn ` --size Standard_D2s_v3 --vnet-name $vnet.newVNet.name --subnet $subnet.name --admin-username \u0026#39;localadmin\u0026#39; --admin-password \u0026#39;verytS3cr3t!\u0026#39; The az vm create command creates all the needed components like an OS disk and a network card.\nSysPrep First the VM must be and sysprepped and generalized. There are several options for running a script on a VM. This is like the Custom Script Extension or an Invoke-AzRunCommand in PowerShell. In this case I‚Äôm using the Azure CLI.\n$vm = $deployVm | ConvertFrom-Json az vm run-command invoke --command-id RunPowerShellScript --name $vm.id.Split(\u0026#34;/\u0026#34;)[-1] --resource-group $resourceGroupName --scripts \u0026#39;param([string]$sysprep,[string]$arg) Start-Process -FilePath $sysprep -ArgumentList $arg\u0026#39; --parameters \u0026#34;sysprep=C:\\Windows\\System32\\Sysprep\\Sysprep.exe\u0026#34; \u0026#34;arg=/generalize /oobe /shutdown /quiet /mode:vm\u0026#34; Using the az vm run-command CLI comand gives me the option to skip creating an separate script first. In the ‚Äìscripts part of the command is able to create a PowerShell on-the-fly. In the ‚Äìparameters part I will send these parameters.\nWhen running the command, the VM will create a PowerShell file on the local machine. The provided ‚Äìscripts content is stored in that local file.\nGeneralize VM Next step before creating an image version is generalize and capture the virtual machine. To achieve that goal we are using the CLI again.\naz vm generalize --name $vm --resource-group $resourceGroupName Create image version Finally, it is time to create a new image version. In the first place I‚Äôm converting the outputs from the gallery and definition deployment. Because I want to create an image from a VM, I use the source as input for the image creation. I also refer to the gallery- and definition-deployment.\n$gallery = $deploySig | ConvertFrom-Json $imageDef = $deploySigDefinition | ConvertFrom-Json $vm = $deployVm | ConvertFrom-Json $deployImageVersion = az sig image-version create --resource-group $resourceGroupName --gallery-image-version 2021.1907.01 ` --gallery-image-definition $imageDef.name --gallery-name $gallery.name --managed-image $vm.id After executing the CLI command it could take a while before the image is ready. While the task is running the shell shows the running state. In the meantime, I take a look in the Azure Portal. You see the image has the creating status.\nAzure Virtual Desktop Now every needed component is in place it is time to deploy the the Azure Virtual Desktop environment with Azure CLI. In this deployment we are going to deploy a host pool. The next step is the application group. The last step is a workspace.\nBefore using the desktop virtualization commands, we need to install the Azure CLI desktopvirtualization extension. When executing a ‚Äòdesktopvirtualization‚Äô-command the extension will automatically install.\nAVD Azure AD join Automated We all heard the news that Azure AD join is now in public preview. Because of that I decided to use that new feature. To make this feature work you‚Äôll need to walk through some steps. I will explain these steps in short.\nFirst is adding an extra RDP property to the host pool, targetisaadjoined:i:1. The host pool also needs the validation environment set.\nSecond we need assign a Virtual Machine User Login or Virtual Machine Administrator Login role. This role can be assigned at VM, resource group or at subscription level. I choose the resource group scope. I use the All Users AD group for example.\nTo realize that a few command are needed. At first we need to get the All Users objectId\n$objectId = az ad group show --group \u0026#34;All Users\u0026#34; --query \u0026#34;objectId\u0026#34; Next is finding the correct role. I use the Virtual Machine User Login role. This role is a BuilldIn role.\n$role = az role definition list --name \u0026#34;Virtual Machine User Login\u0026#34; --query \u0026#34;name\u0026#34; $roleName = $role | Convertfrom-json Now we have the correct Id‚Äôs it is time to assign the role to the resource group for All Users\naz role assignment create --assignee $objectId --role $roleName.name --resource-group $resourceGroupName At last we need to install the correct VM extension. I will disqus that point later in the session host deployment.\nMore info about Azure AD Join: https://docs.microsoft.com/en-us/azure/virtual-desktop/deploy-azure-ad-joined-vm\nHostpool First, I create a date in string format with an addition of 1 hour. One hour is the minimum expiration, 27 days is the maximum.\n$date = (Get-Date).AddHours(1).ToString(\u0026#34;yyyy\u0026#39;-\u0026#39;MM\u0026#39;-\u0026#39;dd\u0026#39;T\u0026#39;HH\u0026#39;:\u0026#39;mm\u0026#39;:\u0026#39;ss.fffffffK\u0026#34;) $deployAvdHostpool = az desktopvirtualization hostpool create --location $location --resource-group $resourceGroupName --name Sunset-Hostpool ` --description \u0026#34;For a nice relaxing sunset\u0026#34; --friendly-name \u0026#34;Orange Sunset Hostpool\u0026#34; --host-pool-type \u0026#34;Pooled\u0026#34; --load-balancer-type \u0026#34;BreadthFirst\u0026#34; ` --max-session-limit 10 --personal-desktop-assignment-type \u0026#34;Automatic\u0026#34; --registration-info expiration-time=$date registration-token-operation=\u0026#34;Update\u0026#34; --custom-rdp-property \u0026#34;targetisaadjoined:i:1\u0026#34; --validation-environment $true Second, the AVD host pool is deployed with the Azure AD needed RDP properties.\nApplication group I used the code below to deploy the application group. The application group is the place where to assign users/groups to the AVD environment.\n$hostpool = $deployAvdHostpool | ConvertFrom-Json $deployAvdApplicationGroup = az desktopvirtualization applicationgroup create --location $location --resource-group $resourceGroupName --name \u0026#34;Orange-ApplicationGroup\u0026#34; ` --description \u0026#34;Application group with oranges\u0026#34; --application-group-type \u0026#34;RemoteApp\u0026#34; --friendly-name \u0026#34;The Sweet Orange Sunset group\u0026#34; --host-pool-arm-path $hostpool.id Workspace In front of AVD we have the workspace. This is the place where people subscribe to. I‚Äôm also referring to the application output. Based on that output I created a workspace name.\n$applicationGroup = $deployAvdApplicationGroup | ConvertFrom-Json $deployAvdWorkspace = az desktopvirtualization workspace create --location $location --resource-group $resourceGroupName --name \u0026#34;Sweet-Workspace\u0026#34; ` --description \u0026#34;A Sweet Workspace\u0026#34; --friendly-name \u0026#34;Sweet Workplace\u0026#34; --application-group-references $applicationGroup.id Monitoring As every environment we also like to monitor this environment. To monitor this environment we are going to use Log Analytics.\nI used a template which deploys a LogAnalytics workspace. This will enable the provided logs for the AVD environment. After creating the workspace the diagnostic settings will be deployed.\nFor the WorkspaceId I referenced the output of the LogAnalytics Workspace resource deployment.\n$deployLogAnalytics = az monitor log-analytics workspace create --resource-group $resourceGroupName --workspace-name Orange-LA-Workspace Next we need to configure the diagnostic settings for the AVD host pool. In the command below I configure the checkpoint and error logs. The logs are send to the just created Log Analytics Workspace.\n$workspace = $deployLogAnalytics | ConvertFrom-Json $logs = \u0026#39;[{\u0026#34;\u0026#34;category\u0026#34;\u0026#34;: \u0026#34;\u0026#34;Checkpoint\u0026#34;\u0026#34;, \u0026#34;\u0026#34;categoryGroup\u0026#34;\u0026#34;: null, \u0026#34;\u0026#34;enabled\u0026#34;\u0026#34;: true, \u0026#34;\u0026#34;retentionPolicy\u0026#34;\u0026#34;: { \u0026#34;\u0026#34;days\u0026#34;\u0026#34;: 0, \u0026#34;\u0026#34;enabled\u0026#34;\u0026#34;: false }},{\u0026#34;\u0026#34;category\u0026#34;\u0026#34;: \u0026#34;\u0026#34;Error\u0026#34;\u0026#34;,\u0026#34;\u0026#34;categoryGroup\u0026#34;\u0026#34;: null,\u0026#34;\u0026#34;enabled\u0026#34;\u0026#34;: true,\u0026#34;\u0026#34;retentionPolicy\u0026#34;\u0026#34;: {\u0026#34;\u0026#34;days\u0026#34;\u0026#34;: 0,\u0026#34;\u0026#34;enabled\u0026#34;\u0026#34;: false}}]\u0026#39; $deployDiagnostics = az monitor diagnostic-settings create --name avd-diag-settings --resource $hostpool.id --workspace $workspace.id --logs $logs To explain the context I used one line with JSON formatted text. Finally, you should use a JSON formatted file. In that case use ‚Äò@{file}‚Äô to load from a file.\nAVD Session hosts Last is deploying the session hosts into the AVD hostpool. Before deploying a session host I decided to deploy an Azure Key Vault first. Into the key vault, I will store the administrator password for the domain join. In the later steps I will reference to this key vault secret in the template.\nAzure Key vault In this step I will create an Azure Key Vault with Azure CLI and store the administrator password in it.\n$deployKeyVault = az keyvault create --location $location --resource-group $resourceGroupName --name SweetOrange-KeyVault $keyvault = $deployKeyVault | ConvertFrom-Json $deploySecretPass = az keyvault secret set --name vmjoinerPassword --vault-name $keyvault.name --value \u0026#39;veryS3cretP@ssw0rd!\u0026#39; Info: Because I deploy a key vault from my own account I allready have access to the key vault. In case of deploying a key vault from DevOps for example the context may be different. Make sure you set the correct permissions for users.\nAzure AAD Join Settings To join an AVD session host to Azure AD you need some different settings relating to the default settings (native AD). First, thing to remember is the Desired State Config extension. Second thing which is different is the VM identity. A not AAD joined VM has no system assigned identity in the Azure AD.\nAVD Extension The AVD module is a Desired State Config (DSC) extension. DSC is a management platform in PowerShell that enables you to ‚Äòpush‚Äô configuration settings as code to a resource. In case of AVD, means that the extension installs the AVD software with specific AVD environment settings. For example the registration token and the host pool name. Now AADJoin is part of the family a new module came up. This new module accepts the aadJoin parameter, which the native module does not.\nIf you install the native module with the AADJoin parameter you will get a message like below.\n(ArtifactNotFound) The VM extension with publisher ‚ÄòMicrosoft.Azure.ActiveDirectory‚Äô and type ‚ÄòActiveDirectory‚Äô could not be found.\n(VMExtensionProvisioningError) VM has reported a failure when processing extension ‚ÄòDSC‚Äô. Error message: ‚ÄúThe DSC Extension received an incorrect input: A parameter cannot be found that matches parameter name ‚ÄòaadJoin‚Äô.\nAfter digging into the deployment I found the correct artifact URL.\nhttps://wvdportalstorageblob.blob.core.windows.net/galleryartifacts/Configuration_01-19-2023.zip To deploy the AVD DSC extension with Azure CLI I used the code below. Make a notice about the aadJoin parameter in the settings.\n$moduleLocation = \u0026#34;https://wvdportalstorageblob.blob.core.windows.net/galleryartifacts/Configuration_01-19-2023.zip\u0026#34; $avdExtensionName = \u0026#34;DSC\u0026#34; $avdExtensionPublisher = \u0026#34;Microsoft.Powershell\u0026#34; $avdExtensionVersion = \u0026#34;2.73\u0026#34; $avdExtensionSetting = \u0026#39;{\u0026#34;\u0026#34;modulesUrl\u0026#34;\u0026#34;: \u0026#34;\u0026#34;\u0026#39;+$moduleLocation+\u0026#39;\u0026#34;\u0026#34;,\u0026#34;\u0026#34;ConfigurationFunction\u0026#34;\u0026#34;:\u0026#34;\u0026#34;Configuration.ps1\\\\AddSessionHost\u0026#34;\u0026#34;,\u0026#34;\u0026#34;Properties\u0026#34;\u0026#34;: {\u0026#34;\u0026#34;hostPoolName\u0026#34;\u0026#34;: \u0026#34;\u0026#34;\u0026#39;+ $($hostpool.name) + \u0026#39;\u0026#34;\u0026#34;,\u0026#34;\u0026#34;registrationInfoToken\u0026#34;\u0026#34;: \u0026#34;\u0026#34;\u0026#39;+ $($hostpool.registrationInfo.token) + \u0026#39;\u0026#34;\u0026#34;, \u0026#34;\u0026#34;aadJoin\u0026#34;\u0026#34;: \u0026#34;\u0026#39;+ $true + \u0026#39;\u0026#34;}}\u0026#39; System identity The next difference between a native domain joined VM is the resource identity.\nFrom Microsoft: System-assigned Some Azure services allow you to enable a managed identity directly on a service instance. When you enable a system-assigned managed identity, the identity is created in Azure AD that is tied to the lifecycle of that service instance. So when the resource is deleted, Azure automatically deletes the identity for you. By design, only that Azure resource can use this identity to request tokens from Azure AD.\nUse the Azure CLI command below for assigning the system identity.\naz vm identity assign --name $vmName --resource-group $resourceGroupName --identities System In addition to this context please check the following URL for more information: https://docs.microsoft.com/en-us/azure/active-directory/managed-identities-azure-resources/overview\nCreate session host Now it is time creating session hosts with Azure CLI. Before starting the deployment we need to create a host pool registration token first. This token will be use to add the session host into the correct host pool.\n$hostpoolToken¬†=¬†az¬†desktopvirtualization¬†hostpool¬†update¬†--resource-group¬†RG-ROZ-MINTYBREEZE-COCKTAIL¬†--name¬†MintyBreeze-Hostpool¬†--registration-info¬†expiration-time=\u0026#34;2021-06-16T15:01:54.9571247Z\u0026#34;¬†registration-token-operation=\u0026#34;Update\u0026#34;¬†--query¬†\u0026#39;registrationInfo.token\u0026#39; The command below will take care for getting the domain join password from the key vault.\n$adminpassword¬†=¬†az¬†KeyVault¬†secret¬†show¬†--vault-name¬†$keyvault.name¬†--name¬†vmjoinerPassword --query¬†value Public IP and NSG When deploying a virtual machine with Azure CLI, a Public IP address and a NSG is configured as default. Because I don‚Äôt want public ip‚Äôs assigned to my session hosts we need to avoid that. Because the VNET has a NSG allready, I also don‚Äôt want a NSG created. The way to avoid creating a public IP and a NSG is adding a null value in the parameters.\n--public-ip-address \u0026#39;\u0026#34;\u0026#34;\u0026#39; --nsg \u0026#39;\u0026#34;\u0026#34;\u0026#39; At the end the complete session host deployment code will look like below. The used commands can be found at my GitHub page.\n$sessionHostCount = 1 $initialNumber = 1 $VMLocalAdminUser = \u0026#34;LocalAdminUser\u0026#34; $adminpassword = az KeyVault secret show --vault-name $keyvault.name --name vmjoinerPassword --query value $avdPrefix = \u0026#34;sun-\u0026#34; $vmSize = \u0026#34;Standard_D2s_v3\u0026#34; $image = $deployImageVersion | ConvertFrom-Json $domainJoinName = \u0026#34;AADLoginForWindows\u0026#34; $domainJoinPublisher = \u0026#34;Microsoft.Azure.ActiveDirectory\u0026#34; $domainJoinVersion = \u0026#34;1.0\u0026#34; $domainJoinSettings = \u0026#39;{\u0026#34;\u0026#34;mdmId\u0026#34;\u0026#34;: \u0026#34;\u0026#34;0000000a-0000-0000-c000-000000000000\u0026#34;\u0026#34;}\u0026#39; $moduleLocation = \u0026#34;https://wvdportalstorageblob.blob.core.windows.net/galleryartifacts/Configuration_01-19-2023.zip\u0026#34; $avdExtensionName = \u0026#34;DSC\u0026#34; $avdExtensionPublisher = \u0026#34;Microsoft.Powershell\u0026#34; $avdExtensionVersion = \u0026#34;2.73\u0026#34; $avdExtensionSetting = \u0026#39;{\u0026#34;\u0026#34;modulesUrl\u0026#34;\u0026#34;: \u0026#34;\u0026#34;\u0026#39;+$moduleLocation+\u0026#39;\u0026#34;\u0026#34;,\u0026#34;\u0026#34;ConfigurationFunction\u0026#34;\u0026#34;:\u0026#34;\u0026#34;Configuration.ps1\\\\AddSessionHost\u0026#34;\u0026#34;,\u0026#34;\u0026#34;Properties\u0026#34;\u0026#34;: {\u0026#34;\u0026#34;hostPoolName\u0026#34;\u0026#34;: \u0026#34;\u0026#34;\u0026#39;+ $($hostpool.name) + \u0026#39;\u0026#34;\u0026#34;,\u0026#34;\u0026#34;registrationInfoToken\u0026#34;\u0026#34;: \u0026#34;\u0026#34;\u0026#39;+ $($hostpool.registrationInfo.token) + \u0026#39;\u0026#34;\u0026#34;, \u0026#34;\u0026#34;aadJoin\u0026#34;\u0026#34;: \u0026#34;\u0026#39;+ $true + \u0026#39;\u0026#34;}}\u0026#39; Do { $vmName = $avdPrefix+\u0026#34;$initialNumber\u0026#34; az vm create --name $vmName --resource-group $resourceGroupName --image $image.id --size $vmSize --vnet-name $vnet.newVNet.name --subnet $subnet.name --admin-username $VMLocalAdminUser --admin-password $adminpassword --public-ip-address \u0026#39;\u0026#34;\u0026#34;\u0026#39; --nsg \u0026#39;\u0026#34;\u0026#34;\u0026#39; az vm identity assign --name $vmName --resource-group $resourceGroupName az vm extension set --vm-name $vmName --resource-group $resourceGroupName --name $domainJoinName --publisher $domainJoinPublisher --version $domainJoinVersion --settings $domainJoinSettings az vm extension set --vm-name $vmName --resource-group $resourceGroupName --name $avdExtensionName --publisher $avdExtensionPublisher --version $avdExtensionVersion --settings $avdExtensionSetting $initialNumber++ $sessionHostCount-- Write-Output \u0026#34;$vmName deployed\u0026#34; } while ($sessionHostCount -ne 0) { Write-Verbose \u0026#34;Session hosts are created\u0026#34; } Finally, at the end, we have Azure AD Join session hosts.\nI have the automated assignment of users to application group under investigation.\nFor now everything is in place you only have to assign a usergroup to the application group and that‚Äôs it.\nConclusion Azure CLI is fast but has a steep learning curve. The basics are quite simple, but if you go more advance I will take some time. For example if you want to pass some JSON formatting. I had to use my ARM knowledge to create the correct JSON format for the extension deployment. Digging into ARM deployments helped me to get the correct information for the extension deployments. Also via ARM I noticed the VM needs an Azure AD system assigned identity.\nI have chosen to keep it as simple as possible from my opinion. I used some fixed values as well. In fact all these values should be in one singles parameter file.\nIn basic I‚Äôm very surprised about Azure CLI. It feels very fast in relation to PowerShell, at least with deployment (or other ‚Äòpush‚Äô tasks). Querying info feels a bit slow. A big advantage is that you don‚Äôt need modules, well not as many as in PowerShell :). For the desktop virtualization part I had to install an extension but that‚Äôs it (which goes automatically).\nAn another point is that you don‚Äôt have to remember use a lot different commands like get-avd, set-avd, update-avd. It all starts with the same context and then what you like to do, show, remove, update for example.\nThank you! I hope you liked the Sweet Orange üçä Sunset ‚òÄÔ∏è you are a bit inspired. Now you know deploy AVD automated with Azure CLI is one of the options. If you like an another cocktail feel free to stay and check the AVD Automation Cocktail menu.\nThank you for reading my blog avd automation cocktail ‚Äì avd automated with azure cli. I hope you got a bit inspired. Enjoy your day and happy automating üëã ","date":"July 20, 2021","image":"http://localhost:1313/avd-automation-cocktail-avd-automated-with-azure-cli/cocktail-orange-sunset.png","permalink":"/avd-automation-cocktail-avd-automated-with-azure-cli/","title":"AVD Automation Cocktail ‚Äì AVD automated with Azure CLI"},{"categories":["AVD Cocktail","Azure Virtual Desktop"],"contents":"Welcome to the AVD Automation Cocktail. In this cocktail series, I will show different AVD deployment strategies and languages. In this cocktail, the Pineapple Citrus Splash, I will show you how to deploy an AVD environment automated with REST API, and JSON executed with PowerShell.\nTable Of Contents Recipe Before to drink List of REST API ingredients Aftertaste AVD automated with REST API REST API with PowerShell Invoke-WebRequest Invoke-RestMethod Authenticating Formatting Resource group Networking Shared Image Gallery Initial image version Create a virtual machine Sysprep Generalize Create image version Azure Virtual Desktop AVD Hostpool Application group Workspace Monitoring Create an AVD session host with REST API Conclusion Thank you! Recipe In this ‚ÄúPineappleCitrus‚Äù-deployment recipe I will deploy an AVD environment automated with REST API, JSON input and PowerShell. You can leave PowerShell of you like an other coding language which is able to send API calls.\nBefore to drink To begin with the most important thing. Make sure you have an Active Directory domain present for the domain join.\nList of REST API ingredients Method JSON Body WebRequest Aftertaste This cocktail has REST API as main ingredient with some JSON and PowerShell on the side. During this post I will show how to deploy all the needed resources. Finally at the end you will have an AVD environment in Azure deployed automated with REST API. Some of these resources are a host pool, a workspace, an application group. Also there are some session hosts. At last these hosts have an underlying image from a shared image gallery.\nTo learn more about REST API please check the following url: https://docs.microsoft.com/en-us/rest/api/azure/\nAVD automated with REST API In this chapter I will explain how to deploy an AVD environment automated with REST API. Also I‚Äôm using JSON formatting and PowerShell for execution. REST API, or Representational State Transfer (REST) APIs are service endpoints that support sets of HTTP operations (methods). REST API‚Äôs provide create, retrieve, update, or delete access to the service‚Äôs resources.\nTo be sure you won‚Äôt miss code, I recommend to clone my AVD GitHub repository to get all the needed files. Also for the other cocktails.\nAt the end as a result an AVD environment with a SIG in basics.\nREST API with PowerShell At first, REST API‚Äôs have less dependencies like PowerShell modules or ARM templates. For this reason it is useful to know how REST API work. In the examples below I use PowerShell to send the API calls. Important to realize is that PowerShell has two commands which allows you to send REST API calls.\nThe Invoke-RestMethod and Invoke-WebRequest. There is some similarity between both commands. In the first place both accepts JSON input and are able to send requests to an API URL for example. Beside that there are some big differences.\nInvoke-WebRequest From Microsoft: The Invoke-WebRequest cmdlet sends HTTP and HTTPS requests to a web page or web service. It parses the response and returns collections of links, images, and other significant HTML elements.\nIn fact this means you will get a website response with content. For example a code like 200 OK or 503 Service Unavailable. By catching these codes you know the status of your API call.\nHowever the result is returned in JSON format returned. To read the content as PowerShell objects you need to deserialize it. Deserialize code can be done with ConvertFrom-Json whit PowerShell readable objects as a result.\nInvoke-RestMethod From Microsoft: The Invoke-RestMethod cmdlet sends HTTP and HTTPS requests to Representational State Transfer (REST) web services that return richly structured data.\nPowerShell formats the response based to the data type. For an RSS or ATOM feed, PowerShell returns the Item or Entry XML nodes. For JavaScript Object Notation (JSON) or XML, PowerShell converts, or deserializes, the content into [PSCustomObject] objects.\nIn short this means you get PowerShell objects as a result. You don‚Äôt need to deserialize it. A disadvantage is that you have no idea of the current status.\nIn addition to the context above there is more about creating galleries and images at the links below:\nhttps://docs.microsoft.com/en-us/powershell/module/microsoft.powershell.utility/invoke-restmethod?view=powershell-7.1 https://docs.microsoft.com/en-us/powershell/module/microsoft.powershell.utility/invoke-webrequest?view=powershell-7.1 Authenticating Before you are able to use the REST API you will need to authenticate to the API. Because we are using PowerShell for executing the REST API calls we need to authenticate in PowerShell. Within the current context we are gathering a header token for the REST API.\nfunction¬†GetAuthToken($resource)¬†{ $context¬†=¬†[Microsoft.Azure.Commands.Common.Authentication.Abstractions.AzureRmProfileProvider]::Instance.Profile.DefaultContext $Token¬†=¬†[Microsoft.Azure.Commands.Common.Authentication.AzureSession]::Instance.AuthenticationFactory.Authenticate($context.Account,¬†$context.Environment,¬†$context.Tenant.Id.ToString(),¬†$null,¬†[Microsoft.Azure.Commands.Common.Authentication.ShowDialog]::Never,¬†$null,¬†$resource).AccessToken $authHeader¬†=¬†@{ \u0026#39;Content-Type\u0026#39;¬†=¬†\u0026#39;application/json\u0026#39; Authorization¬†=¬†\u0026#39;Bearer¬†\u0026#39;¬†+¬†$Token } return¬†$authHeader } $token¬†=¬†GetAuthToken¬†-resource¬†\u0026#34;https://management.azure.com\u0026#34; The authentication token returns into the $token variable. This is the token we need for requesting API calls to Azure.\nFormatting Formatting is big point in JSON. Because the other side need to ‚Äòunpack‚Äô or deserialize the code it is important the correct format is provided. When converting PowerShell objects, arrays and hashtables into JSON format make sure every items is in the JSON code. If you have nested tables or arrays you will need the provide the -Depth command. Default depth is 2.\nIn the example below you see the same content with a depth of 4 and 5. See the difference.\n$nicBody¬†|¬†ConvertTo-Json¬†-Depth¬†4 $nicBody¬†|¬†ConvertTo-Json¬†-Depth¬†5 For more about ConvertTo-Json please check the URL below.\nhttps://docs.microsoft.com/en-us/powershell/module/microsoft.powershell.utility/convertto-json?view=powershell-7.1\nResource group Because we need a resource group, the first step in our deployment is creating a new one. In this group I will deploy all the resources in this blog post. I start with a subscriptionId, a resourcegroupName and a location. Later on these value will reused. Because the resourcegroup output is needed in every deployment I save the output into a variable.\n$subscriptionId = [Microsoft.Azure.Commands.Common.Authentication.Abstractions.AzureRmProfileProvider]::Instance.Profile.DefaultContext.Subscription.id $ResourceGroupName = \u0026#39;RG-ROZ-PINEAPPLECITRUS\u0026#39; $location = \u0026#34;West Europe\u0026#34; $url = \u0026#34;https://management.azure.com/subscriptions/\u0026#34; + $($subscriptionId) + \u0026#34;/resourcegroups/\u0026#34; + $ResourceGroupName + \u0026#34;?api-version=2021-04-01\u0026#34; $body = @{ location = $location } $parameters = @{ uri = $url method = \u0026#39;PUT\u0026#39; header = $token body = $body | ConvertTo-Json } $ResourceGroup = Invoke-RestMethod @Parameters Networking The second step of our ‚ÄúPinappleCitrus‚Äù-deployment is deploying a VNET. Thereafter two subnets and a network security group (NSG) are deployed. A NSG will help us protecting our network for unwanted traffic. To make sure we can find our domain controller I added a custom DNS.\nBecause the NSG is securing our network it is deployed at first. To deploy a network I used the code below.\n$networkSecurityGroupName = \u0026#34;NSG-PineappleCitrus\u0026#34; $nsgUrl = \u0026#34;https://management.azure.com/subscriptions/\u0026#34; + $($subscriptionId) + \u0026#34;/resourcegroups/\u0026#34; + $ResourceGroupName + \u0026#34;/providers/Microsoft.Network/networkSecurityGroups/\u0026#34; + $networkSecurityGroupName + \u0026#34;?api-version=2020-11-01\u0026#34; $nsgBody = @{ location = $location properties = @{ } } $nsgParameters = @{ uri = $nsgUrl method = \u0026#39;PUT\u0026#39; header = $token body = $nsgBody | ConvertTo-Json } $networkSecurityGroup = Invoke-WebRequest @nsgParameters Once the NSG is deployed, we are creating the virtual network including the subnets. To create a network I reuse the resource group name and adding the virtual network name. For example, in the JSON body I‚Äôve set some fixed address.\n$vnetName¬†=¬†\u0026#34;vnet-PineappleCitrus\u0026#34; $vnetUrl = \u0026#34;https://management.azure.com/subscriptions/\u0026#34; + $($subscriptionId) + \u0026#34;/resourcegroups/\u0026#34; + $ResourceGroupName + \u0026#34;/providers/Microsoft.Network/virtualNetworks/\u0026#34; + $vnetName + \u0026#34;?api-version=2021-02-01\u0026#34; $vnetBody = @{ location = $location properties = @{ AddressSpace = @{ addressPrefixes = @( \u0026#34;10.0.0.0/16\u0026#34; ) } dhcpOptions = @{ dnsServers = @( \u0026#34;10.1.3.4\u0026#34; ) } subnets = @( @{ name = \u0026#39;defaultSubnet\u0026#39; properties = @{ addressPrefix = \u0026#34;10.0.1.0/24\u0026#34; } }, @{ name = \u0026#39;CitrusSubnet\u0026#39; properties = @{ addressPrefix = \u0026#34;10.0.2.0/24\u0026#34; } } ) } } $vnetParameters = @{ uri = $vnetUrl method = \u0026#39;PUT\u0026#39; header = $token body = $vnetBody | ConvertTo-Json -Depth 4 } $virtualNetwork = Invoke-WebRequest @vnetParameters At the end the virtual network is deployed with the correct settings.\nMore about deploying networks with REST API, please check the Microsoft Virtual Network documentation\nShared Image Gallery Because I want to avoid updating every single session host in feature I create a shared image gallery with versions. We going to use these versions to start AVD sessions host later in this article.\nFirst we are creating the gallery itself.\n$galleryName = \u0026#34;Citrus_Gallery\u0026#34; $galleryUrl = \u0026#34;https://management.azure.com/subscriptions/\u0026#34; + $($subscriptionId) + \u0026#34;/resourcegroups/\u0026#34; + $ResourceGroupName + \u0026#34;/providers/Microsoft.Compute/galleries/\u0026#34; + $galleryName + \u0026#34;?api-version=2021-03-01\u0026#34; $galleryBody = @{ location = $location properties = @{ description = \u0026#34;A really fresh gallery with pineapple and citrus.\u0026#34; } } $galleryParameters = @{ uri = $galleryUrl method = \u0026#39;PUT\u0026#39; header = $token body = $galleryBody | ConvertTo-Json -Depth 4 } $sharedImageGallery = Invoke-WebRequest @galleryParameters ### Shared Image Gallery Definition\nAn image definition are image properties, for example the OS is Windows and it is generalized. To deploy an image definition I used the code below.\n$sharedImageGalleryInfo = ($sharedImageGallery | ConvertFrom-Json).id $galleryImageName = \u0026#34;Win10-Pineapple-Image\u0026#34; $galleryImageUrl = \u0026#34;https://management.azure.com/\u0026#34; + $sharedImageGalleryInfo + \u0026#34;/images/\u0026#34; + $galleryImageName + \u0026#34;?api-version=2021-03-01\u0026#34; $galleryImageBody = @{ location = $location properties = @{ identifier = @{ offer = \u0026#34;Pineapple\u0026#34; publisher = \u0026#34;Rozemuller\u0026#34; sku = \u0026#34;Citrus\u0026#34; } osState = \u0026#34;Generalized\u0026#34; osType = \u0026#34;Windows\u0026#34; description = \u0026#34;Citrus are lovely\u0026#34; hyperVGeneration = \u0026#34;V2\u0026#34; } } $galleryImageParameters = @{ uri = $galleryImageUrl method = \u0026#39;PUT\u0026#39; header = $token body = $galleryImageBody | ConvertTo-Json -Depth 4 } $sharedImageGalleryImage = Invoke-WebRequest @galleryImageParameters https://docs.microsoft.com/en-us/rest/api/compute/gallery-images\nInitial image version To start with a good base it is time to create an initial image version. Later on we are able to create new versions based on this one.\nCreate a virtual machine The first step is creating a new Windows 10 Multi Session generation 2 virtual machine. This VM generation must be the same as the image definition Hyper-V generation.\n$vmName¬†=¬†\u0026#34;vm-Pineapple\u0026#34; $vmUrl¬†=¬†\u0026#34;https://management.azure.com/\u0026#34;¬†+¬†$ResourceGroupUrl¬†+¬†\u0026#34;/providers/Microsoft.Compute/virtualMachines/\u0026#34;¬†+¬†$vmName¬†+¬†\u0026#34;?api-version=2021-03-01\u0026#34; $vmBody¬†=¬†@{ location¬†=¬†$location properties¬†=¬†@{ hardwareProfile¬†=¬†@{ vmSize¬†=¬†\u0026#34;Standard_B2ms\u0026#34; } networkProfile¬†=¬†@{ networkInterfaces¬†=¬†@( @{ id¬†=¬†$nicId properties¬†=¬†@{ primary¬†=¬†$true } } ) } storageProfile¬†=¬†@{ imageReference¬†=¬†@{ sku¬†=¬†\u0026#34;21h1-ent-g2\u0026#34; version¬†=¬†\u0026#34;latest\u0026#34; offer¬†=¬†\u0026#34;Windows-10\u0026#34; publisher¬†=¬†\u0026#34;MicrosoftWindowsDesktop\u0026#34; } osDisk¬†=¬†@{ caching¬†=¬†\u0026#34;ReadWrite\u0026#34; managedDisk¬†=¬†@{ storageAccountType¬†=¬†\u0026#34;Standard_LRS\u0026#34; } name¬†=¬†\u0026#34;os-pineapple\u0026#34; createOption¬†=¬†\u0026#34;FromImage\u0026#34; } } osProfile¬†=¬†@{ adminUsername¬†=¬†\u0026#34;citrus-user\u0026#34; computerName¬†=¬†$vmName adminPassword¬†=¬†\u0026#34;VeryS3cretP@44W0rd!\u0026#34; } } } $vmParameters¬†=¬†@{ uri¬†=¬†$vmUrl¬†method¬†=¬†\u0026#39;PUT\u0026#39; header¬†=¬†$token body¬†=¬†$vmBody¬†|¬†ConvertTo-Json¬†-Depth¬†5 } $virtualMachine¬†=¬†Invoke-WebRequest¬†@vmParameters Because I need the VM info I stored the deployment output in a variable.\nSysprep It is possible to execute commands via the REST API. This is a great way to execute scripts without creating local scripts. Because of that I send an array of commands through the REST API.\n$script¬†=¬†[System.Collections.ArrayList]@() $script.Add(\u0026#39;$sysprep¬†=¬†\u0026#34;C:\\Windows\\System32\\Sysprep\\Sysprep.exe\u0026#34;\u0026#39;) $script.Add(\u0026#39;$arg¬†=¬†\u0026#34;/generalize¬†/oobe¬†/shutdown¬†/quiet¬†/mode:vm\u0026#34;\u0026#39;) $script.Add(\u0026#39;Start-Process¬†-FilePath¬†$sysprep¬†-ArgumentList¬†$arg\u0026#39;) $scriptBody¬†=¬†@{ commandId¬†=¬†\u0026#34;RunPowerShellScript\u0026#34; script¬†=¬†$script } $virtualMachineId¬†=¬†($virtualMachine¬†|¬†ConvertFrom-Json).id $url¬†=¬†\u0026#34;https://management.azure.com\u0026#34;¬†+¬†$virtualMachineId¬†+¬†\u0026#34;/runCommand?api-version=2021-03-01\u0026#34; $parameters¬†=¬†@{ URI¬†=¬†$url¬†Method¬†=¬†\u0026#34;POST\u0026#34; Body¬†=¬†$scriptBody¬†|¬†ConvertTo-Json Headers¬†=¬†$token } $executeSysprep¬†=¬†Invoke-WebRequest¬†@parameters Again, the REST API sends only a request. To make sure the virtual machine is stopped the current status must be checked. The virtual machine is stopped after the local PowerShell task is done.\nGeneralize Next is generalizing a virtual machine before we are able to create an image version. Generalizing a machine is done by one simple command.\n$generalizeUrl¬†=¬†\u0026#34;https://management.azure.com\u0026#34;¬†+¬†$virtualMachineId¬†+¬†\u0026#34;/generalize?api-version=2021-03-01\u0026#34; $generalizeParameters¬†=¬†@{ uri¬†=¬†$generalizeUrl method¬†=¬†\u0026#39;POST\u0026#39; header¬†=¬†$token } $generalizeVM¬†=¬†Invoke-WebRequest¬†@generalizeParameters Create image version The last step is creating a new image version into the gallery. As you can see I‚Äôm using information which I already know and has been stored into variables in the earlier steps. Now that the image has been deployed, the next step is creating an AVD session hosts from this image.\nBeside the resource group I need the Shared Image Gallery info, the virtual machine info and I have to wait till the VM is prepared.\nFor example, you could use some date formatting to generate a version number. A version number must have a SemVer notation.\n$sharedImageGalleryImageUrl¬†=¬†($sharedImageGalleryImage¬†|¬†ConvertFrom-Json).id $galleryVersionName¬†=¬†Get-Date¬†-Format¬†yyyy.MM.dd $versionUrl¬†=¬†\u0026#34;https://management.azure.com\u0026#34;¬†+¬†$sharedImageGalleryImageUrl¬†+¬†\u0026#34;/versions/\u0026#34;¬†+¬†$galleryVersionName¬†+¬†\u0026#34;?api-version=2021-03-01\u0026#34; $versionBody¬†=¬†@{ location¬†=¬†$location properties¬†=¬†@{ storageProfile¬†=¬†@{ source¬†=¬†@{ id¬†=¬†$virtualMachineId } } } } $imageVersionParameters¬†=¬†@{ uri¬†=¬†$versionUrl¬†method¬†=¬†\u0026#39;PUT\u0026#39; header¬†=¬†$token body¬†=¬†$versionBody¬†|¬†ConvertTo-Json¬†-Depth¬†5 } $imageVersion¬†=¬†Invoke-WebRequest¬†@imageVersionParameters In addition to the context above there is more about creating galleries and images at the links below:\nhttps://docs.microsoft.com/en-us/rest/api/compute/galleries/create-or-update https://docs.microsoft.com/en-us/rest/api/compute/gallery-images/create-or-update https://docs.microsoft.com/en-us/rest/api/compute/gallery-image-versions/create-or-update Azure Virtual Desktop After the AVD back-end is prepared, it is time to deploy the AVD environment. To keep my code clean I deploy every part in separate jobs.\nAVD Hostpool First in this post about deploying an AVD environment automated with REST API is creating an AVD host pool. At the same time I also configuring the new StartVMOnConnect option. This option allows you to shutdown session hosts at the end of the day. When the first account is connecting the host will start automatically.\nBecause I need a registration token later in the sequence, I create a new token based on the current date and time. I‚Äôm adding a few hours from now which means the token is valid till that time.\nAfter deployment I convert the output to PowerShell objects. One of these objects is the registration token. By the time I will use that token later in the sequence.\n$hostpoolInfo¬†=¬†($hostpool¬†|¬†ConvertFrom-Json) $token¬†=¬†$hostpoolInfo.properties.registrationInfo.token Application group The application group is the place where to assign users/groups to the AVD environment. Be sure you make a notice about the $hostpoolInfo variable.\n$applicationGroupName¬†=¬†\u0026#34;Pinapple-applications\u0026#34; $applicationGroupUrl¬†=¬†\u0026#34;https://management.azure.com\u0026#34;¬†+¬†$ResourceGroupUrl¬†+¬†\u0026#34;/providers/Microsoft.DesktopVirtualization/applicationGroups/\u0026#34;¬†+¬†$applicationGroupName¬†+¬†\u0026#34;?api-version=2021-01-14-preview\u0026#34; $applicationGroupBody¬†=¬†@{ location¬†=¬†$location properties¬†=¬†@{ applicationGroupType¬†=¬†\u0026#39;Desktop\u0026#39; hostPoolArmPath¬†=¬†$hostpoolInfo.id description¬†=¬†\u0026#39;A¬†nice¬†group¬†with¬†citrus¬†fruits\u0026#39; friendlyName¬†=¬†\u0026#39;Pineapple¬†Application¬†Group\u0026#39; } } $applicationGroupParameters¬†=¬†@{ uri¬†=¬†$applicationGroupUrl¬†method¬†=¬†\u0026#39;PUT\u0026#39; header¬†=¬†$token body¬†=¬†$applicationGroupBody¬†|¬†ConvertTo-Json¬†-Depth¬†5 } $applicationGroup¬†=¬†Invoke-WebRequest¬†@applicationGroupParameters Because there is no workspace, it is not assigned in the deployment. Next, after creating the workspace, we will assign the workspace.\nWorkspace In front of AVD we have the workspace. This is the place where people subscribe at. I‚Äôm also referring to the host pool output. Based on that output I created a workspace name.\n$applicationGroupInfo¬†=¬†($applicationGroup¬†|¬†ConvertFrom-Json) $workspaceName¬†=¬†\u0026#34;Citrus-Workspace\u0026#34; $workspaceUrl¬†=¬†\u0026#34;https://management.azure.com\u0026#34;¬†+¬†$ResourceGroupUrl¬†+¬†\u0026#34;/providers/Microsoft.DesktopVirtualization/workspaces/\u0026#34;¬†+¬†$workspaceName¬†+¬†\u0026#34;?api-version=2021-01-14-preview\u0026#34; $workspaceBody¬†=¬†@{ location¬†=¬†$location properties¬†=¬†@{ applicationGroupReferences¬†=¬†@( $applicationGroupInfo.id ) description¬†=¬†\u0026#39;A¬†workspace¬†with¬†nice¬†citrus¬†fruits\u0026#39; friendlyName¬†=¬†\u0026#39;Citrus¬†Workspace\u0026#39; } } $workspaceParameters¬†=¬†@{ uri¬†=¬†$workspaceUrl¬†method¬†=¬†\u0026#39;PUT\u0026#39; header¬†=¬†$token body¬†=¬†$workspaceBody¬†|¬†ConvertTo-Json¬†-Depth¬†5 } $workspace¬†=¬†Invoke-WebRequest¬†@workspaceParameters Make a note at the green box. After there is a workspace, we are able to assign the workspace to an application group.\nMake sure, after deployment, you will assign the workspace to the correct users or groups.\nMonitoring Now the environment is running we need monitoring. In the first place, we need to install a Log Analytics Workspace. As soon as the Log Analytics Workspace is created we will enable diagnostic settings on the host pool.\n$LAWorkspace¬†=¬†\u0026#34;log-analytics-avd-\u0026#34;¬†+¬†(Get-Random¬†-Maximum¬†99999) $LawsBody¬†=¬†@{ location¬†=¬†$location properties¬†=¬†@{ retentionInDays¬†=¬†\u0026#34;30\u0026#34; sku¬†=¬†@{ name¬†=¬†\u0026#34;PerGB2018\u0026#34; } } } $lawsUrl¬†=¬†\u0026#34;https://management.azure.com\u0026#34;¬†+¬†$ResourceGroupUrl¬†+¬†\u0026#34;/providers/Microsoft.OperationalInsights/workspaces/\u0026#34;¬†+¬†$LAWorkspace¬†+¬†\u0026#34;?api-version=2020-08-01\u0026#34; $loganalyticsParameters¬†=¬†@{ URI¬†=¬†$lawsUrl¬†Method¬†=¬†\u0026#34;PUT\u0026#34; Body¬†=¬†$LawsBody¬†|¬†ConvertTo-Json Headers¬†=¬†$token } $laws¬†=¬†Invoke-WebRequest¬†@loganalyticsParameters Second is assigning the just created workspace to the AVD Hostpool.\nMake a notice about the $hostpoolInfo.id variable. This is the variable from the hostpool deployment output.\n$diagnosticsBody¬†=¬†@{ Properties¬†=¬†@{ workspaceId¬†=¬†$Laws.id logs¬†=¬†@( @{ Category¬†=¬†\u0026#39;Error\u0026#39; Enabled¬†=¬†$true }, @{ Category¬†=¬†\u0026#39;Connection\u0026#39; Enabled¬†=¬†$true } ) } }¬†$diagnosticsUrl¬†=¬†\u0026#34;https://management.azure.com/\u0026#34;¬†+¬†$($hostpoolInfo.Id)¬†+¬†\u0026#34;/providers/microsoft.insights/diagnosticSettings/\u0026#34;¬†+¬†$LAWorkspace¬†+¬†\u0026#34;?api-version=2017-05-01-preview\u0026#34;¬†$diagnosticsParameters¬†=¬†@{ uri¬†=¬†$diagnosticsUrl Method¬†=¬†\u0026#34;PUT\u0026#34; Headers¬†=¬†$token Body¬†=¬†$diagnosticsBody¬†|¬†ConvertTo-Json¬†-Depth¬†4 } $diagnostics¬†=¬†Invoke-WebRequest¬†@diagnosticsParameters To keep an eye on our environment we need monitoring. For example I added the error and connection logging. These settings are the basics for a good monitoring environment. In addition to monitoring please check my post about enabling AVD monitoring automated. Further I recommend monitoring the AVD required URLs also. Check my post about how to achieve that goal.\nCreate an AVD session host with REST API At last we are deploying the session hosts into the AVD hostpool. In fact, a session host is nothing more than a domain joined virtual machine with an AVD agent installed. This is exactly what I‚Äôm going to do. First I repeat creating a NIC and a VM.\nNow I‚Äôm using the image gallery output as image reference.\nIn the first place I create the network interface. I use the $vmName variable as basics for all other resources.\n$vmName¬†=¬†\u0026#39;pinci-0\u0026#39; $nicName¬†=¬†\u0026#34;nic-\u0026#34;+$vmName $nicUrl¬†=¬†\u0026#34;https://management.azure.com/\u0026#34;¬†+¬†$ResourceGroupUrl¬†+¬†\u0026#34;/providers/Microsoft.Network/networkInterfaces/\u0026#34;¬†+¬†$nicName¬†+¬†\u0026#34;?api-version=2021-02-01\u0026#34; $nicBody¬†=¬†@{ location¬†=¬†$location properties¬†=¬†@{ ipConfigurations¬†=¬†@( @{ name¬†=¬†\u0026#34;ipconfig1\u0026#34; properties¬†=¬†@{ subnet¬†=¬†@{ id¬†=¬†$subnetId } } } ) } } $nicParameters¬†=¬†@{ uri¬†=¬†$nicUrl method¬†=¬†\u0026#39;PUT\u0026#39; header¬†=¬†$token body¬†=¬†$nicBody¬†|¬†ConvertTo-Json¬†-Depth¬†5 } $networkInterface¬†=¬†Invoke-WebRequest¬†@nicParameters $nicId¬†=¬†($networkInterface¬†|¬†ConvertFrom-Json).id Next, after creating the network card, I create the virtual machine.\n$imageInfo¬†=¬†($sharedImageGalleryImage¬†|¬†ConvertFrom-Json) $vmUrl¬†=¬†\u0026#34;https://management.azure.com/\u0026#34;¬†+¬†$ResourceGroupUrl¬†+¬†\u0026#34;/providers/Microsoft.Compute/virtualMachines/\u0026#34;¬†+¬†$vmName¬†+¬†\u0026#34;?api-version=2021-03-01\u0026#34; $sessionHostBody¬†=¬†@{ location¬†=¬†$location properties¬†=¬†@{ hardwareProfile¬†=¬†@{ vmSize¬†=¬†\u0026#34;Standard_B2ms\u0026#34; } networkProfile¬†=¬†@{ networkInterfaces¬†=¬†@( @{ id¬†=¬†$nicId properties¬†=¬†@{ primary¬†=¬†$true } } ) } storageProfile¬†=¬†@{ imageReference¬†=¬†@{ id¬†=¬†$imageInfo.id } osDisk¬†=¬†@{ caching¬†=¬†\u0026#34;ReadWrite\u0026#34; managedDisk¬†=¬†@{ storageAccountType¬†=¬†\u0026#34;Standard_LRS\u0026#34; } name¬†=¬†\u0026#34;os-\u0026#34;+$vmName createOption¬†=¬†\u0026#34;FromImage\u0026#34; } } osProfile¬†=¬†@{ adminUsername¬†=¬†\u0026#34;citrus-user\u0026#34; computerName¬†=¬†$vmName adminPassword¬†=¬†\u0026#34;VeryS3cretP@44W0rd!\u0026#34; } } } $sessionHostParameters¬†=¬†@{ uri¬†=¬†$vmUrl¬†method¬†=¬†\u0026#39;PUT\u0026#39; header¬†=¬†$token body¬†=¬†$sessionHostBody¬†|¬†ConvertTo-Json¬†-Depth¬†5 } $sessionHost¬†=¬†Invoke-WebRequest¬†@sessionHostParameters At last, if the virtual machine is deployed, it is time to install the domain join extension.\n$domain¬†=¬†\u0026#39;domain.local\u0026#39; $ouPath¬†=¬†\u0026#34;OU=Computers,OU=AVD,DC=domain,DC=local\u0026#34; $vmjoinerUser¬†=¬†\u0026#39;vmjoiner@domain.local\u0026#39; $securePassword¬†=¬†\u0026#39;verySecretPasswordforDomain@1\u0026#39; $domainJoinExtensionName¬†=¬†\u0026#34;JsonADDomainExtension\u0026#34; $domainJoinUrl¬†=¬†\u0026#34;https://management.azure.com/\u0026#34;¬†+¬†$ResourceGroupUrl¬†+¬†\u0026#34;/providers/Microsoft.Compute/virtualMachines/\u0026#34;¬†+¬†$vmName¬†+¬†\u0026#34;/extensions/\u0026#34;¬†+¬†$domainJoinExtensionName¬†+¬†\u0026#34;?api-version=2021-03-01\u0026#34; $domainJoinBody¬†=¬†@{ location¬†=¬†$location properties¬†=¬†@{ publisher¬†=¬†\u0026#34;Microsoft.Compute\u0026#34; type¬†=¬†\u0026#34;JsonADDomainExtension\u0026#34; typeHandlerVersion¬†=¬†\u0026#34;1.3\u0026#34; settings¬†=¬†@{ name¬†=¬†$domain ouPath¬†=¬†$ouPath user¬†=¬†$vmjoinerUser restart¬†=¬†$true options¬†=¬†\u0026#34;3\u0026#34; } protectedSettings¬†=¬†@{ password¬†=¬†$securePassword } } } $domainJoinParameters¬†=¬†@{ uri¬†=¬†$domainJoinUrl method¬†=¬†\u0026#39;PUT\u0026#39; header¬†=¬†$token body¬†=¬†$domainJoinBody¬†|¬†ConvertTo-Json¬†-Depth¬†5 } $domainJoin¬†=¬†Invoke-WebRequest¬†@domainJoinParameters Now we arrived at the final and last step of the AVD agent deployment. To get a session host into a AVD host pool we need to install the AVD dsc extension. Like the code above, this code will install an another extension. This extension installs the RDS agent which connects to the AVD host pool.\n$avdExtensionName¬†=¬†\u0026#34;dscextension\u0026#34; $artifactLocation¬†=¬†\u0026#34;https://wvdportalstorageblob.blob.core.windows.net/galleryartifacts/Configuration_01-19-2023.zip\u0026#34; $avdExtensionUrl¬†=¬†\u0026#34;https://management.azure.com/\u0026#34;¬†+¬†$ResourceGroupUrl¬†+¬†\u0026#34;/providers/Microsoft.Compute/virtualMachines/\u0026#34;¬†+¬†$vmName¬†+¬†\u0026#34;/extensions/\u0026#34;¬†+¬†$avdExtensionName¬†+¬†\u0026#34;?api-version=2021-03-01\u0026#34; $avdExtensionBody¬†=¬†@{ location¬†=¬†$location properties¬†=¬†@{ publisher¬†=¬†\u0026#34;Microsoft.Powershell\u0026#34; type¬†=¬†\u0026#34;DSC\u0026#34; typeHandlerVersion¬†=¬†\u0026#34;2.73\u0026#34; settings¬†=¬†@{ modulesUrl¬†=¬†$artifactLocation configurationFunction¬†=¬†\u0026#34;Configuration.ps1\\\\AddSessionHost\u0026#34; properties¬†=¬†@{ hostPoolName¬†=¬†$hostpoolInfo.name registrationInfoToken¬†=¬†$hostpoolInfo.properties.registrationInfo.token } } } } $avdExtensionParameters¬†=¬†@{ uri¬†=¬†$avdExtensionUrl method¬†=¬†\u0026#39;PUT\u0026#39; header¬†=¬†$token body¬†=¬†$avdExtensionBody¬†|¬†ConvertTo-Json¬†-Depth¬†5 } $avdExtension¬†=¬†Invoke-WebRequest¬†@avdExtensionParameters Now we are at the end of the AVD deployment automated with REST API. For all files please check my GitHub repository.\nConclusion In the first place I like to say that deploying with REST API is a great. It is fast and because of you don‚Äôt have to install PowerShell modules or write complex templates. As an illustration in the tasks above I have set some fixed values and some variables. In production environments you should use a JSON parameter file with all needs. At the start of the script you will read that file and use all the values.\nKnowledge about ARM templates is a pre before you begin with REST API. After all the REST API handles almost the same values as an ARM template.\nHowever there are some cons about using REST API. Yes a lot is possible, but you will have to build your own checks. An API call is just a trigger which pushes the first domino stone. In other words if the stone is pushed you will get an OK returned. This will not mean the task is accomplished (successful). In the examples above I use the PUT method. If you want to check the outcome and the status you need to request the status with the GET method.\nFrom where to start?\nMake sure you know how REST API is working. At first start with a simple request like creating a resource group. Next try to make a match with an ARM template. You will notice similarity between ARM and REST API.\nSecond is learn how to deal with PowerShell hashtables and arrays. Hashtables and arrays are converted to JSON and send as the body in the API call.\nThank you! Finally I would say, I hope you liked the Pineapple üçç Citrus üçä Splash üåä . In the hope that you got a bit inspired. Now you know deploy AVD automated with REST API, JSON and PowerShell is one of the options. If you like an another cocktail feel free to stay and check the AVD Automation Cocktail menu.\nThank you for reading my blog avd automation cocktail - azure virtual desktop automated with rest api. I hope you got a bit inspired. Enjoy your day and happy automating üëã ","date":"July 13, 2021","image":"http://localhost:1313/avd-automation-cocktail-avd-automated-with-rest-api/cocktail-pinapple-citrus.png","permalink":"/avd-automation-cocktail-avd-automated-with-rest-api/","title":"AVD Automation Cocktail - Azure Virtual Desktop automated with REST API"},{"categories":["AVD Cocktail","Azure Virtual Desktop","DevOps"],"contents":"Welcome to the AVD Automation Cocktail. In this cocktail series I will show different AVD deployment strategies and languages. In this cocktail, the Strawberry Banana Mix, I will show you how to deploy an AVD environment automated with DevOps, ARM templates and a bit of PowerShell.\nTable Of Contents Recipe Before to drink List DevOps of ingredients Aftertaste Introduction to Azure DevOps YAML Pipelines Pipeline variables DevOps ARM template output PowerShell YAML templates Referencing pipeline variables AVD automated with DevOps YAML and ARM Resource group Networking Shared Image Gallery Initial image version Create a virtual machine Sysprep Generalize Create image version Azure Virtual Desktop AVD Hostpool Application group Workspace Monitoring AVD Session hosts DevOps Library Groups Create session host Conclusion Thank you! Recipe In this ‚ÄúStrawberryBanana‚Äù-deployment recipe I will deploy an AVD environment automated with DevOps and ARM templates. I also using some PowerShell to glue some parts together.\nBefore to drink To start enrolling AVD automated with DevOps and ARM templates you will need to have a DevOps organization. In that organization, you need a project. In that project you will have to configure a service connection to an Azure tenant. Configuring the DevOps requirements is not the scope of this cocktail. For more info about configuring DevOps the automated way please check my series ‚Äòthe DevOps project‚Äô\nAlso, make sure you have an Active Directory domain present for the domain join.\nList DevOps of ingredients ARM templates YAML templates DevOps Libraries PowerShell Aftertaste This cocktail has an Azure DevOps in YAML base with ARM templates as main ingredient. Sometimes there is a little PowerShell taste. At the end you will have an AVD environment in Azure deployed automated with PowerShell with all the needed resources. These are a hostpool, a workspace, an application group. Also there are some session hosts. These hosts have an underlying image from a shared image gallery.\nIntroduction to Azure DevOps In contrast to the other cocktails I feel the need to write a little bit about the techniques I used. This because these techniques are very important to the whole automation sequence and differers from other cocktails.\nAs I mentioned in earlier cocktails, variables are key in automation. I‚Äôm using variables all the time which helps me avoid providing input all the time.\nIn coding languages like PowerShell, Azure CLI, bash, Python you are able to use variables. In Azure DevOps it is a bit different because Azure DevOps isn‚Äôt a programming language. But there are options, of course.\nYAML Pipelines Maybe you know there are two ways to create pipelines, classic and YAML. The classic release pipeline is a clickable page where you are able to create stages and jobs. In this article, I‚Äôm using the other option, YAML.\nOne if the reasons why using YAML is the ability to use output variables over different jobs. Another good reason is that YAML pipelines are saved in the current projects repository. This make the use of version, import and export possible. Yes, the learning curve is a bit steep but it is worth it.\nhttps://docs.microsoft.com/en-us/azure/devops/pipelines/process/variables?view=azure-devops\u0026amp;tabs=yaml%2Cbatch#use-output-variables-from-tasks\nPipeline variables DevOps provides the ability using jobs outcome as output variables. But it is not that simple. I challenged myself using the native methods instead of using PowerShell in Devops. Because I‚Äôm using ARM templates for deployment I use the ARM template deployment task.\nAdvantage of using this task is you just have to fill in parameters and don‚Äôt have to write PowerShell code for deployment. But no worries we still need PowerShell ;).\nDevOps ARM template output As you properly know ARM templates also have ARM parameter files. Within this parameter files you are able to store deployment parameters. From automation perspective you try to reuse parameters and use output for the next step as much as possible.\nWith that in mind you like to pick up an ARM deployment result. An example is a just created resource and its id. This id is needed in the next step.\nIn the ARM template deployment there is an advanced option called Deployment outputs. This is the point where it becomes interesting. Deployment outputs are returned in JSON format. Before you are able to use this output you first need to convert it from JSON into a pipeline variable. For more details about that point check this.\nPowerShell As the description says we need PowerShell to convert the output from JSON to something. If you want to use the output as variables in your pipeline you will need to send the output values to the pipeline.\nSending outputs to YAML pipeline variables with PowerShell can be done by the code below.\nWrite-Host \u0026#34;##vso[task.setvariable variable=variableName;isOutput=true]$variableValue\u0026#34; Make sure you will add the isOutput=true to the line. Otherwise there is no output.\nOn the internet several solutions where provided but that wasn‚Äôt enough for me. So I used one of the solutions as base and added some extra options. The main idea of all is that the output will be send to the PowerShell as object. The function will look for an output item with a name and a value. This is the output from an ARM template. All items within the object will be generated to a pipeline variable with the code above.\n$armOutputObj = $ArmOutputString | ConvertFrom-Json $armOutputObj.PSObject.Properties | ForEach-Object { $type = ($_.value.type).ToLower() $keyname = $($_.name) $value = $_.value.value switch ($type) { securestring { Write-Host \u0026#34;##vso[task.setvariable variable=$keyname;isOutput=true;issecret=true]$value\u0026#34; Write-Host \u0026#34;Added Azure DevOps secret variable \u0026#39;$keyname\u0026#39; (\u0026#39;$type\u0026#39;)\u0026#34; } string { Write-Host \u0026#34;##vso[task.setvariable variable=$keyname;isOutput=true]$value\u0026#34; Write-Host \u0026#34;Added Azure DevOps variable \u0026#39;$keyname\u0026#39; (\u0026#39;$type\u0026#39;) with value \u0026#39;$value\u0026#39;\u0026#34; } array { ($armOutputObj.PSObject.Properties | where { $_.Name -eq $keyname }).Value.value | ForEach-Object { Write-Host \u0026#34;##vso[task.setvariable variable=$_.name;isOutput=true]$_.value\u0026#34; Write-Host \u0026#34;Added Azure DevOps variable \u0026#39;$($_.name)\u0026#39; (\u0026#39;$type\u0026#39;) with value \u0026#39;$($_.value)\u0026#39;\u0026#34; } } default { Throw \u0026#34;Type \u0026#39;$type\u0026#39; is not supported for \u0026#39;$keyname\u0026#39;\u0026#34; } } } The whole function to convert ARM output variables in DevOps to pipeline variables can be found in my GitHub. The function also provides an option to insert an ARM array output which the other solutions don‚Äôt have.\nYAML templates An another automation principal is Don‚Äôt Repeat Yourself. That is the reason why we are using functions and templates. I knew I will need this PowerShell code from above very often in my automation sequence. With the principal in mind I created a YAML task template which I will use after every ARM deployment job.\nIn this template I copied the whole PowerShell code. I have chosen to paste it into the YAML file as a PowerShell task. The reason is that I don‚Äôt need to request an another file. Because of the amount of requests I like to speed up the pipeline a bit. The YAML task template asks for the ARMOutputString which is the output of the ARM deployment task, a prefix to make every pipeline variable unique and the task name. The task name is the reference in the steps later.\nThe template will requested in the master pipeline. I pasted a code block of one of the jobs. The job starts with a deployment task with some output. In this task I called the output ‚ÄòrgOutput‚Äô. This output is send as parameter into the template.\n# Deploy Resource Groups jobs: - job: DeployRG displayName: Deploying Resouregroup steps: - task: AzureResourceManagerTemplateDeployment@3 name: DeployResourceGroup displayName: \u0026#39;Deploy VNET Configuration including subnets\u0026#39; enabled: true inputs: deploymentScope: \u0026#39;Subscription\u0026#39; azureResourceManagerConnection: $(serviceConnection) subscriptionId: $(subscriptionId) location: $(location) templateLocation: \u0026#39;Linked artifact\u0026#39; csmFile: \u0026#39;$(Build.SourcesDirectory)/StrawberryBanana/Templates/ResourceGroups/deploy-resourcegroup.json\u0026#39; csmParametersFile: \u0026#39;$(Build.SourcesDirectory)/StrawberryBanana/Parameters/resourcegroup.parameters.json\u0026#39; deploymentMode: \u0026#39;Incremental\u0026#39; deploymentOutputs: \u0026#39;rgOutput\u0026#39; - template: \u0026#39;StrawberryBanana/DevOps Pipelines/Templates/create-pipelinevariables.yml@Rozemuller\u0026#39; parameters: taskName: \u0026#39;rgDeploy\u0026#39; ArmOutputString: \u0026#39;$(rgOutput)\u0026#39; variablePrefix: rgDeploy If your template is in an another repository then your pipeline you will need to add that repository into your pipeline. Because the master pipeline must be able to run in every project I added the repository in the master pipeline by default.\nBy using templates I reduce the code in the master pipeline and I don‚Äôt repeat the ConvertFrom-Json code every time.\nMore information about YAML templates check the Microsoft docs\nReferencing pipeline variables Finally we have output send into pipeline variables. Now it is time to put the parts together and use them in our pipelines.\nLet‚Äôs start with the code. In the next job I need the output of the resourceGroup deployment.\ndependsOn: DeployRG variables: resourceGroupName: $[ \u0026lt;em\u0026gt;dependencies.DeployRG.outputs\u0026lt;/em\u0026gt;[\u0026lt;em\u0026gt;\u0026#39;rgDeploy.rgName\u0026#39;\u0026lt;/em\u0026gt;] ] To get outputs from previous jobs we need three pointers. The job name (DeployRG), the task name and the output variable name (rgName).\nThe code shows us we have to deal with dependencies. This means we have need to set dependsOn with the jobs we are relying on. This are the job names. Make sure you have set every job you are depending on otherwise the variable will stay empty. However the variable is empty there is no error.\nThe second part is the task name. That is the reason I configured the template accepting task names. Now I don‚Äôt need to know how the tasks is called inside the template. The last part is the output variable name. This is the name of the output in the ARM template.\nThe convert to JSON template helps you a little bit. Below the output of that task.\nAVD automated with DevOps YAML and ARM In this chapter I will explain how to deploy an AVD environment automated with Azure DevOps in YAML with ARM templates. In some cases I‚Äôm also using PowerShell to convert ARM template outputs into pipeline variables. These variables are used later in the pipeline. I will recommend to clone my AVD GitHub repository to get all the needed files, also for the other cocktails.\nFor every purpose, in this case create an AVD environment with a SIG in basics.\nResource group The first step in our deployment is creating a new resource group. In this group I will deploy all the resources in this blog post. When starting a pipeline search for the ARM deployment task on the right side of the screen.\nFor the first task, I chose the subscription deployment scope. This is because I‚Äôm deploying a new resource group at a subscription. In the next tasks I choose the resource group deployment scope. This is because I‚Äôm updating a resource group.\n# Deploy Resource Groups jobs: - job: DeployRG displayName: Deploying Resouregroup steps: - task: AzureResourceManagerTemplateDeployment@3 name: DeployResourceGroup displayName: \u0026#39;Deploy Resouregroup Configuration in $(location)\u0026#39; enabled: true inputs: deploymentScope: \u0026#39;Subscription\u0026#39; azureResourceManagerConnection: $(serviceConnection) subscriptionId: $(subscriptionId) location: $(location) templateLocation: \u0026#39;Linked artifact\u0026#39; csmFile: \u0026#39;$(Build.SourcesDirectory)/StrawberryBanana/Templates/ResourceGroups/deploy-resourcegroup.json\u0026#39; csmParametersFile: \u0026#39;$(Build.SourcesDirectory)/StrawberryBanana/Parameters/resourcegroup.parameters.json\u0026#39; deploymentMode: \u0026#39;Incremental\u0026#39; deploymentOutputs: \u0026#39;rgOutput\u0026#39; - template: \u0026#39;StrawberryBanana/DevOps Pipelines/Templates/create-pipelinevariables.yml@Rozemuller\u0026#39; parameters: taskName: \u0026#39;rgDeploy\u0026#39; ArmOutputString: \u0026#39;$(rgOutput)\u0026#39; variablePrefix: rgDeploy The task has some default inputs like the template file and the parameter file. Make a notice about the ‚ÄòdeploymentsOutputs‚Äô name ‚ÄòrgOutput‚Äô. This is the input for the pipeline variable template.\nTo reduce the length of this post for the next steps I only will paste the differences like the files input or deploymentOutputs.\nNetworking The second step of our ‚ÄúStrawberryBanana‚Äù-deployment is deploying a VNET with a custom DNS (for our domain join), two subnets and a network security group. A network security group, or NSG, will help us protecting our network for unwanted traffic.\nFirst I will deploy the NSG with the code below. As you can see the resource group output is used here as variable. To make this work make sure you have added the needed job in the dependsOn part.\n# Deploy Network Security Group - job: DeployNSG dependsOn: DeployRG displayName: Deploying Network Security Group variables: resourceGroupName: $[ dependencies.DeployRG.outputs[\u0026#39;rgDeploy.rgName\u0026#39;] ] In the task within the job I changed the scope to resource group and using the resource group variable.\n- task: AzureResourceManagerTemplateDeployment@3 name: DeployNSG displayName: \u0026#39;Deploy Network Security Group\u0026#39; enabled: true inputs: deploymentScope: \u0026#39;Resource Group\u0026#39; resourceGroupName: $(resourceGroupName) csmFile: \u0026#39;$(Build.SourcesDirectory)/StrawberryBanana/Templates/Network/deploy-nsg.json\u0026#39; csmParametersFile: \u0026#39;$(Build.SourcesDirectory)/StrawberryBanana/Parameters/network-nsg.parameters.json\u0026#39; deploymentOutputs: \u0026#39;nsgOutput\u0026#39; After the NSG and the subnet config deployment we are creating the virtual network. To create a network I stored all the unknown values in the parameter file. I also stored a DNS server into the parameter file. In this task I added the DeployNSG as dependency. Good to know is that you have to declare variables at every job.\n- job: DeployVnet dependsOn: - DeployNSG - DeployRG displayName: Deploying Network and Subnets variables: resourceGroupName: $[ dependencies.DeployRG.outputs[\u0026#39;rgDeploy.rgName\u0026#39;] ] nsgId: $[ dependencies.DeployNSG.outputs[\u0026#39;nsgDeploy.nsgId\u0026#39;] ] The nsgId variable is used as parameter value in the ARM deployment. Overriding parameters can be done with the overrideParameter option.\n- task: AzureResourceManagerTemplateDeployment@3 name: DeployNSG displayName: \u0026#39;Deploy VNET Configuration including subnets\u0026#39; inputs: resourceGroupName: $(resourceGroupName) csmFile: \u0026#39;$(Build.SourcesDirectory)/StrawberryBanana/Templates/Network/deploy-network.json\u0026#39; csmParametersFile: \u0026#39;$(Build.SourcesDirectory)/StrawberryBanana/Parameters/network-with-subnet.parameters.json\u0026#39; overrideParameters: \u0026#39;-nsgId $(nsgId)\u0026#39; deploymentMode: \u0026#39;Incremental\u0026#39; deploymentOutputs: \u0026#39;vnetOutput\u0026#39; At the end the virtual network is deployed with the correct settings.\nShared Image Gallery The next part is creating a shared image gallery. In this gallery we are creating image versions. We going to use these versions to start AVD sessions host later in this article.\nFirst, we are creating the gallery itself and the image definition with the task below.\n$galleryParameters¬†=¬†@{ GalleryName¬†=¬†\u0026#34;CoconutBeachGallery\u0026#34; # Deploy Shared Image Gallery Configuration - job: DeploySig dependsOn: - DeployRG displayName: Deploying Shared Image Gallery variables: resourceGroupName: $[ dependencies.DeployRG.outputs[\u0026#39;rgDeploy.rgName\u0026#39;] ] steps: - task: AzureResourceManagerTemplateDeployment@3 name: DeploySIG displayName: \u0026#39;Deploy SIG Configuration including definition\u0026#39; inputs: csmFile: \u0026#39;$(Build.SourcesDirectory)/StrawberryBanana/Templates/SharedImageGallery/deploy-sig.json\u0026#39; csmParametersFile: \u0026#39;$(Build.SourcesDirectory)/StrawberryBanana/Parameters/shared-image-gallery.parameters.json\u0026#39; deploymentOutputs: \u0026#39;sigOutput\u0026#39; Initial image version It is time to create an initial image version. This version is the base of our AVD environment. Later on we are able to create new versions based on this one.\nCreate a virtual machine The first step is creating a new Windows 10 Multi Session virtual machine. The VM has a custom OS disk size of 128 GB. Make a notice of the G2 image SKU in the parameter file. The G2 is the HyperV generation. This must be the same generation as the image definition HyperV generation.\n- job: DeployVm dependsOn: - DeployRG - DeployVnet - DeployNSG displayName: Deploying intial VM variables: resourceGroupName: $[ dependencies.DeployRG.outputs[\u0026#39;rgDeploy.rgName\u0026#39;] ] nsgId: $[ dependencies.DeployNSG.outputs[\u0026#39;nsgDeploy.nsgId\u0026#39;] ] virtualNetworkName: $[ dependencies.DeployVnet.outputs[\u0026#39;vnetDeploy.vnetName\u0026#39;] ] I use the VM deployment output for the VM details and using the vmName to connect to in the next steps.\nSysprep Executing some code isn‚Äôt an Azure deployment. Because of ARM can only be used to deploy resources we need to grab back to PowerShell. In the next few tasks we are executing PowerShell scripts via DevOps. In this first task I‚Äôm sysprepping the virtual machine.\n- job: PrepareVM dependsOn: - DeployVM displayName: Preparing VM for sysprep and generalizing variables: vmName: $[ dependencies.DeployVM.outputs[\u0026#39;vmDeploy.virtualMachineName\u0026#39;] ] steps: - task: AzurePowerShell@5 name: Execute_Sysprep inputs: ScriptType: \u0026#39;InlineScript\u0026#39; Inline: | $vm = Get-AzVM -Name $(vmName) $vm | Invoke-AzVMRunCommand -CommandId \u0026#34;RunPowerShellScript\u0026#34; -ScriptPath \u0026#39;$(Build.SourcesDirectory)/StrawberryBanana/PowerShell/sysprep.ps1\u0026#39; -Wait azurePowerShellVersion: \u0026#39;LatestVersion\u0026#39; pwsh: true Using the -Wait command will keep the task running till the Sysprep is finished. The virtual machine is stopped after the task is done.\nGeneralize Generalizing a virtual machine is the last step before we are able to create an image version. Generalizing a machine is done by one simple command. This command is also executed within a PowerShell task. This task is a part of the prepareVm job.\n-¬†task:¬†AzurePowerShell@5 name:¬†Execute_Generalize displayName:¬†Generalizing¬†$(vmName) inputs: ScriptType:¬†\u0026#39;InlineScript\u0026#39; Inline:¬†| $vm¬†=¬†Get-AzVM¬†-Name¬†$(vmName) $vm¬†|¬†Set-AzVm¬†-Generalized azurePowerShellVersion:¬†\u0026#39;LatestVersion\u0026#39; pwsh:¬†true Create image version The last step is creating a new image version into the gallery. As you can see I‚Äôm using information which I already know and has been stored into variables in the earlier steps. After the image has been deployed the next step is create AVD session hosts from this image.\nI created a new job with a separate ARM template file for just creating an image version. This because I will need this template more often then only in the initial enrollment.\n‚ÄúKeeping templates as small as possible will help you troubleshooting deployment issues.‚Äú\nThe PrepareSigVersion jobs had some dependencies from out of the whole automation sequence. Beside the resource group I need the Shared Image Gallery info, the virtual machine info and I have to wait till the VM is prepared.\nI use some date formatting to generate a version number. A version number must have a SemVer notition. To create a job variable I use the Get-Date option as input to the PowerShell task. The PowerShell task generates a version number.\nWhen the version numbers has been created this value will inserted into the version deployment job.\n#¬†Creating¬†new¬†SIG¬†Version -¬†job:¬†DeploySigVersion dependsOn:¬†-¬†DeployRG -¬†DeployVM -¬†DeploySig -¬†PrepareVM displayName:¬†Preparing¬†image¬†version variables:¬†resourceGroupName:¬†$[¬†dependencies.DeployRG.outputs[\u0026#39;rgDeploy.rgName\u0026#39;]¬†] vmId:¬†$[¬†dependencies.DeployVM.outputs[\u0026#39;vmDeploy.resourceId\u0026#39;]¬†] major:¬†$(Get-Date¬†-format¬†yyyy)¬†minor:¬†$(Get-Date¬†-format¬†MMdd)¬†patch:¬†$[counter(variables[\u0026#39;minor\u0026#39;],¬†0)] galleryName:¬†$[¬†dependencies.DeploySig.outputs[\u0026#39;sigDeploy.galleryName\u0026#39;]¬†] galleryImageDefinitionName:¬†$[¬†dependencies.DeploySig.outputs[\u0026#39;sigDeploy.galleryImageDefinitionName\u0026#39;]¬†] steps:¬†-¬†powershell:¬†| write-host¬†\u0026#34;##vso[task.setvariable¬†variable=galleryImageVersionName;isOutput=true]$(major).$(minor).$(patch)\u0026#34; name:¬†CreateImageVersionName -¬†bash:¬†echo¬†\u0026#34;the¬†version¬†value¬†is¬†$(CreateImageVersionName.galleryImageVersionName)\u0026#34;¬†-¬†task:¬†AzureResourceManagerTemplateDeployment@3 name:¬†DeployVersion displayName:¬†\u0026#39;Deploy¬†Initial¬†Version\u0026#39; enabled:¬†true inputs: deploymentScope:¬†\u0026#39;Resource¬†Group\u0026#39; azureResourceManagerConnection:¬†$(serviceConnection) subscriptionId:¬†$(subscriptionId) action:¬†\u0026#39;Create¬†Or¬†Update¬†Resource¬†Group\u0026#39; resourceGroupName:¬†$(resourceGroupName) location:¬†$(location) templateLocation:¬†\u0026#39;Linked¬†artifact\u0026#39; csmFile:¬†\u0026#39;$(Build.SourcesDirectory)/StrawberryBanana/Templates/SharedImageGallery/deploy-sig-version.json\u0026#39; overrideParameters:¬†\u0026#39;-sourceId¬†$(vmId)¬†-galleryName¬†$(galleryName)¬†-galleryImageDefinitionName¬†$(galleryImageDefinitionName)¬†-galleryImageVersionName¬†$(CreateImageVersionName.galleryImageVersionName)\u0026#39; deploymentMode:¬†\u0026#39;Incremental\u0026#39; deploymentOutputs:¬†\u0026#39;versionOutput\u0026#39; -¬†template:¬†\u0026#39;StrawberryBanana/DevOps¬†Pipelines/Templates/create-pipelinevariables.yml@Rozemuller\u0026#39; parameters: taskName:¬†\u0026#39;versionDeploy\u0026#39; ArmOutputString:¬†\u0026#39;$(versionOutput)\u0026#39; variablePrefix:¬†versionDeploy¬†Azure Virtual Desktop After the backend is prepared, it is time to deploy the AVD environment. In the cocktails where templates and parameter files are used I recommend creating single template files. These files are used for repeatable actions like deploying session host. This will make life easier and you code a lot more cleaner.\nAVD Hostpool The first step in deploying an AVD enviroment automated with DevOps and ARM templates is creating an AVD host pool. In this hostpool I also configuring the new StartVMOnConnect option. This option allows you to shutdown session hosts at the end of the day. When the first account is connecting the host will start automatically.\nDuring the pipeline run I create a new date which is used to create a new token for the next few hours. In the first step of the job the PowerShell task is creating a new date format. This date format will override the ‚ÄòtokenExpirationTime‚Äô in the template. The template will return a new AVD hostpool token through the ARM template output.\nApplication group The application group is the place where to assign users/groups to the AVD environment. I created a separate template file for that. The only parameter needed is the host pool name. This will be provided in the template. Because I‚Äôm still in the same job I don‚Äôt need to add a dependency for the variables. Referring to the task in combination with the output variable name is good.\nThe .hostpoolName is the output from the ARM template.\nWorkspace The AVD workspace is the frontend of the AVD enviroment. This is the place where people subscribing to. Deploying the workspace is the next step. I also created a separate template for the workspace deployment.\nI‚Äôm also referring to the host pool output. Based on that output I created a workspace name.\n-¬†task:¬†AzureResourceManagerTemplateDeployment@3 name:¬†DeployAVDWorkspace displayName:¬†\u0026#39;Deploy¬†AVD¬†Workspace\u0026#39; enabled:¬†true inputs: deploymentScope:¬†\u0026#39;Resource¬†Group\u0026#39; azureResourceManagerConnection:¬†$(serviceConnection) subscriptionId:¬†$(subscriptionId) action:¬†\u0026#39;Create¬†Or¬†Update¬†Resource¬†Group\u0026#39; resourceGroupName:¬†$(resourceGroupName) location:¬†$(location) templateLocation:¬†\u0026#39;Linked¬†artifact\u0026#39; csmFile:¬†\u0026#39;$(Build.SourcesDirectory)/StrawberryBanana/Templates/AVD/deploy-avd-workspace.json\u0026#39; overrideParameters:¬†\u0026#39;-hostpoolName¬†$(hostpoolDeploy.hostpoolName)¬†-workSpaceName¬†$(hostpoolDeploy.hostpoolName)-Workspace\u0026#39; deploymentMode:¬†\u0026#39;Incremental\u0026#39; deploymentOutputs:¬†\u0026#39;avdWorkspaceOutput\u0026#39; Make sure, after deployment, you will assign the workspace to the correct users or groups.\nMonitoring The next step in our sequence is the monitoring part. In this part we are going to install a Log Analytics Workspace and will enable diagnostic settings on the host pool.\nFor monitoring I created a template file as well with a parameter file. Additional I added the host pool name.\n-¬†task:¬†AzureResourceManagerTemplateDeployment@3 name:¬†DeployAVDDiagnostics displayName:¬†\u0026#39;Deploy¬†AVD¬†Diagnostics\u0026#39; enabled:¬†true inputs: deploymentScope:¬†\u0026#39;Resource¬†Group\u0026#39; azureResourceManagerConnection:¬†$(serviceConnection) subscriptionId:¬†$(subscriptionId) action:¬†\u0026#39;Create¬†Or¬†Update¬†Resource¬†Group\u0026#39; resourceGroupName:¬†$(resourceGroupName) location:¬†$(location) templateLocation:¬†\u0026#39;Linked¬†artifact\u0026#39; csmFile:¬†\u0026#39;$(Build.SourcesDirectory)/StrawberryBanana/Templates/AVD/deploy-avd-diagnostics.json\u0026#39; csmParametersFile:¬†\u0026#39;$(Build.SourcesDirectory)/StrawberryBanana/Parameters/avd-diagnostics.parameters.json\u0026#39; overrideParameters:¬†\u0026#39;-hostpoolName¬†$(hostpoolDeploy.hostpoolName)\u0026#39; Enabling these settings is the basics for a good monitoring environment. If you like to configure advanced AVD monitoring automated please check my post about enabling AVD monitoring automated. I will recommend monitoring the AVD required URLs also. Check my post about how to achieve that goal.\nAVD Session hosts The last step is deploying the session hosts into the AVD hostpool. Before deploying a session host I decided to deploy an Azure Key Vault first. Into the key vault I will store the administrator password for the domain join. In the later steps I will reference to this key vault secret in the template.\nDevOps Library Groups In this step, I will create a DevOps Libray group. In this group, I will set the domain password for the domain join. Because the password is a secure value I don‚Äôt want this value plain in a parameter file.\nTo take advantage of the library you need to add this variable group to the pipeline variables.\nvariables: -¬†group:¬†StrawberryVariables Later in the pipeline you can request the variable like all others.\nMore information about using libraries check the url: https://docs.microsoft.com/en-us/azure/devops/pipelines/library/variable-groups?view=azure-devops\u0026amp;tabs=yaml\nCreate session host Finally it is time to create our first AVD session host with DevOps and ARM. During the host pool deployment I created a hostpool registration. This token will be use to add the session host into the correct hostpool.\nAll the static and non secure parameters are stored in the parameter file. I started the job with all of its dependencies and the new variables.\n-¬†job:¬†DeployAVDSessionHost dependsOn:¬†-¬†DeployRG -¬†DeploySig -¬†DeploySigVersion -¬†DeployAVDEnvironment -¬†DeployVnet displayName:¬†Deploying¬†First¬†AVD¬†Session¬†Host variables:¬†resourceGroupName:¬†$[¬†dependencies.DeployRG.outputs[\u0026#39;rgDeploy.rgName\u0026#39;]¬†] hostpoolToken:¬†$[¬†dependencies.DeployAVDEnvironment.outputs[\u0026#39;hostpoolDeploy.hostpoolToken\u0026#39;]¬†] domainPassword:¬†$(vmjoinerPassword) In the deployment part the variables will override the template parameters.\ntemplateLocation:¬†\u0026#39;Linked¬†artifact\u0026#39; csmFile:¬†\u0026#39;$(Build.SourcesDirectory)/StrawberryBanana/Templates/AVD/deploy-avd-sessionhosts.json\u0026#39; csmParametersFile:¬†\u0026#39;$(Build.SourcesDirectory)/StrawberryBanana/Parameters/avd-sessionhost.parameters.json\u0026#39; overrideParameters:¬†\u0026#39;-administratorAccountPassword¬†$(domainPassword)¬†-hostpoolToken¬†$(hostpoolToken)\u0026#39; This is the end of the AVD deployment automated with Azure DevOps and ARM templates. For all the YAML files, ARM templates and parameter files, please check my GitHub repository.\nConclusion DevOps is a great way for automating tasks. This in combination with the use of ARM templates makes it a really good combination.\nTo be honest, learning DevOps and/or ARM from the beginning is pretty hard. The default ARM templates are doing their job but if you want to stitch the templates into one automation sequence there is a lot of trial and error. But at the end it is definitely worth it.\nWhere to start?\nMake sure you will understand how YAML pipelines are working. How to use YAML templates and work jobs and dependencies. If you know the pipeline architecture it is just a matter of adding the ARM deployment jobs with the correct parameters.\nCheck the YAML schema over here: https://docs.microsoft.com/en-us/azure/devops/pipelines/yaml-schema?view=azure-devops\u0026amp;tabs=schema%2Cparameter-schema\nThe next thing which could help you is using YAML templates but this is the next step. Using templates will keep you master pipeline more readable.\nCheck how to make use of templates over here: https://docs.microsoft.com/en-us/azure/devops/pipelines/process/templates?view=azure-devops\nThe last thing are the ARM templates. In the first place make a start with static parameters in the parameter files and make sure all of your resources are deployed. If your deployment is successful the next step is using job output variables.\nHow to use output variables please check this link: https://docs.microsoft.com/en-us/azure/devops/pipelines/process/variables?view=azure-devops\u0026amp;tabs=yaml%2Cbatch\nThank you! I hope you liked the Strawberry Banana Mix üçì üçå and got a bit inspired. Now you know deploy AVD automated with DevOps and ARM templates is one of the options. If you like another cocktail feel free to stay and check the AVD Automation Cocktail menu.\nThank you for reading my blog avd automation cocktail - azure virtual desktop automated with devops and arm. I hope you got a bit inspired. Enjoy your day and happy automating üëã ","date":"July 7, 2021","image":"http://localhost:1313/avd-automation-cocktail-avd-automated-with-devops-and-arm/cocktail-strawberry-banana.png","permalink":"/avd-automation-cocktail-avd-automated-with-devops-and-arm/","title":"AVD Automation Cocktail - Azure Virtual Desktop automated with DevOps and ARM"},{"categories":["AVD Cocktail","Azure Virtual Desktop"],"contents":"Welcome to the AVD Automation Cocktail. In this cocktail series, I will show different AVD deployment strategies and languages. In this cocktail, the Coconut Beach Party, I will show you how to deploy an AVD environment automated with PowerShell only.\nTable Of Contents Recipe Before to drink List PowerShell of ingredients Aftertaste AVD automated with PowerShell only Resource group Networking Shared Image Gallery Initial image version Create a virtual machine Sysprep Generalize Create image version Azure Virtual Desktop AVD Hostpool Start VM on Connect Application group Workspace AVD Session hosts Azure Key vault Create session host Install AVD extensions with PowerShell Monitoring Conclusion Thank you! Recipe In this ‚ÄúCoconutBeach‚Äù-deployment recipe I will deploy an AVD environment automated with PowerShell only. PowerShell is commonly used for automating the management of systems.\nBefore to drink To start enrolling AVD automated with PowerShell you will need to install PowerShell on your system.\nhttps://docs.microsoft.com/en-us/powershell/scripting/install/installing-powershell?view=powershell-7.1\nIn automation variables are key. In the code snippets below you will notice I use variables in combination with hash tables a lot. After creating a hash table I will use the splatting technique. For me the most important reason of splatting parameters is it will keep my code short and clean.\nList PowerShell of ingredients To make PowerShell working without issues you will need to some PowerShell modules. For installing and importing modules use the code below. Change the module name with the modules in the list. At the start of every chapter I wrote down which module is needed to run the automation.\nInstall-Module Az.Network Import-Module Az.Network Az.Network Az.Resources Az.Compute Az.Avd Aftertaste This cocktail has a strong PowerShell taste with a coconut aftertaste. In the beginning the cocktail the cocktail starts a bit light when the taste gets stronger. At the end you will have an AVD environment in Azure deployed automated with PowerShell with all the needed resources. These are a hostpool, a workspace, an application group. Also there are some session hosts. These hosts have an underlying image from a shared image gallery.\nAVD automated with PowerShell only In this chapter I will explain how to deploy an AVD environment automated with PowerShell. If you are not that good in ARM, CLI or Bicep this cocktail can help you automating I will recommend to clone my AVD GitHub repository to get all the needed files, also for the other cocktails.\nFor every purpose, in this case create an AVD environment with a SIG in basics.\nResource group The first step in our deployment is creating a new resource group. In this group I will deploy all the resources in this blog post. Needed module: Az.Resources\n$resourceGroupName = \u0026#34;RG-ROZ-COCONUTBEACH-COCKTAIL\u0026#34; $location = \u0026#34;WestEurope\u0026#34; $parameters = @{ ResourceGroup = $resourceGroupName Location = $location } New-AzResourceGroup @parameters Networking The second step of our ‚ÄúCoconutBeach‚Äù-deployment is deploying a VNET with a custom DNS (for our domain join), two subnets and a network security group. A network security group, or NSG, will help us protecting our network for unwanted traffic.\nNeeded module: Az.Network\nFirst I will deploy the NSG with the code below. The deployment will be stored into a variable. If you look into the variable you will get the deployment result output like a resourceId. Storing output in variables is very useful in for deployments further in the script.\n$nsgParameters¬†=¬†@{ ResourceGroupName¬†=¬†$resourceGroupName¬†Location¬†=¬†$location¬†Name¬†=¬†\u0026#34;nsg-coconut\u0026#34; } $networkSecurityGroup¬†=¬†New-AzNetworkSecurityGroup¬†@nsgParameters The next step is configuring the subnets and assigning them to the recently created NSG. I created a hashtable with all the needed subnets and the NSG variable. It looks like a bit overkill creating an for each loop for two subnets but it is just a way of working I learned myself. Working with arrays will help you avoiding executing the same commands over and over.\n$subnetParameters¬†=¬†@{ defaultSubnet¬†=¬†\u0026#34;10.0.1.0/24\u0026#34; avdSubnet¬†=¬†\u0026#34;10.0.2.0/24\u0026#34; NetworkSecurityGroup¬†=¬†$networkSecurityGroup } $subnets¬†=¬†$subnetParameters.GetEnumerator().ForEach({ New-AzVirtualNetworkSubnetConfig¬†-Name¬†$_.Name¬†-AddressPrefix¬†$_.Value¬†-NetworkSecurityGroup¬†$networkSecurityGroup }) After the NSG and the subnet config deployment we are creating the virtual network. To create a network I stored the subnets variable into the vnet parameters. I also stored a DNS server into the config. The 10.3.1.4 is my domain controller and DNS server.\n$vnetParameters¬†=¬†@{ name¬†=¬†\u0026#34;vnet-coconutbeach\u0026#34; ResourceGroupName¬†=¬†$resourceGroupName Location¬†=¬†$location AddressPrefix¬†=¬†\u0026#34;10.0.0.0/16\u0026#34;¬†Subnet¬†=¬†$subnets DnsServer¬†=¬†\u0026#34;10.3.1.4\u0026#34; } $virtualNetwork¬†=¬†New-AzVirtualNetwork¬†@vnetParameters¬†Subnet parameters stored in the vnet configuration At the end the virtual network is deployed with the correct settings.\nShared Image Gallery The next part is creating a shared image gallery. In this gallery we are creating image versions. We going to use these versions to start AVD sessions host later in this article.\nNeeded module: Az.Compute\nFirst, we are creating the gallery itself with the command below.\n$galleryParameters¬†=¬†@{ GalleryName¬†=¬†\u0026#34;CoconutBeachGallery\u0026#34; ResourceGroupName¬†=¬†$resourceGroupName Location¬†=¬†$location Description¬†=¬†\u0026#34;Shared¬†Image¬†Gallery¬†for¬†my¬†beach¬†party\u0026#34; } $gallery¬†=¬†New-AzGallery¬†@galleryParameters The next step is assigning an Azure AD group to the gallery with contributor permissions.\nBecause I stored the gallery deployment into a variable, I‚Äôm now able to use that deployment in the next one. First I will create a new Azure AD Group called Gallery Contributor.\n$GalleryContributor¬†=¬†New-AzAdGroup¬†-DisplayName¬†\u0026#34;Gallery¬†Contributor\u0026#34;¬†-MailNickname¬†\u0026#34;GalleryContributor\u0026#34;¬†-Description¬†\u0026#34;This¬†group¬†had¬†shared¬†image¬†gallery¬†contributor¬†permissions\u0026#34; $galleryRoleParameters¬†=¬†@{ ObjectId¬†=¬†$GalleryContributor.Id RoleDefinitionName¬†=¬†\u0026#34;contributor\u0026#34; ResourceName¬†=¬†$gallery.Name ResourceType¬†=¬†\u0026#34;Microsoft.Compute/galleries\u0026#34;¬†ResourceGroupName¬†=¬†$gallery.ResourceGroupName } New-AzRoleAssignment¬†@galleryRoleParameters A part of a gallery is the image gallery definition. This is a logical group of images with the same properties like os-type, system state (generalized, specialized) and VM generation.\nI‚Äôm creating a sysprepped Windows installation on a V2 generation. Again I‚Äôm able to reuse deployments from an earlier stadium.\n$imageDefinitionParameters¬†=¬†@{ GalleryName¬†=¬†$gallery.Name ResourceGroupName¬†=¬†$gallery.ResourceGroupName Location¬†=¬†$gallery.Location Name¬†=¬†\u0026#34;CoconutDefinition\u0026#34; OsState¬†=¬†\u0026#34;Generalized\u0026#34; OsType¬†=¬†\u0026#34;Windows\u0026#34; Publisher¬†=¬†\u0026#34;Coconut\u0026#34; Offer¬†=¬†\u0026#34;Beach\u0026#34; Sku¬†=¬†\u0026#34;Party\u0026#34; HyperVGeneration=¬†\u0026#34;V2\u0026#34; } $imageDefinition¬†=¬†New-AzGalleryImageDefinition¬†@imageDefinitionParameters Initial image version It is time to create an initial image version. This version is the base of our AVD enviroment. Later on we are able to create new versions based on this one.\nNeeded module: Az.Compute, Az.Network\nCreate a virtual machine The first step is creating a new Windows 10 Multi Session virtual machine. The VM has a custom OS disk size of 512 GB. Make a notice of the G2 image SKU. The G2 is the HyperV generation. This must be the same generation as the image definition HyperV generation.\n$VMLocalAdminUser¬†=¬†\u0026#34;LocalAdminUser\u0026#34; $VMLocalPassword¬†=¬†\u0026#34;V3rySecretP@ssw0rd\u0026#34; $VMLocalAdminSecurePassword¬†=¬†ConvertTo-SecureString¬†$VMLocalPassword¬†-AsPlainText¬†-Force $VMName¬†=¬†\u0026#34;vm-coconut\u0026#34; $VMSize¬†=¬†\u0026#34;Standard_D2s_v3\u0026#34; $ImageSku¬†=¬†\u0026#34;21h1-evd-g2\u0026#34; $ImageOffer¬†=¬†\u0026#34;Windows-10\u0026#34; $ImagePublisher¬†=¬†\u0026#34;MicrosoftWindowsDesktop\u0026#34; $ComputerName¬†=¬†$VMName $DiskSizeGB¬†=¬†512 $nicName¬†=¬†\u0026#34;nic-$vmName\u0026#34; $NIC¬†=¬†New-AzNetworkInterface¬†-Name¬†$NICName¬†-ResourceGroupName¬†$ResourceGroupName¬†-Location¬†$location¬†-SubnetId¬†($virtualNetwork.Subnets¬†|¬†Where¬†{$_.Name¬†-eq¬†\u0026#34;avdSubnet\u0026#34;}).Id $Credential¬†=¬†New-Object¬†System.Management.Automation.PSCredential¬†($VMLocalAdminUser,¬†$VMLocalAdminSecurePassword); $VirtualMachine¬†=¬†New-AzVMConfig¬†-VMName¬†$VMName¬†-VMSize¬†$VMSize $VirtualMachine¬†=¬†Set-AzVMOperatingSystem¬†-VM¬†$VirtualMachine¬†-Windows¬†-ComputerName¬†$ComputerName¬†-Credential¬†$Credential¬†-ProvisionVMAgent¬†-EnableAutoUpdate $VirtualMachine¬†=¬†Add-AzVMNetworkInterface¬†-VM¬†$VirtualMachine¬†-Id¬†$NIC.Id $VirtualMachine¬†=¬†Set-AzVMOSDisk¬†-Windows¬†-VM¬†$VirtualMachine¬†-CreateOption¬†FromImage¬†-DiskSizeInGB¬†$DiskSizeGB $VirtualMachine¬†=¬†Set-AzVMSourceImage¬†-VM¬†$VirtualMachine¬†-PublisherName¬†$ImagePublisher¬†-Offer¬†$ImageOffer¬†-Skus¬†$ImageSku¬†-Version¬†latest $initialVM = New-AzVM¬†-ResourceGroupName¬†$ResourceGroupName¬†-Location¬†$Location¬†-VM¬†$VirtualMachine Sysprep First, the VM must be generalized and Sysprepped. We are executing the sysprep command with the Invoke-AzRunCommand PowerShell command. This command allows you to run commands on Azure VMs remotely.\nNeeded module: Az.Compute\nIn comparison with Azure CLI PowerShell needs a script input where Azure CLI can handle commands as well. Because of that point I will create a script within my automation sequence. The script is stored at the current working directory.\nOf course you are able to store it where you like. Make a notice about the ‚Äú`‚Äù before the variables. Because $ is a special character we need to use escape characters. Otherwise PowerShell will acts the value as a variable.\nAnother ‚Äòtrick‚Äô is the -Wait parameter in this case. Using the -Wait switch parameter takes care that the PowerShell script will be opened till the Sysprep process is stopped.\nIf you skip the -Wait parameter the Invoke-AzVMRunCommand command will only execute the PowerShell and waits till the PowerShell script is stopped. If the script is stopped the VM will return a success before the actual process, sysprep.exe, is finished.\n$content¬†=¬†@\u0026#34; param¬†( `$sysprep, `$arg ) Start-Process¬†-FilePath¬†`$sysprep¬†-ArgumentList¬†`$arg -Wait \u0026#34;@ Set-Content¬†-Path¬†.\\sysprep.ps1¬†-Value¬†$content After the file has been created the we are able to execute the file on the virtual machine. Because the file accepts parameters I‚Äôm passing the variables in the -parameter command. Make a notice about storing the virtual machine object into a variable. This variable will be used in the next two commands.\n$vm¬†=¬†Get-AzVM¬†-Name¬†$VMName $vm¬†|¬†Invoke-AzVMRunCommand¬†-CommandId¬†\u0026#34;RunPowerShellScript\u0026#34;¬†-ScriptPath¬†.\\sysprep.ps1¬†-Parameter¬†@{sysprep¬†=¬†\u0026#34;C:\\Windows\\System32\\Sysprep\\Sysprep.exe\u0026#34;;arg¬†=¬†\u0026#34;/generalize¬†/oobe¬†/shutdown¬†/quiet¬†/mode:vm\u0026#34;} By using the /shutdown switch the will shutdown after the process is finished. The VM must have a stopped status before we are able to generalize the VM.\nGeneralize Generalizing a virtual machine is the last step before we are able to create an image version. Generalizing a machine is done by one simple command.\nNeeded module: Az.Compute\n$vm¬†|¬†Set-AzVm¬†-Generalized Create image version The last step is creating a new image version into the gallery. As you can see I‚Äôm using information which I already know and has been stored into variables in the earlier steps. After the image has been deployed the next step is create AVD session hosts from this image.\n$imageVersionParameters¬†=¬†@{ GalleryImageDefinitionName¬†=¬†$imageDefinition.Name GalleryImageVersionName¬†=¬†(Get-Date¬†-f¬†\u0026#34;yyyy.MM.dd\u0026#34;) GalleryName¬†=¬†$gallery.Name ResourceGroupName¬†=¬†$gallery.ResourceGroupName Location¬†=¬†$gallery.Location SourceImageId¬†=¬†$vm.id.ToString() } $imageVersion = New-AzGalleryImageVersion¬†@imageVersionParameters Azure Virtual Desktop After the backend is prepared, it is time to deploy the AVD environment. This deployment differs a little in relation to other cocktails like The Fresh Minty Breeze. In the cocktails where templates and parameter files are used I recommend creating single template files. These files are used for repeatable actions like deploying session host. This will make life easier and you code a lot more cleaner.\nIn this case we only use code and we are in full control which commands we are using and when.\nAVD Hostpool The first step in deploying an AVD enviroment automated with PowerShell is creating a hostpool. In this hostpool I also configuring the new StartVMOnConnect option. This option allows you to shutdown sessionhosts at the end of the day. When the first account is connecting the host will start automatically.\nNeeded module: Az.DesktopVirtualization\n$hostpoolParameters¬†=¬†@{ Name¬†=¬†\u0026#34;CoconutBeach-Hostpool\u0026#34; Description¬†=¬†\u0026#34;A¬†nice¬†coconut¬†on¬†a¬†sunny¬†beach\u0026#34; ResourceGroupName¬†=¬†$resourceGroupName Location¬†=¬†$location HostpoolType¬†=¬†\u0026#34;Pooled\u0026#34; LoadBalancerType¬†=¬†\u0026#34;BreadthFirst\u0026#34; preferredAppGroupType¬†=¬†\u0026#34;Desktop\u0026#34; ValidationEnvironment¬†=¬†$true StartVMOnConnect¬†=¬†$true } $avdHostpool¬†=¬†New-AzWvdHostPool¬†@hostpoolParameters Start VM on Connect To save costs it is recommended to enable the Start VM on Connection feature. Enabling this feature allows you to shutdown all the hosts at the end of the day. If all hosts are down and a person is logging in a session host will start automatically.\nTo enable this feature you will need to create a custom role in the Azure AD and assign this role to the session host resourcegroup.\nTo make this process easier I wrote the PowerShell module Az.Avd. In this module there is a command which allows you enabling the Start VM on Connect feature in one simple command. Because this is an initial deployment I need to provide a HostResourceGroup as well. This is the resource group where the session hosts are. In an existing environment you can skip the parameter. The module will find out itself.\nNeeded module: Az.Avd\n$startVmParameters¬†=¬†@{ HostpoolName¬†=¬†$avdHostpool.Name ResourceGroupName¬†=¬†$hostpoolParameters.resourceGroupName HostResourceGroup¬†=¬†$hostpoolParameters.resourceGroupName } $startVmOnConnect¬†=¬†Enable-AvdStartVmOnConnect¬†@startVmParameters Application group The application group is the place where to assign users/groups to the AVD environment. Also in this part I‚Äôm using variables from the previous deployment.\n$applicationGroupParameters¬†=¬†@{ ResourceGroupName¬†=¬†$ResourceGroupName Name¬†=¬†\u0026#34;CoconutBeachApplications\u0026#34; Location¬†=¬†$location FriendlyName¬†=¬†\u0026#34;Applications¬†on¬†the¬†beach\u0026#34; Description¬†=¬†\u0026#34;From¬†the¬†CoconutBeach-deployment\u0026#34; HostPoolArmPath¬†=¬†$avdHostpool.Id ApplicationGroupType¬†=¬†\u0026#34;Desktop\u0026#34; } $applicationGroup¬†=¬†New-AzWvdApplicationGroup¬†@applicationGroupParameters Workspace The AVD workspace is the frontend of the AVD enviroment. This is the place where people subscribing to. Deploying the workspace is the last step before deploying the AVD session hosts. In the code below we are creating a new workspace and assign it to the recently created application group.\n$workSpaceParameters¬†=¬†@{ ResourceGroupName¬†=¬†$ResourceGroupName Name¬†=¬†\u0026#34;Party-Workspace\u0026#34; Location¬†=¬†$location FriendlyName¬†=¬†\u0026#34;The¬†party¬†workspace\u0026#34; ApplicationGroupReference¬†=¬†$applicationGroup.Id Description¬†=¬†\u0026#34;This¬†is¬†the¬†place¬†to¬†party\u0026#34; } $workSpace¬†=¬†New-AzWvdWorkspace¬†@workSpaceParameters Make sure, after deployment, you will assign the workspace to the correct users or groups.\nAVD Session hosts The last step is deploying the session hosts into the AVD hostpool. Before deploying a session host I decided to deploy an Azure Key Vault first. Into the key vault I will store the administrator password for the domain join. In the later steps I will reference to this key vault secret in the template.\nNeeded modules: Az.DesktopVirtualization, Az.KeyVault\nAzure Key vault In this step I will create an Azure Key Vault with PowerShell and store the administrator password in it. The password will be used later in the automation sequence.\n$keyVaultParameters¬†=¬†@{ Name¬†=¬†\u0026#34;CoconutKeyVault\u0026#34; ResourceGroupName¬†=¬†$resourceGroupName Location¬†=¬†$location } $keyVault¬†=¬†New-AzKeyVault¬†@keyVaultParameters $secretString = \u0026#34;V3ryS3cretP4sswOrd!\u0026#34; $secretParameters¬†=¬†@{ VaultName¬†=¬†$keyVault.VaultName Name=¬†\u0026#34;vmjoinerPassword\u0026#34; SecretValue¬†=¬†ConvertTo-SecureString¬†-String¬†$secretString¬†-AsPlainText¬†-Force } $secret¬†=¬†Set-AzKeyVaultSecret¬†@secretParameters Create session host Finally it is time to create our first AVD session host with PowerShell. Before starting the deployment we need to create a hostpool registration token first. This token will be use to add the session host into the correct hostpool.\nNeeded module: Az.Avd, Az.Compute\nFirst I set the needed parameters to deploy AVD session hosts.\n$sessionHostCount¬†=¬†1 $initialNumber¬†=¬†1 $VMLocalAdminUser¬†=¬†\u0026#34;LocalAdminUser\u0026#34; $VMLocalAdminSecurePassword¬†=¬†ConvertTo-SecureString¬†(Get-AzKeyVaultSecret¬†-VaultName¬†$keyVault.Vaultname¬†-Name¬†$secret.Name¬†)¬†-AsPlainText¬†-Force $avdPrefix¬†=¬†\u0026#34;avd-\u0026#34; $VMSize¬†=¬†\u0026#34;Standard_D2s_v3\u0026#34; $DiskSizeGB¬†=¬†512 $domainUser¬†=¬†\u0026#34;joinaccount@domain.local\u0026#34; $domain¬†=¬†$domainUser.Split(\u0026#34;@\u0026#34;)[-1] $ouPath¬†=¬†\u0026#34;OU=Computers,OU=AVD,DC=domain,DC=local\u0026#34; $registrationToken¬†=¬†Update-AvdRegistrationToken¬†-HostpoolName¬†$avdHostpool.name¬†$resourceGroupName $moduleLocation¬†=¬†\u0026#34;https://wvdportalstorageblob.blob.core.windows.net/galleryartifacts/Configuration_01-19-2023.zip\u0026#34; When using templates like ARM or Bicep the template and ARM engine takes care of looping through the machine count. If you deploy with only code you will need to create a loop by yourself. In this case I created an Do-While loop. It executes code as long the condition is not met.\nThe condition is while the sessionHostCount is not 0. In the parameters above I set the count to 1. At the end of the ‚Äúdo‚Äù-part the counter will decrease with 1. Till the counter is not 0 the loop will execute.\nFirst I will create a network card and a VM. After the resources are deployed I install the needed extensions for the AD domain join and the AVD agent.\nDo¬†{ $VMName¬†=¬†$avdPrefix+\u0026#34;$initialNumber\u0026#34; $ComputerName¬†=¬†$VMName $nicName¬†=¬†\u0026#34;nic-$vmName\u0026#34; $NIC¬†=¬†New-AzNetworkInterface¬†-Name¬†$NICName¬†-ResourceGroupName¬†$ResourceGroupName¬†-Location¬†$location¬†-SubnetId¬†($virtualNetwork.Subnets¬†|¬†Where¬†{¬†$_.Name¬†-eq¬†\u0026#34;avdSubnet\u0026#34;¬†}).Id $Credential¬†=¬†New-Object¬†System.Management.Automation.PSCredential¬†($VMLocalAdminUser,¬†$VMLocalAdminSecurePassword); $VirtualMachine¬†=¬†New-AzVMConfig¬†-VMName¬†$VMName¬†-VMSize¬†$VMSize $VirtualMachine¬†=¬†Set-AzVMOperatingSystem¬†-VM¬†$VirtualMachine¬†-Windows¬†-ComputerName¬†$ComputerName¬†-Credential¬†$Credential¬†-ProvisionVMAgent¬†-EnableAutoUpdate $VirtualMachine¬†=¬†Add-AzVMNetworkInterface¬†-VM¬†$VirtualMachine¬†-Id¬†$NIC.Id $VirtualMachine¬†=¬†Set-AzVMOSDisk¬†-Windows¬†-VM¬†$VirtualMachine¬†-CreateOption¬†FromImage¬†-DiskSizeInGB¬†$DiskSizeGB $VirtualMachine¬†=¬†Set-AzVMSourceImage¬†-VM¬†$VirtualMachine¬†-Id¬†$imageVersion.id $sessionHost¬†=¬†New-AzVM¬†-ResourceGroupName¬†$ResourceGroupName¬†-Location¬†$Location¬†-VM¬†$VirtualMachine $initialNumber++ $sessionHostCount-- Write-Output¬†\u0026#34;$VMName¬†deployed\u0026#34; } while¬†($sessionHostCount¬†-ne¬†0)¬†{ Write-Verbose¬†\u0026#34;Session¬†hosts¬†are¬†created\u0026#34; } Install AVD extensions with PowerShell To make a ‚Äònormal‚Äô vm a AVD session host we need to install two extensions. The ActiveDirectory domain join extension and the desired state config (DSC) extension. The DSC extension installs the AVD agent software and registers the host into the AVD hostpool.\nThere are two separate commands available for deploying the specific extension types. More information about these types Set-AzVmDscExtension and Set-AzVMADDomainExtension check the Microsoft documentation. To keep my code as simple as possible I‚Äôve chosen to use the Set-AzVMExtension.\n$domainJoinSettings¬†=¬†@{ Name¬†=¬†\u0026#34;joindomain\u0026#34; Type¬†=¬†\u0026#34;JsonADDomainExtension\u0026#34;¬†Publisher¬†=¬†\u0026#34;Microsoft.Compute\u0026#34; typeHandlerVersion¬†=¬†\u0026#34;1.3\u0026#34; SettingString¬†=¬†\u0026#39;{ \u0026#34;name\u0026#34;:¬†\u0026#34;\u0026#39;+¬†$($domain)¬†+¬†\u0026#39;\u0026#34;, \u0026#34;ouPath\u0026#34;:¬†\u0026#34;\u0026#39;+¬†$($ouPath)¬†+¬†\u0026#39;\u0026#34;, \u0026#34;user\u0026#34;:¬†\u0026#34;\u0026#39;+¬†$($domainUser)¬†+¬†\u0026#39;\u0026#34;, \u0026#34;restart\u0026#34;:¬†\u0026#34;\u0026#39;+¬†$true¬†+¬†\u0026#39;\u0026#34;, \u0026#34;options\u0026#34;:¬†3 }\u0026#39; ProtectedSettingString¬†=¬†\u0026#39;{ \u0026#34;password\u0026#34;:\u0026#34;\u0026#39;¬†+¬†$(Get-AzKeyVaultSecret¬†-VaultName¬†$keyVault.Vaultname¬†-Name¬†$secret.Name¬†-AsPlainText)¬†+¬†\u0026#39;\u0026#34;}\u0026#39; VMName¬†=¬†$VMName ResourceGroupName¬†=¬†$resourceGroupName location¬†=¬†$Location } Set-AzVMExtension¬†@domainJoinSettings $avdDscSettings¬†=¬†@{ Name¬†=¬†\u0026#34;Microsoft.PowerShell.DSC\u0026#34; Type¬†=¬†\u0026#34;DSC\u0026#34;¬†Publisher¬†=¬†\u0026#34;Microsoft.Powershell\u0026#34; typeHandlerVersion¬†=¬†\u0026#34;2.73\u0026#34; SettingString = \u0026#34;{ \u0026#34;\u0026#34;modulesUrl\u0026#34;\u0026#34;:\u0026#39;$avdModuleLocation\u0026#39;, \u0026#34;\u0026#34;ConfigurationFunction\u0026#34;\u0026#34;:\u0026#34;\u0026#34;Configuration.ps1\\\\AddSessionHost\u0026#34;\u0026#34;, \u0026#34;\u0026#34;Properties\u0026#34;\u0026#34;: { \u0026#34;\u0026#34;hostPoolName\u0026#34;\u0026#34;: \u0026#34;\u0026#34;$($fileParameters.avdSettings.avdHostpool.Name)\u0026#34;\u0026#34;, \u0026#34;\u0026#34;registrationInfoToken\u0026#34;\u0026#34;: \u0026#34;\u0026#34;$($registrationToken.token)\u0026#34;\u0026#34;, \u0026#34;\u0026#34;aadJoin\u0026#34;\u0026#34;: true } }\u0026#34; VMName¬†=¬†$VMName ResourceGroupName¬†=¬†$resourceGroupName location¬†=¬†$Location } Set-AzVMExtension¬†@avdDscSettings¬†Check the complete do-while loop at my GitHub page.\nMonitoring The next step in our sequence is the monitoring part. In this part we are going to install a Log Analytics Workspace and will enable diagnostic settings on the hostpool and the workspace.\nIn the code below I‚Äôm creating a new Log Analytics Workspace with a fixed prefix and a random number. This because of a Log Analytics workspaces must be unique per resource group.\n$loganalyticsParameters¬†=¬†@{ Location¬†=¬†$Location¬†Name¬†=¬†\u0026#34;log-analytics-avd-\u0026#34;¬†+¬†(Get-Random¬†-Maximum¬†99999) Sku¬†=¬†\u0026#34;Standard\u0026#34;¬†ResourceGroupName¬†=¬†$resourceGroupName } #¬†Create¬†the¬†workspace $laws¬†=¬†New-AzOperationalInsightsWorkspace¬†@loganalyticsParameters In the next part I‚Äôm connecting the workspace to the AVD diagnostics settings. Also in this situation I will use the deployment variable.\nMake a notice on the orange arrow. This variable was created in the hostpool part of the AVD environment.\nEnabling these settings are the basics for a good monitoring environment. If you like to configure advanced AVD monitoring automated please check my post about enabling AVD monitoring automated. I will recommend monitoring the AVD required URLs also. Check my post about how to achieve that goal.\nConclusion PowerShell is my ‚Äòmother‚Äô-language for automation. I‚Äôve created a lot of scripts with PowerShell but must say, during writing this post, I‚Äôve seen some new commands. I took some time to figure it out how these work the best. Especially the extension part took some time.\nThe great of PowerShell is that it is community driven. This makes it possible to created modules to make life easier. For example the Az.Avd module I wrote. In this post I use my own module and the native modules like Az.Resources or Az.Compute.\nAll these modules are great and nasty at the same time. If you think you‚Äôre all set a new module dependency came up. So make sure you have install all the needed module.\nIf you are using this sequence in a DevOps pipeline for example, I would suggest to only install the needed modules. Installing all the Az* module will take some time and is totally overkill.\nFor me it was really fun creating a fully PowerShell driven deployment for AVD. Of course I prefer parameter files as well but in fact in the very beginning I just started with a location and a resource group name. From that perspective I‚Äôm able to deploy an AVD enviroment fully automated.. This will allow you to generate an AVD environment fully automated by just providing a location only.\nAnd that‚Äôs what I like about PowerShell.\nThank you! I hope you liked the Coconut Beach Party üßâ üèñ ü•≥ and got a bit inspired. Now you know deploy AVD automated with PowerShell is one of the options. If you like another cocktail feel free to stay and check the AVD Automation Cocktail menu.\nThank you for reading my blog avd automation cocktail - azure virtual desktop automated with powershell. I hope you got a bit inspired. Enjoy your day and happy automating üëã ","date":"June 28, 2021","image":"http://localhost:1313/avd-automation-cocktail-avd-automated-with-powershell/cocktail-coconut-party.png","permalink":"/avd-automation-cocktail-avd-automated-with-powershell/","title":"AVD Automation Cocktail - Azure Virtual Desktop automated with PowerShell"},{"categories":["AVD Cocktail","Azure Virtual Desktop"],"contents":"Welcome to the AVD Automation Cocktail. In this cocktail series I will show different AVD deployment strategies and languages. In this cocktail, the Fresh Minty Breeze, I will show you how to deploy an AVD environment automated with Bicep and Azure CLI.\nTable Of Contents Recipe Before to drink List CLI of ingredients Aftertaste AVD automated with Bicep en Azure CLI Resource Group Networking Shared Image Gallery Initial Image Version Create a virtual machine SysPrep Generalize VM Create image version Azure Virtual Desktop Monitoring AVD Session hosts Azure Key vault Create session host Conclusion Thank you! Recipe In this ‚ÄúMintyBreeze‚Äù-deployment recipe I will deploy an AVD environment automated with Bicep and Azure CLI. Bicep is a fresh new coding language for deploying Azure resources. Bicep helps reduce the syntax complexity which ARM templates has. Because it is very new I like to show in this article how to deploy an AVD environment with Bicep. To give Azure CLI also a stage I thought this could be a really nice combination.\nBefore to drink To start enrolling AVD automated with Bicep and Azure CLI you will need to install the Bicep CLI and Azure CLI software first. To download the Bicep installer go to the following URL, choose your favorite operating system and follow the instructions. https://github.com/Azure/bicep/blob/main/docs/installing.md#install-the-bicep-cli-details\nAfter the installation of Bicep install the Azure CLI software by clicking this URL: https://docs.microsoft.com/en-us/cli/azure/install-azure-cli\nMore information about Bicep or Azure CLI please check the following URLs:\nhttps://docs.microsoft.com/en-us/azure/azure-resource-manager/bicep/overview https://docs.microsoft.com/en-us/cli/azure/ List CLI of ingredients https://docs.microsoft.com/en-us/cli/azure/vm/run-command?view=azure-cli-latest#az_vm_run_command_invoke (VM Run command) https://docs.microsoft.com/en-us/cli/azure/vm?view=azure-cli-latest#az_vm_capture (VM Capture) https://docs.microsoft.com/en-us/azure/virtual-machines/image-version-vm-cli (Image Version) https://docs.microsoft.com/en-us/cli/azure/vm?view=azure-cli-latest#az_vm_generalize (VM Generalize) https://docs.microsoft.com/en-us/cli/azure/desktopvirtualization/hostpool?view=azure-cli-latest#az_desktopvirtualization_hostpool_update (AVD Hostpool token) https://docs.microsoft.com/en-us/cli/azure/ad/signed-in-user?view=azure-cli-latest (AD Signed in User) https://docs.microsoft.com/en-us/cli/azure/deployment/group?view=azure-cli-latest#az_deployment_group_create (Deployment) Aftertaste In the beginning this cocktail has a fresh taste with a lot Bicep and a pinch of Azure CLI. At the end you will have an AVD environment in Azure deployed with all the needed resources. These are a hostpool, a workspace, an application group. Also there are some session hosts. These hosts have an underlying image from a shared image gallery.\nAVD automated with Bicep en Azure CLI In this chapter I will explain how to deploy an AVD environment automated with Bicep and Azure CLI. I will recommend to clone my AVD GitHub repository to get all the needed files, also for the other cocktails. I like to work with modules as much as I can. This will avoid you are repeating yourself in lines of code and will help you keeping your code nice and clean. You will notice that when looking back in the file structure I‚Äôve setup in the repository.\nFor every purpose, in this case create an AVD environment with a SIG in basics, I will create a main.bicep file and will use modules in it.\nLater on if I need a new environment without a SIG for example I only have to create a new main.bicep without the SIG part. Yes there are options for a conditional deploy but will skip that part for now to keep it as simple as possible.\nResource Group Before deploying anything we need to login and create a new resource group.\naz login az¬†group¬†create¬†--name¬†RG-ROZ-BREEZE-COCKTAIL¬†--location¬†westeurope Networking The base of every environment is networking. In this step the idea is to deploy a new virtual network (vnet) with two subnets, a default subnet and an AVD-Breeze-subnet. Because my goal is to deploy as much as I can from parameter files I‚Äôve create an array into a parameter file. That array will be transformed to a Bicep array. The array is used to deploy the vnet.\nThe parameter snippet looks like the following:\n\u0026lt;pre class=\u0026#34;wp-block-code\u0026#34;\u0026gt;```json \u0026#34;vnetSubnets\u0026#34;:¬†{ \u0026#34;value\u0026#34;:¬†{ \u0026#34;subnets\u0026#34;:¬†[ { \u0026#34;name\u0026#34;:¬†\u0026#34;DefaultSubnet\u0026#34;, \u0026#34;addressPrefix\u0026#34;:¬†\u0026#34;10.0.0.0/24\u0026#34; }, { \u0026#34;name\u0026#34;:¬†\u0026#34;AVDBreezeSubnet\u0026#34;, \u0026#34;addressPrefix\u0026#34;:¬†\u0026#34;10.0.1.0/24\u0026#34; } ] } } A dynamic array is created which will be inserted into the deployment. A dynamic array has its pros because now you are scale able within your parameter file. It doesn‚Äôt matter how many subnets are in the parameter file. If you need one extra just add it to the file.\nThe parameter accepts the incoming parameter. It will create the array subnets. After that it will insert it into the subnets resource object.\nparam¬†vnetSubnets¬†object var¬†subnets¬†=¬†[for¬†item¬†in¬†vnetSubnets.subnets:¬†{ name:¬†item.name properties:¬†{ addressPrefix:¬†item.addressPrefix } }] resource¬†vnet¬†\u0026#39;Microsoft.Network/virtualNetworks@2020-06-01\u0026#39;¬†=¬†{ name:¬†vnetName location:¬†location properties:¬†{ addressSpace:¬†{ addressPrefixes:¬†[ addressPrefix ] } enableVmProtection:¬†false enableDdosProtection:¬†false subnets:¬†subnets } } Mention the format of the array. It must be the same format as regular deployment code.\nAfter the template and parameters are ready to deploy use the CLI command below to deploy the resources.\naz¬†deployment¬†group¬†create¬†--resource-group¬†RG-ROZ-MINTYBREEZE-COCKTAIL¬†--template-file¬†.\\Templates\\Network\\deploy-vnet-with-subnet.bicep¬†--parameters¬†.\\Parameters\\vnet-with-subnet.parameters.json Shared Image Gallery In this step, we are going to create a new image version into a Shared Image Gallery. In the next Bicep files we taking care of creating the SIG. It will create the image definition and the version.\nI created a template bicep file for creating a SIG and its definition. As you can see in the screenshot it will deploy two resources. The thing which is not in the examples (yet) , is deploying a SIG with dependencies. Bicep will try to deploy resources in parallel. This means it will deploy these both resources at once. Because the definition depends on the gallery the deployment will fail. (I created an pull request to fix that)\nIf you have dependencies then tell which dependencies it has. The definition deployment will wait for the main resource.\nAnd then the deployment is successful. In the code below I used the SIG template only with a parameter file. Check my GitHub repository for the SIG template and parameter file.\naz¬†deployment¬†group¬†create¬†--resource-group¬†RG-ROZ-BREEZE-COCKTAIL¬†--template-file¬†.\\Templates\\SIG\\deploy-shared-image-gallery.bicep¬†--parameters¬†.\\Parameters\\shared-image-gallery.parameters.json Initial Image Version The next step in our MintyBreeze-deployment is creating a new virtual machine. This machine is used for creating a new image version. This version will be deployed into the gallery. In the first step, we are creating a parameter file with the virtual machine configuration. The Bicep file for a VM and NIC deployment.\nCreate a virtual machine I have chosen to keep is as simple as possible, so no Network Security Group for example. The machine has no public IP and will be removed after a sysprep. Sysprep is the next step after the VM deployment.\n//create¬†nic resource¬†vmNic¬†\u0026#39;Microsoft.Network/networkInterfaces@2017-06-01\u0026#39;¬†=¬†{ name:¬†defaultVmNicName location:¬†defaultLocation properties:¬†{ ipConfigurations:¬†[ { name:¬†\u0026#39;ipconfig1\u0026#39; properties:¬†{ subnet:¬†{ id:¬†resourceId(\u0026#39;Microsoft.Network/virtualNetworks/subnets\u0026#39;,¬†vnetName,¬†subnetName) } privateIPAllocationMethod:¬†privateIPAllocationMethod } } ] } } //create¬†VM resource¬†vm¬†\u0026#39;Microsoft.Compute/virtualMachines@2019-07-01\u0026#39;¬†=¬†{ name:¬†vmName location:¬†defaultLocation properties:¬†{ osProfile:¬†{ computerName:¬†vmName adminUsername:¬†localAdminName adminPassword:¬†localAdminPassword } hardwareProfile:¬†{ vmSize:¬†vmSize } storageProfile:¬†{ imageReference:¬†{ publisher:¬†\u0026#39;MicrosoftWindowsDesktop\u0026#39; offer:¬†vmOffer sku:¬†vmOs version:¬†\u0026#39;latest\u0026#39; } osDisk:¬†{ createOption:¬†\u0026#39;FromImage\u0026#39; } } licenseType:¬†\u0026#39;Windows_Client\u0026#39; networkProfile:¬†{ networkInterfaces:¬†[ { properties:¬†{ primary:¬†true } id:¬†vmNic.id } ] } } } Check my GitHub repository at this location for the VM deployment file.\naz¬†deployment¬†group¬†create¬†--resource-group¬†RG-ROZ-MINTYBREEZE-COCKTAIL¬†--template-file¬†.\\Templates\\VM\\deploy-vm-win10.bicep¬†--parameters¬†.\\Parameters\\vm-win10.parameters.json SysPrep First the VM must be generalized and Sysprepped. There are several options for running a script on a VM. Think about a Custom Script Extension or an Invoke-AzRunCommand in PowerShell. In this case I‚Äôm using the Azure CLI.\naz¬†vm¬†run-command¬†invoke¬†--command-id¬†RunPowerShellScript¬†--name¬†MintyBreezeVm¬†-g¬†RG-ROZ-MINTYBREEZE-COCKTAIL¬†--scripts¬†\u0026#39;param([string]$sysprep,[string]$arg)¬†Start-Process¬†-FilePath¬†$sysprep¬†-ArgumentList¬†$arg\u0026#39;¬†--parameters¬†\u0026#34;sysprep=C:\\Windows\\System32\\Sysprep\\Sysprep.exe\u0026#34;¬†\u0026#34;arg=/generalize¬†/oobe¬†/shutdown¬†/quiet¬†/mode:vm\u0026#34;¬†Using the az vm run-command CLI comand gives me the option to skip creating an separate script first. In the ‚Äìscripts part of the command is able to create a PowerShell on-the-fly. In the ‚Äìparameters part I will send these parameters.\nWhen running the command, the VM will create a PowerShell file on the local machine. The provided ‚Äìscripts content is stored in that local file.\nGeneralize VM Next step beforce creating an image version is generalize and capture the virtual machine. To achieve that goal we are using the CLI again.\naz¬†vm¬†generalize¬†-g¬†RG-ROZ-MINTYBREEZE-COCKTAIL¬†-n¬†MintyBreezeVm Create image version The last step is to create a new image version. This is a deployment task so we using Bicep again. In the first step I will search for the VM Id with Azure CLI. The VM Id is the source input for the image creation. The dynamic parameters (VM Id) are additional passed in the Azure CLI command together with the parameter file\nThe parameter file will look like below.\n\u0026lt;pre class=\u0026#34;wp-block-code\u0026#34;\u0026gt;```json { \u0026#34;$schema\u0026#34;:¬†\u0026#34;https://schema.management.azure.com/schemas/2019-04-01/deploymentParameters.json#\u0026#34;, \u0026#34;contentVersion\u0026#34;:¬†\u0026#34;1.0.0.0\u0026#34;, \u0026#34;parameters\u0026#34;:¬†{ \u0026#34;galleryName\u0026#34;:¬†{ \u0026#34;value\u0026#34;:¬†\u0026#34;SigMintyBreeze\u0026#34; }, \u0026#34;definitionName\u0026#34;:¬†{ \u0026#34;value\u0026#34;:¬†\u0026#34;The-MintyBreeze-Deployment\u0026#34; }, \u0026#34;replicaRegion\u0026#34;:¬†{ \u0026#34;value\u0026#34;:¬†\u0026#34;westeurope\u0026#34; } } } Creating versions is a task which is executed often. Due that reason I created a separate Bicep file for that. The gallery and definition are deployed in the initial phase. The galleryName, definitionName and replicaRegion are pretty fixed and stored in the parameter file. The version name is unique and created by the CLI.\nparam¬†galleryName¬†string param¬†definitionName¬†string param¬†versionName¬†string param¬†replicaRegion¬†string param¬†source¬†string resource¬†sigVersion¬†\u0026#39;Microsoft.Compute/galleries/images/versions@2019-12-01\u0026#39;¬†=¬†{ name:¬†\u0026#39;${galleryName}/${definitionName}/${versionName}\u0026#39; location:¬†resourceGroup().location tags:¬†{} properties:¬†{ publishingProfile:¬†{ targetRegions:¬†[ { name:¬†replicaRegion } ] replicaCount:¬†1 } storageProfile:¬†{ source:¬†{ id:¬†source } osDiskImage:¬†{ hostCaching:¬†\u0026#39;ReadWrite\u0026#39; } } } } In the Azure CLI code below the deployment should start. Before starting the deployment I‚Äôm searching for the VM id. I use the ‚Äìquery to get the id only. (Like the select command in PowerShell)\n$vm¬†=¬†az¬†vm¬†show¬†--resource-group¬†RG-ROZ-MINTYBREEZE-COCKTAIL¬†--name¬†MintyBreezeVM¬†--query¬†\u0026#39;id\u0026#39; az¬†deployment¬†group¬†create¬†--resource-group¬†RG-ROZ-MINTYBREEZE-COCKTAIL¬†--template-file¬†.\\Templates\\SIG\\deploy-shared-image-gallery-version.bicep¬†--parameters¬†.\\Parameters\\shared-image-gallery-version.parameters.json¬†versionName=\u0026#39;2021.06.14\u0026#39;¬†source=$vm Azure Virtual Desktop Now every needed component is in place it is time to deploy the the Azure Virtual Desktop environment with Bicep. In this deployment we are going to deploy a host pool. The next step is the application group. The last step is a workspace. I have chosen to create a separate Bicep file for the session host. This for exactly the same reason as for an image gallery.\nI created two Bicep templates. The AVD backend part with the host pool, application group and workspace. The second template for the AVD session hosts. This for exactly the same reason as for an image gallery.\n//Create¬†WVD¬†Hostpool resource¬†hp¬†\u0026#39;Microsoft.DesktopVirtualization/hostpools@2019-12-10-preview\u0026#39;¬†=¬†{ name:¬†hostpoolName location:¬†avdlocation properties:¬†{ friendlyName:¬†hostpoolFriendlyName hostPoolType:¬†hostPoolType loadBalancerType:¬†loadBalancerType preferredAppGroupType:¬†preferredAppGroupType } } //Create¬†WVD¬†AppGroup resource¬†ag¬†\u0026#39;Microsoft.DesktopVirtualization/applicationgroups@2019-12-10-preview\u0026#39;¬†=¬†{ name:¬†appgroupName location:¬†avdlocation properties:¬†{ friendlyName:¬†appgroupNameFriendlyName applicationGroupType:¬†applicationgrouptype hostPoolArmPath:¬†hp.id } } //Create¬†WVD¬†Workspace resource¬†ws¬†\u0026#39;Microsoft.DesktopVirtualization/workspaces@2019-12-10-preview\u0026#39;¬†=¬†{ name:¬†workspaceName location:¬†avdlocation properties:¬†{ friendlyName:¬†workspaceNameFriendlyName applicationGroupReferences:¬†[ ag.id ] } } For the initial parts I created a parameter file. Check my GitHub repository for these files.\naz¬†deployment¬†group¬†create¬†--resource-group¬†RG-ROZ-MINTYBREEZE-COCKTAIL¬†--template-file¬†.\\Templates\\AVD\\deploy-avd-environment.bicep¬†--parameters¬†.\\Parameters\\avd-enviroment.parameters.json \u0026gt; Monitoring As in every environment, we also like to monitor this environment. To monitor this environment we are going to use Log Analytics.\nI used a template that deploys a LogAnalytics workspace. This will enable the provided logs for the AVD environment. After creating the workspace the diagnostic settings will be deployed.\nFor the WorkspaceId I referenced the output of the LogAnalytics Workspace resource deployment.\naz¬†deployment¬†group¬†create¬†--resource-group¬†RG-ROZ-MINTYBREEZE-COCKTAIL¬†--template-file¬†.\\Templates\\LogAnalytics\\deploy-LogAnalytics.bicep¬†--parameters¬†logAnalyticsWorkspaceName=\u0026#39;avdLogAnalyticsWorkspace\u0026#39;¬†logAnalyticsWorkspaceSku=\u0026#39;pergb2018\u0026#39; AVD Session hosts The last step is deploying the session hosts into the AVD hostpool. Before deploying a session host I decided to deploy an Azure Key Vault first. Into the key vault I will store the administrator password for the domain join. In the later steps I will reference to this key vault secret in the template.\nAzure Key vault In this step I will create an Azure Key Vault with Bicep and store the administrator password in it. I created a template and a parameter file. When running the deploy command the CLI will ask you for the vmjoiner password.\naz deployment group create --resource-group RG-ROZ-MINTYBREEZE-COCKTAIL --template-file .\\Templates\\KeyVault\\deploy-keyvault-with-secret.bicep --parameter .\\Parameters\\keyvault-parameters.json objectId=$objectId A new Bicep release is available: v0.4.63. Upgrade now by running \u0026#34;az bicep upgrade\u0026#34;. C:\\Users\\Sander\\Personal Repo\\AVD\\Deployment\\Bicep\\Templates\\KeyVault\\deploy-keyvault-with-secret.bicep(51,28) : Warning secure-parameter-default: Secure parameters should not have hardcoded defaults (except for empty or newGuid()). [https://aka.ms/bicep/linter/secure-parameter-default] Please provide string value for \u0026#39;vmJoinerPassword\u0026#39; (? for help): An another point is setting the key vault access policy. To grant access to the key vault I search for the current users ObjectId and add it to the access policy.\nThe parameter file and the objectId are passed into the deployment.\n$objectId¬†=¬†az¬†ad¬†signed-in-user¬†show¬†--query¬†objectId az¬†deployment¬†group¬†create¬†--resource-group¬†RG-ROZ-MINTYBREEZE-COCKTAIL¬†--template-file¬†.\\Templates\\KeyVault\\deploy-keyvault-with-secret.bicep¬†--parameter¬†.\\Parameters\\keyvault-parameters.json¬†objectId=$objectId The secret is added with the correct access policy.\nCreate session host The next step is creating session with a Bicep template. Before starting the deployment we need to create a hostpool registration token first. This token will be use to add the session host into the correct hostpool.\n$hostpoolToken¬†=¬†az¬†desktopvirtualization¬†hostpool¬†update¬†--resource-group¬†RG-ROZ-MINTYBREEZE-COCKTAIL¬†--name¬†MintyBreeze-Hostpool¬†--registration-info¬†expiration-time=\u0026#34;2021-06-16T15:01:54.9571247Z\u0026#34;¬†registration-token-operation=\u0026#34;Update\u0026#34;¬†--query¬†\u0026#39;registrationInfo.token\u0026#39; The command below will take care for getting the domain join password from the key vault.\n$adminpassword¬†=¬†az¬†KeyVault¬†secret¬†show¬†--vault-name¬†MintyBreezeKeyVault¬†--name¬†vmJoinerPassword¬†--query¬†value The value above are dynamic and/or secure values which are not recommended to save in a parameter files. I store these dynamic values into a variable which are added in the deployment command together with the Bicep template and parameter file.\naz¬†deployment¬†group¬†create¬†--resource-group¬†RG-ROZ-MINTYBREEZE-COCKTAIL¬†--template-file¬†.\\Templates\\AVD\\deploy-avd-sessionhosts.bicep¬†--parameters¬†.\\Parameters\\avd-sessionhost.parameters.json¬†administratorAccountPassword=$adminpassword¬†hostpoolToken=$hostpoolToken After the deployment the session hosts are created in the correct OU.\nI have the automated assignment of users to application group under investigation.\nFor now everything is in place you only have to assign a usergroup to the application group and that‚Äôs it.\nConclusion I‚Äôm very exited about the way how Bicep works. Bicep is well documented (check the GitHub page) but it is still under development. During my deployment, as you read above, I had to use my ARM knowledge to add additional object into the Bicep template and figuring out how a deployment name should be.\nBut after all Bicep in basics is very useful can help a lot of people with less coding experience.\nI have chosen to keep it as simple as possible from my opinion. Yes there are a lot more options like creating a main.bicep file which accepts all the parameters from one parameter file and creating modules with output.\nIn fact with all this Bicep files from this article you are able to create a main.bicep file. In the main.bicep you can create modules. These modules referrers to the template file. In the template file you configure the resource deployment.\nI‚Äôm also very surprised about Azure CLI. It feels very fast in relation to PowerShell, at least with deployment (or other ‚Äòpush‚Äô tasks). As you read above I‚Äôm also querying some info which isn‚Äôt very fast. A big advantage in relation to PowerShell is that you don‚Äôt need modules, well not as many as in PowerShell :). For the desktop virtualization part I had to install an extension but that‚Äôs it.\nAn another point is that you don‚Äôt have to remember use a lot different commands like get-avd, set-avd, update-avd. It all starts with the same context and then what you like to do, show, remove, update for example.\nThank you! I hope you liked the Fresh Minty Breeze üçÉüí® you are a bit inspired. Now you know deploy AVD automated with Bicep and Azure CLI is one of the options. If you like another cocktail feel free to stay and check the AVD Automation Cocktail menu.\nThank you for reading my blog avd automation cocktail - azure virtual desktop automated with bicep and azure cli. I hope you got a bit inspired. Enjoy your day and happy automating üëã ","date":"June 17, 2021","image":"http://localhost:1313/avd-automation-cocktail-avd-with-bicep-and-azure-cli/cocktail-minty-breeze.png","permalink":"/avd-automation-cocktail-avd-with-bicep-and-azure-cli/","title":"AVD Automation Cocktail - Azure Virtual Desktop automated with Bicep and Azure CLI"},{"categories":["AVD Cocktail","Azure Virtual Desktop"],"contents":"Welcome to a fresh new series about deploying Azure Virtual Desktop environments automated called the ‚ÄúAVD Automation Cocktail‚Äù. In this series I will serve several cocktails and will take you along on the AVD automated deployment journey with different types of automation languages. Every type has its own pros and cons, deployment strategy, parameter structure and commands. In the series all of these items will pass and hopefully it will give you a good overview about the possibilities.\nAt this time of the year with the summer ahead we all looking forward to the well deserved vacation, relax time, BBQ and drinking some beer and cocktails. Until that time I will serve some fresh automation cocktail recipes with PowerShell, ARM, Bicep, JSON, REST API and Azure CLI.\nBefore serving the menu I like to make some agreements first and like to say a few words.\nThere is no good or bad or wrong or right. Every way of automation is fine (if it works of course :)) as long if it works for you Automation can be really frustrating but after all its fun. Keep the fun alive These cocktails are for all ages Every cocktail has its very own recipe, with its own taste and color. You like the color and taste or not but at the end it all goes, just like a real cocktail, about enjoying the moment and having fun. Thank you in advance for your time to read this nice series and I hope you will get inspired.\nWhat‚Äôs on the menu Please make choice between really nice and fresh cocktails with different recipes. Feel free to try them all and have fun with the AVD automated deployment.\nFresh Minty Breeze (The fresh new programming language Bicep will served with a little breeze of Azure CLI) Coconut Beach Party (If you like just to party on a beach full with coconut palm trees then this PowerShell only cocktail is for you) Strawberry Banana Mix (Great mix of the best ingredients DevOps en ARM in one cocktail) Pineapple Citrus Splash (If you really thirsty and need a splash of REST API and JSON with a little sweetness of PowerShell this is the cocktail you need) Sweet Orange Sunset (If you like orange and orange only with no other sidesteps like modules etc, then take this Azure CLI only cocktail) Dutch Bloody Harry (An orange coloured cocktail with Terraform Cloud, GitHub and PowerShell. Also with automated assignements and custom role creation) Santa‚Äôs Tree Ride (A festive cocktail with Azure DevOps, YAML and Az.Avd PowerShell module.) Enjoy your vacation üå¥ and this cocktail üßâ. Thank you for reading my blog avd automation cocktail - the menu. I hope you got a bit inspired. Enjoy your day and happy automating üëã ","date":"June 10, 2021","image":"http://localhost:1313/avd-automation-cocktail-the-menu/avd-cocktail-menu.png","permalink":"/avd-automation-cocktail-the-menu/","title":"AVD Automation Cocktail - The Menu"},{"categories":["Azure Virtual Desktop"],"contents":"It is very common to use a golden image in Azure Virtual Desktop environments. Some are using Azure images, others are using a Shared Image Gallery. A great advantage of using preconfigured images is that you just have to create a new session host from that image and you‚Äôre all set.\nThe change process for an image version is very simple, you will start a virtual machine from the version and you will make the changes. But what if you need to change the OS disk size of the golden image? In this article, I will explain how to change an Azure Virtual Desktop disk size when using a golden image based on the existing environment automated.\nAt some day you will have to extend or shrink an OS disk for your session hosts. You login into the portal, create a new virtual machine, change the disk size afterward, log in for running a Sysprep and at last generate a new version. These are quite a lot of steps that can be simplified in an automation sequence. In the next chapters, I will explain the process of how to change an Azure Virtual Desktop session host disk size automated.\nTable Of Contents Environment explained Change avd disk size process Through the portal Automated Install Az.avd module Getting information Create a virtual machine profile Verify Generalize Create image version Sources Environment explained In this scenario, I have a basic Azure Virtual Desktop environment with a shared image gallery (SIG). I configured a Windows 10 Multisession Generalized V2 image definition.\nAn image definition is a logical group of images with the same specifications like OS-type.\nThe image definition has one image version with a basic Windows 20H2 installation with a 128GB disk. At the end of this article, a new version has been made with a greater disk.\nChange avd disk size process Before changing anything it is good to know how an Azure VM is built. An Azure VM consists of various resources. The most simple has a size (B2 or D4), a disk and a network interface card (NIC). In this change process, we are changing the disk resource.\nCheck this doc which explains more about an Azure VM: https://docs.microsoft.com/en-us/azure/architecture/reference-architectures/n-tier/windows-vm\nIn the end, an image(version) at his place represents a disk state.\nTo know what we are doing first I will describe, in short, how to change a disk size in the portal.\nThrough the portal The change process consists of the following:\nCreate a new virtual machine from an image version Change the disk size in the disks blade Login into the virtual machine for Sysprep Generalize machine Create an image from the virtual machine`s disk Automated Now the fun begins when we are going to automate things, this time with PowerShell. The main difference in the process is that we are now creating a VM with the correct disk size.\nInstall Az.avd module The first step is to make sure you have installed the new Az.avd PowerShell module.\ninstall-module az.avd import-module az.avd This module will help you get the session host\u0026rsquo;s resources and id‚Äôs. In the example below, I search for one session host to get the latest gallery version. This will return all the needed information to create a new virtual machine.\nGet-avdImageVersionStatus -HostpoolName rozemuller-hostpool -ResourceGroupName rg-avd-001 | select -Last 1 vmLatestVersion : 1 vmName : avd-0 imageName : Win10-MultiSession currentImageVersion : 2021.04.26 vmId : /subscriptions/\u0026lt;s\u0026gt;xx\u0026lt;/s\u0026gt;/resourceGroups/RG-avd-001/providers/Microsoft.Compute/virtualMachines/avd-0 imageGallery : Rozemuller_ImageGallery resourceGroupName : rg-sig-001 subscriptionId : \u0026lt;s\u0026gt;xx\u0026lt;/s\u0026gt; lastVersion : 2021.04.26 hostpoolName : rozemuller-hostpool sessionHostName : avd-0.rozemuller.local Getting information The next step is gathering the virtual machine information of an existing session host. This is needed for getting the network information and virtual machine size. To achieve this goal we need the command below. Recommended is to save the command above into a variable so we can use it later.\n$parameters¬†=¬†@{ HostpoolName¬†=¬†\u0026#34;rozemuller-hostpool\u0026#34; ResourceGroupName¬†=¬†\u0026#34;rg-avd-001\u0026#34; } $sessionHostImageInfo¬†=¬†Get-AvdImageVersionStatus¬†@parameters¬†|¬†select¬†-Last¬†1 $resources¬†=¬†Get-AvdSessionHostResources¬†@parameters¬†-SessionHostName¬†$sessionHostImageInfo.sessionHostName Create a virtual machine profile Based on the returned information we are now able to create a virtual machine profile, with the correct disk size. A big difference between the portal, where the disk size is changed afterward, and the automation part.\nAs you can see in the example below I‚Äôm creating a new virtual machine with a 256 GB disk. Besides the credentials and virtual machine name, all other information is gathered from the existing Azure Virtual Desktop environment.\n$imageParameters¬†=¬†@{ GalleryName¬†=¬†$sessionHostImageInfo.imageGallery resourceGroupName¬†=¬†$sessionHostImageInfo.resourceGroupName Name¬†=¬†$sessionHostImageInfo.imageName } $galleryImageDefintion¬†=¬†Get-AzGalleryImageDefinition¬†@imageParameters $LocalAdminUser = \u0026#34;tempVmUser\u0026#34; $LocalAdminPass = \u0026#34;TempVmP@ssword123\u0026#34; $vmName = \u0026#34;UpdateDiskVm\u0026#34; $DiskSizeGB = 256 $LocalAdminSecurePassword = ConvertTo-SecureString $LocalAdminPass -AsPlainText -Force $Credentials = New-Object System.Management.Automation.PSCredential ($LocalAdminUser, $LocalAdminSecurePassword) $virtualNetworkSubnet = (Get-AzNetworkInterface -ResourceId $resources.NetworkProfile.NetworkInterfaces.id).IpConfigurations.subnet.id $SubnetConfig = Get-AzVirtualNetworkSubnetConfig -ResourceId $virtualNetworkSubnet $NIC = New-AzNetworkInterface -Name \u0026#34;$vmName-nic\u0026#34; -ResourceGroupName $resources.ResourceGroupName -Location $resources.Location -SubnetId $SubnetConfig.Id $VirtualMachine = New-AzVMConfig -VMName $vmName -VMSize $resources.HardwareProfile.VmSize $VirtualMachine = Set-AzVMOperatingSystem -VM $VirtualMachine -Windows -ComputerName $vmName -Credential $Credentials $VirtualMachine = Set-AzVMSourceImage -VM $VirtualMachine -Id $galleryImageDefintion.Id $VirtualMachine = Set-AzVMOSDisk -Windows -VM $VirtualMachine -CreateOption FromImage -DiskSizeInGB $DiskSizeGB $VirtualMachine = Add-AzVMNetworkInterface -VM $VirtualMachine -Id $NIC.Id New-AzVM -VM $VirtualMachine -ResourceGroupName $resources.ResourceGroupName -Location WestEurope Verify A few minutes later the virtual machine has been created with the new disk size. When looking under the hood you will notice the new virtual machine has been started from the image version out of the shared image gallery.\nGeneralize Before creating a new image version we first need to generalize the virtual machine. To run a command on the virtual machine I use the Invoke-AzRunCommand and I have created a simple PowerShell script which will be executed on the machine. The script can be downloaded from my GitHub. Download the file to your local machine from where you are able to send the saved PowerShell into the Invoke-AzRunCommand.\nGet-AzVM¬†-Name¬†$vmName¬†|¬†Invoke-AzVMRunCommand¬†-CommandId¬†\u0026#39;RunPowerShellScript\u0026#39;¬†-ScriptPath¬†.\\execute-sysprep.ps1 If you don‚Äôt like a file download then use the Azure CLI command az vm run-command. This command accepts script lines, like the PowerShell script block. https://docs.microsoft.com/en-us/cli/azure/vm/run-command?view=azure-cli-latest\naz¬†vm¬†run-command¬†invoke¬†--command-id¬†RunPowerShellScript¬†--name¬†$vmName¬†-g¬†$resources.resourceGroupName¬†--scripts¬†\u0026#39;$sysprep¬†=¬†\u0026#34;C:\\Windows\\System32\\Sysprep\\Sysprep.exe\u0026#34;¬†$arg¬†=¬†\u0026#34;/generalize¬†/oobe¬†/shutdown¬†/quiet¬†/mode:vm\u0026#34;\u0026#39;¬†\\¬†\u0026#39;Start-Process¬†-FilePath¬†$sysprep¬†-ArgumentList¬†$arg\u0026#39;¬†After a short time, the virtual machine has been shut down and we need to generalize the machine before we can create an image version. Before generalizing a virtual machine make sure the machine is stopped. The shutdown command in the Sysprep script will take care of that.\nTo make sure the machine is stopped I wrote a simple PowerShell loop that will check the virtual machine status.\nfunction¬†test-VMstatus($virtualMachineName)¬†{ $vmStatus¬†=¬†Get-AzVM¬†-name¬†$virtualMachineName¬†-Status return¬†\u0026#34;$virtualMachineName¬†status¬†\u0026#34;¬†+¬†$vmstatus.PowerState } do¬†{ $status¬†=¬†test-vmStatus¬†-virtualMachineName¬†$vmName $status Start-Sleep¬†10 }¬†until¬†(¬†$status¬†-match¬†\u0026#34;stopped\u0026#34;) Write-Output¬†\u0026#34;$vmName¬†has¬†status¬†$status\u0026#34; Get-AzVM¬†-Name¬†$vmName¬†|¬†Set-AzVm¬†-Generalized For more information about creating images please check my series about avd image management automated.\nFor a while, it is possible to create a new image version from a virtual machine directly. This will make it possible to skip making a snapshot so we are going to deploy a new image from the machine directly.\nCreate image version The final step is creating an image version in the Azure Compute Gallery. In this part, we are reusing the gallery variables from the steps above. Creating a new image could take a while and depends on things like disk type (SSD, HDD) and replication regions.\n$ImageParameters¬†=¬†@{ GalleryImageDefinitionName¬†=¬†$galleryImageDefintion.Name GalleryImageVersionName¬†=¬†(Get-Date¬†-format¬†\u0026#34;yyyy.MM.dd\u0026#34;) GalleryName¬†=¬†$sessionHostImageInfo.imageGallery ResourceGroupName¬†=¬†$sessionHostImageInfo.ResourceGroupName Location¬†=¬†\u0026#39;WestEurope\u0026#39; SourceImageId¬†=¬†(Get-AzVM¬†-Name¬†$vmName).id.ToString() } New-AzGalleryImageVersion¬†@ImageParameters After executing the command you will see a new version will be provisioned.\nIf you like to know the replication status you could click on the version to check the actual status.\nIn the end, you will be able to use this image for your Azure Virtual Desktop session hosts with an expanded disk.\nSources To get more information about the used resources please check to sources below.\nhttps://docs.microsoft.com/en-us/powershell/module/az.compute/set-azvmosdisk?view=azps-6.0.0 https://docs.microsoft.com/en-us/powershell/module/az.compute/new-azvm?view=azps-6.0.0 https://docs.microsoft.com/en-us/powershell/module/az.compute/invoke-azvmruncommand?view=azps-6.0.0 https://docs.microsoft.com/en-us/azure/virtual-machines/windows/capture-image-resource#create-an-image-of-a-vm-using-powershell Thank you for reading my blog change avd disk size based on the azure compute gallery image automated. I hope you got a bit inspired. Enjoy your day and happy automating üëã ","date":"May 31, 2021","image":"http://localhost:1313/change-avd-disk-size-based-on-a-shared-image-gallery-image-automated/floppy.png","permalink":"/change-avd-disk-size-based-on-a-shared-image-gallery-image-automated/","title":"Change AVD disk size based on the Azure Compute Gallery image automated"},{"categories":["Automation","Azure","Azure Virtual Desktop","Powershell"],"contents":"During my travel the past year in the world of AVD I noticed I‚Äôm using the common Az.DesktopVirtualization PowerShell module very often, but there are some limits. In basics they do their job but if you like more intelligence or add more resource types you will need to combine PowerShell commands to get useful information. That‚Äôs the point where I started writing a AVD PowerShell module and now it is time to share my functions as a fresh new module called Az.Avd.\nI‚Äôm really glad to announce that the new Az.Wvd has been published into the official PowerShell Gallery. Inspired by my colleague John de Jager (@johnde_jager on Twitter) I started writing a PowerShell module to maintain Windows Virtual Desktop environments.\nTable Of Contents Download locations Install Az.Avd Roadmap Final words Download locations The PowerShell Gallery and my GitHub page are the places to get the Az.Avd module and install from.\nIf you have registered the PSGallery repository you will be able to install the Az.Avd module directly from the PowerShell Gallery.\nTo register the PowerShell Gallery repository in your own environment run the command below.\nFor PowerShell 5+ you will need this command.\nRegister-PSRepository -Default -InstallationPolicy Trusted For older PowerShell environments use this command.\nRegister-PSRepository -Name PSGallery -SourceLocation https://www.powershellgallery.com/api/v2/ -InstallationPolicy Trusted After registering you will be able to test the repostory with the command below.\nPS C:\u0026gt; Get-PSRepository Name InstallationPolicy SourceLocation ---- ------------------ -------------- PSGallery Trusted https://www.powershellgallery.com/api/v2 Posh Test Gallery Trusted https://www.poshtestgallery.com/api/v2/ PS C:\u0026gt; If you like to update Powershell please check the Microsoft documentation about installing Powershell\nPowerShell Gallery link: https://www.powershellgallery.com/packages/Az.Avd GitHub link: https://github.com/srozemuller/AzWvd Install Az.Avd After registering the PSGallery repository you will be able to install the Az.Avd module from that location directly by running the command in PowerShell.\nInstall-Module -Name Az.Wvd The module is also downloadable from my GitHub repository. Clone the repository by executing the git clone command in PowerShell on the decided location, or download the package from the repository location. If you want to use the GitHub command make sure you have install the Git software first.\ngit clone https://github.com/srozemuller/AzWvd.git After cloning or unpacking the repository, go to the directory (in PowerShell) to import the module with the Import-Module command.\nRoadmap In this first version the main focus was getting information about a AVD environment. Knowing all the components, if there are are still used and if the hosts are running on a latest image version is the first step to get a stable environment with the lowest cost as possible.\nIn the upcoming months the focus will be on the deployment and housekeeping.\nFinal words I hope you will enjoy the Az.Avd PowerShell module and feel free to contribute at my GitHub page.\nAnd I will thank John for pointing me into this direction.\nThank you for reading my post about the launch of the Az.Avd module.\nThank you for reading my blog launching the first version of az.avd wvd powershell module. I hope you got a bit inspired. Enjoy your day and happy automating üëã ","date":"April 29, 2021","image":"http://localhost:1313/launching-the-first-version-of-az-avd-powershell-module/azavd-module.jpg","permalink":"/launching-the-first-version-of-az-avd-powershell-module/","title":"Launching the first version of Az.Avd WVD PowerShell module"},{"categories":["Azure Virtual Desktop","Azure AD"],"contents":"An Azure Virtual Desktop environment has two types of host pools, a pooled and a personal type. The main difference between them is the user assignment. In this article I will show how to deal with personal assigned session hosts and how to delete the assigned user and will save work.\nIntroduction At some day we all run to it. You are logging in to a AVD environment, through the web or client, and like to start a session. After logging in you noticed this is the wrong user, an admin user or a test user for example. Or someone is leaving the organisation or get a new user account. These are things which happens all the time and that‚Äôs OK :).\nIn case of an AVD pooled environment it is still fine, you will logout your session. On a AVD personal environment it is a bit uncomfortable because the session host is claimed by a user and can not be used by someone else.\nIn this article I will explain how to delete an assigned user from a personal AVD session host. This will save removing and adding session hosts.\nGood to know is that in the portal there is no option to remove a user from a session host and have to be done manually, at least till now.\nTable Of Contents Introduction Host pool types explained Pooled Delete assigned from personal session host Generate host pool token Host pool types explained As mentioned a AVD environment has two types of host pools, a pooled and a personal type.\nThe main difference is how the session hosts are used.\nPooled A pooled environment has several users are using the same session host. When selecting the pooled type the portal will give you an extra option, Load balancing algorithm, where to select how to balance users over the session hosts.\nIn a Breath-First situation the environment will added session like the old round-robin way and will add session equal over all the session hosts.\nIn a Depth-First situation every new session is assigned to next available session host which has not reached its limit.\nFor example:\nYou have 3 session hosts with a max session limit of 10. The first session host has 8 sessions, the second has 4 and the last has 7. In a breath-first the next session is assigned to the second host (with 4 sessions). In case of depth-first the first session host (with 8 sessions) will be used.\n![wvd pooled hostpool type](https://www.rozemuller.com/wp-content/uploads/2021/04/image-7.png)### Personal In a personal environment there is no load balancing. Every user has its own session host. The only thing which can be configured is the assignment type, automatically or direct. There could be reasons to choose the direct option but most of the time the automatically option will good.\n![wvd hostpool types](https://www.rozemuller.com/wp-content/uploads/2021/04/image-6.png)More information about host pool load balancing you can check the [Microsoft documentation about these methods](https://docs.microsoft.com/en-us/azure/virtual-desktop/host-pool-load-balancing). What happens if a user logs in, an available session host will be assigned to that user and will keep it as long as the session host exists. A great advantage in relation to a pooled environment is that you are able enable start VM on connect and shutdown the session hosts at the end of the day. This will save consumption and so costs.\n![](https://www.rozemuller.com/wp-content/uploads/2021/04/image-9-1024x87.png)An disadvantage is that when a user is leaving, needs a new login name or in my case, I logged in with a wrong test user, the session host is claimed by that user account. Of course you can delete that session host and create a new one but some side effects are you will have to clean up your Active Directory and remove all related resources as well. So I like to remove that user from the host without creating a new VM and all other. Delete assigned from personal session host Now it is time to remove an assigned user from a session host. What is happening in the basic is that we will re-register the session host into the host pool. In the first step we will create a new host pool token.\nGenerate host pool token Before we are able to register session hosts into a host pool we have to create a new host pool token. This is a global unique id which represents the host pool as long the token is available. This token will be used by the RDInfra agent which will search in the Azure cloud for a host pool with this token and then will add the session host to the host pool.\nThere are several way to create a new token. Through the portal under the host pool overview.\n![](https://www.rozemuller.com/wp-content/uploads/2021/04/image-11.png)Or by PowerShell via the command below. (I will generate a new token for 30 minutes in this case). I will save the token into a $token variable to use it later. $now¬†=¬†get-date $Parameters¬†=¬†@{ HostpoolName¬†=¬†\u0026#39;hostpool-intune-personal\u0026#39; ResourceGroupName¬†=¬†\u0026#39;rg-wvd-001\u0026#39;¬†} $token = New-AzWvdRegistrationInfo¬†@Parameters -ExpirationTime¬†$now.AddMinutes(30) ### Finding the user‚Äôs session host The next step is to find the session host with the user which need to be removed. This can be done by executing the Get-AzWvdSessionHost command and will result in the session host information.\n$sessionHost = Get-AzWvdSessionHost @Parameters | where {$_.AssignedUser -eq \u0026#39;useraccount\u0026#39;} | FL ![](https://www.rozemuller.com/wp-content/uploads/2021/04/image-10.png)Important is that the session host needs to be removed from the host pool. (Not the VM it self) Execute the command below to remove the session host. $sessionHost | Remove-AzWvdSessionHost ### Change registry values Now we have a new token and the correct session host it is time to re-register the host. To achieve that goal there are also some options. Option 1 is re-installing the RDInfra agent. When installing the agent you will be asked for the new token.\nIn my case I decided to change the needed registry keys so the agent will register after a reboot.\nBecause I like the automated way I will execute some commands remotely on the virtual machine with the Invoke-AzRunCommand PowerShell command.\nBeforce running the Invoke command we need the correct virtual machine name. In the step before we found the correct session host. Each session host has a resourceId, don‚Äôt get confused by the Id, which can be used to find the Azure resource with the Get-AzResource command. After finding the correct resource we will have a name and a resource group name.\nThe $token.token value is the value from the generate host pool token part.\n$vm = Get-AzResource -ResourceId $sessionHost.ResourceId Invoke-AzVMRunCommand -ResourceGroupName $vm.ResourceGroupName -VMName $vm.Name -CommandId \u0026#39;RunPowerShellScript\u0026#39; -ScriptPath .\\delete-wvdassigneduser.ps1 -Parameter @{HostpoolToken = $token.token} An Invoke-AzVmRunCommand in combination with the commandId ‚ÄòRunPowerShellScript‚Äô will need a PowerShell script. See the script I used below. This script will be downloaded by the VM and executed on the VM. As you can see the script accepts the host pool token. [CmdletBinding()] param ( [Parameter(Mandatory = $true)] [ValidateNotNullOrEmpty()] [string]$HostpoolToken ) Set-ItemProperty -Path \u0026#34;HKLM:\\Software\\Microsoft\\RDInfraAgent\u0026#34; -Name \u0026#34;RegistrationToken\u0026#34; -Value $HostpoolToken Set-ItemProperty -Path \u0026#34;HKLM:\\Software\\Microsoft\\RDInfraAgent\u0026#34; -Name \u0026#34;IsRegistered\u0026#34; -Value 0 ### Restart session host The final step is restarting the VM\nGet-AzVM -Name $vm.Name | Restart-AzVm The final result is a clean session host for a new user.\n![](https://www.rozemuller.com/wp-content/uploads/2021/04/image-12.png)![](https://www.rozemuller.com/wp-content/uploads/2021/04/image-13.png) The change-sessionhost-token.ps1 script can be find at my [Github repository](https://github.com/srozemuller/Windows-Virtual-Desktop/blob/master/Recycle/UserAssignment/change-sessionhost-token.ps1) Thank you for reading my blog post about delete an assign user from a personal Windows Virtual Desktop session host.\n","date":"April 19, 2021","image":"http://localhost:1313/delete-assigned-user-from-a-personal-avd-session-host-automated/avd-recycle-scaled.jpg","permalink":"/delete-assigned-user-from-a-personal-avd-session-host-automated/","title":"Delete assigned user from a personal AVD session host automated"},{"categories":["Azure Virtual Desktop"],"contents":"Costs are a topic of an Azure conversation very often. As we all know you pay for usage, if it is storage, CPU or bandwidth usage, (almost) every resource has its price. For Windows Virtual Desktop it means you have to pay for CPU usage, for example. It is important then to keep this usage as low as possible. Now there is an option to keep these usage as low as possible with AVD Start VM on Connect.\nIntroduction At the end of March (31-03-2021) Microsoft announced the AVD Start VM on Connect is in Public Preview for personal AVD environments. This means in short you are able to shutdown your AVD session, and so the VM. To keep costs as low as possible, make sure the VM‚Äôs are deallocated. Shutting down a VM from Windows isn‚Äôt enough. In this article I will show how to configure AVD Start VM on Connect fully automated with one single login via REST API.\nThe shutdown process is out of scope in the article. I‚Äôm working on a new article about the whole shutdown process.\nSee the official announcement: https://techcommunity.microsoft.com/t5/windows-virtual-desktop/start-vm-on-connect-starts-public-preview/m-p/2247357\nUPDATE: On 12 May Microsoft announced this feature is now also available for pooled AVD environments.\nSee the office announcement: https://techcommunity.microsoft.com/t5/windows-virtual-desktop/leverage-start-vm-on-connect-for-pooled-host-pools-and-azure-gov/m-p/2349866\nTable Of Contents Introduction Requirements and limitations Considerations Windows Virtual Desktop Enable AVD Start VM on Connect Active Directory Authentication Service Principal Create custom role Role assignment Testing Requirements and limitations Good to know is that this feature is in public preview. This means it is for testing purposes only.\nOther things to keep in mind are the following:\nFrom the office announcement:\nYou can configure the setting on validation pool only This setting is for personal host pools only. Microsoft announced it is working for personal as for pooled AVD host pools. This setting can be accessed from PowerShell and RestAPI only. From the docs:\nThe Windows client (version 1.2748 or later) Check the documentation for all the information.\nAt last the AVD Start VM on Connect can only be enabled on existing AVD hostpools. In case of a new environment you will have to deploy a hostpool first and then configure this option afterwards.\nConsiderations Before we start to code we have some considerations about the code strategy. Azure AD differs from Azure with it resources. As the documentation says we have to create and assign Azure AD roles to the ‚ÄòWindows Virtual Desktop‚Äô service principal and we have to update the AVD hostpool, which is Azure.\nThis means when automating with PowerShell we have to connect to two different environments, the Azure AD and Azure. In PowerShell you will have to install an additional module AzureADPreview. To connect to the Azure AD use the command.\nConnect-AzureAD Later you will have to connect to Azure with the command below.\nConnect-AzAccount In the world of automation you will have less interactions as possible, which means you want to connect only once if possible. I will explain how to deal with that.\nWindows Virtual Desktop As you maybe know a AVD hostpool has two types of hostpools, personal and pooled. A personal hostpool means every user has it‚Äôs own sessionhost, where in a pooled type you will have to share a sessionhost with other users.\nEnable AVD Start VM on Connect The first and most important requirement is you will need to have a personal hostpool type and it have to be set as a validation environment. In the script is it the first thing which will be checked.\nEnabling AVD Start VM on Connect is only possible with PowerShell or the REST API.\n$HostpoolName = \u0026#39;avd-hostpool-personal-001\u0026#39; $ResourceGroupName = \u0026#39;rg-avd-001\u0026#39; try { $HostpoolParameters = @{ HostPoolName = $HostpoolName ResourceGroupName = $ResourceGroupName } $Hostpool = Get-AzWvdHostPool @HostpoolParameters if ($Hostpool.ValidationEnvironment -eq $true) { Update-AzWvdHostPool @HostpoolParameters -StartVMOnConnect:$true Write-Verbose \u0026#34;Hostpool updated, StartVMOnConnect is set\u0026#34; } } catch { Throw \u0026#34;The hostpooltype for provided hostpool $Hostpoolname must be a check as a validation environment\u0026#34; } If the check is passed the statement immediately will enable Start VM on Connect on the AVD hostpool.\nThat was the most simple part. Now it is time to prepare the Azure Active Directory\nActive Directory To make things work you have to configure the correct permissions in the Azure AD. This means that you have to create and assign a new role to the Windows Virtual Desktop principal. These permissions are needed to start the virtual machines.\n‚ÄúMicrosoft.Compute/virtualMachines/start/action‚Äù,\n‚ÄúMicrosoft.Compute/virtualMachines/read‚Äù\nAuthentication The main consideration is about the authentication. As mentioned I will like to authenticate as less as possible.\nThere is some nuance about authentication. In the script I need to authenticate several times to the different parts of the REST API but I only have to authenticate once manual. To achieve my goal I have created a function for that.\nfunction GetAuthToken($resource) { $context = [Microsoft.Azure.Commands.Common.Authentication.Abstractions.AzureRmProfileProvider]::Instance.Profile.DefaultContext $Token = [Microsoft.Azure.Commands.Common.Authentication.AzureSession]::Instance.AuthenticationFactory.Authenticate($context.Account, $context.Environment, $context.Tenant.Id.ToString(), $null, [Microsoft.Azure.Commands.Common.Authentication.ShowDialog]::Never, $null, $resource).AccessToken $authHeader = @{ \u0026#39;Content-Type\u0026#39; = \u0026#39;application/json\u0026#39; Authorization = \u0026#39;Bearer \u0026#39; + $Token } return $authHeader } The function will use the current context (from Connect-AzAccount) to authenticate to the Graph API.\nService Principal In the first step we need to get information about the Windows Virtual Desktop service principal. This is the principal which will get permissions to start the virtual machines. The service principal already exists if a AVD hostpool is created. The information we need is the service principal ID. In some cases there are two of them, then assign the new created role to both of them. The script will take care of that.\nIn the first part I will authenticate to the Graph API and will request the service principals from the Azure AD.\n$GraphResource = \u0026#34;https://graph.microsoft.com\u0026#34; $GraphHeader = GetAuthToken -resource $GraphResource $ServicePrincipalURL = \u0026#34;$($GraphResource)/beta/servicePrincipals?`$filter=displayName eq \u0026#39;Windows Virtual Desktop\u0026#39;\u0026#34; $ServicePrincipals = Invoke-RestMethod -Method GET -Uri $ServicePrincipalURL -Headers $GraphHeader In my case the code above has two results.\nCreate custom role The next step is creating a new role which allows the principal to manage the virtual machines. First I determine in which resource group the session hosts are. For security reasons I will use this resource group to scope the permissions to. (In the docs the scope is set at subscription level).\nIn the next step I will automatically authenticate to the Azure API.\n$ScopeResourceGroup = Get-AzResource -ResourceID (Get-AzWvdSessionHost @HostpoolParameters | Select -Last 1).Id $Scope = \u0026#34;subscriptions/$($context.Subscription.Id)/Resourcegroups/$($ScopeResourceGroup.ResourceGroupName)\u0026#34; $AzureResource = \u0026#34;https://management.azure.com\u0026#34; $AzureHeader = GetAuthToken -resource $AzureResource First I will create a random GUID which it needed to create an unique role based on the Id. The roleName and description doesn‚Äôt need any further introduction. The assignableScopes part is where to scope this role too.\nThe permissions are set to virtualMachines start en read.\nAt last the whole body is converted to JSON and will be send to Azure API. Good to know is that the PUT method creates and updates the role. If you need to edit the role afterwards just search for this role, modify the settings and run a PUT request again.\n#Region create custom role # Building a new role GUID $RoleGuid = (New-Guid).Guid # Generating the role body $RoleBody = @{ name = $RoleGuid properties = @{ roleName = \u0026#34;AVD Start VM on connect\u0026#34; description = \u0026#34;This role is used to start VM when connecting\u0026#34; assignableScopes = @( $Scope ) permissions = @( @{ actions = @( \u0026#34;Microsoft.Compute/virtualMachines/start/action\u0026#34;, \u0026#34;Microsoft.Compute/virtualMachines/read\u0026#34; ) notActions = @() dataActions = @() notDataActions = @() } ) } } $RoleJsonBody = $RoleBody | ConvertTo-Json -Depth 5 $DefinitionUrl = \u0026#34;$($AzureResource)/$Scope/providers/Microsoft.Authorization/roleDefinitions/$($RoleGuid)?api-version=2018-07-01\u0026#34; $CustomRole = Invoke-RestMethod -Method PUT -Body $RoleJsonBody -Headers $AzureHeader -URi $DefinitionUrl Role assignment Now we have the Windows Virtual Desktop service principal id (or two) and a custom role it is time to assign this custom role to the service principal and add it to the resource group.\nThe assignment API works the same as the role definition API. You will have to create a unique Id first.\nThe assign body is where the custom role resource Id and the principal come together. The assign URL tells where to assign the couple to. In this case the scope is the resource group.\n$ServicePrincipals.value.id | foreach { $AssignGuid = (New-Guid).Guid $AssignURL = \u0026#34;$AzureResource/$Scope/providers/Microsoft.Authorization/roleAssignments/$($AssignGuid)?api-version=2015-07-01\u0026#34; $assignBody = @{ properties = @{ roleDefinitionId = $CustomRole.id principalId = $_ } } $JsonBody = $assignBody | ConvertTo-Json Invoke-RestMethod -Method PUT -Uri $AssignURL -Headers $AzureHeader -Body $JsonBody } At the end I‚Äôve created, assigned and enabled StartVMOnConnect by simple providing a AVD hostpool and its resource group.\nTesting I tested the following scenario‚Äôs:\nShutdown session host with Windows (VM is not deallocated) Stopped the VM with the Stop-AzVM PowerShell command. (VM is deallocated) In both scenario‚Äôs the VM came back.\nThank you for reading my post about enabling AVD Start VM on Connect fully automated :).\nThank you for reading my blog configure avd start vm on connect automated with role assignments and graph api. I hope you got a bit inspired. Enjoy your day and happy automating üëã ","date":"April 1, 2021","image":"http://localhost:1313/configure-avd-start-vm-on-connect-automated-with-role-assignments-and-graph-api/connecting-vm-scaled.jpg","permalink":"/configure-avd-start-vm-on-connect-automated-with-role-assignments-and-graph-api/","title":"Configure AVD Start VM on Connect automated with role assignments and Graph API"},{"categories":["Azure Virtual Desktop","DevOps"],"contents":"As mentioned in earlier posts Azure DevOps is a really nice way to deploy Azure resources automated. During my travel through Azure DevOps for managing Windows Virtual Desktop resources I moved from classic pipelines and releases to the new YAML pipelines. Using YAML has a lot of advantages in relation to classic. But there are some new challenges coming, approval or manual interventions for example. In this article I will explain how to use approvals and checks with dynamic recipients in DevOps environments within a YAML configuration.\nTable Of Contents Introduction Windows Virtual Desktop Image management Azure DevOps components Environments Job type Approval and checks Azure Resources Azure Functions Azure Logic App Microsoft Teams Azure Pipelines with Microsoft Teams Adaptive Cards Connecting the dots Stage configuration DevOps Environment Function App Logic App Response Troubleshooting Used Resources Introduction Azure DevOps is a service which helps you streaming projects, storing code and creating automated deployment sequences. For deploying an AVD environment Azure DevOps is a really good place to start if you like to automate deployments and install applications. Sometimes you just want to stop the sequence for a while to do some manual actions.\nFor example:\nYou need manual installation A 3rd party which need to install software by itself You have a DTAP (Developement, Testing, Acceptanc, Production) environment which need manual intervention between stages No special thing so far, Azure DevOps has manual interventions between stages and environments. But what if you don‚Äôt know where to send manual intervention messages at the first place before a task is started.\nFor example when a user starts a pipeline for creating a new image which needs some manual intervention afterwards. That is the part which Azure DevOps environments can‚Äôt handle. When configuring an Azure DevOps environments approvals needs an existing recipient from the Azure AD.\nIn this blog post I will zoom into that part and how to deal with dynamic recipients in a DevOps environments you just don‚Äôt know at the start or even don‚Äôt exists in the Azure AD at all.\nWindows Virtual Desktop Image management In this chapter I will explain the process at top level. This will help you understanding the different types of interventions and why. In the chapters thereafter I will explain the used components and to configure and connect them to each other.\nIn the illustration below I draw the main AVD image management process.\nAt the start you have a current production image which you like to update. You will start an Azure virtual machine from that image and will install the applications. The first intervention is after install applications. Most of the time an internal user is working on that image. After the applications are installed the pipeline will continue with the next steps. After generalizing the new image is ready. That is the next manual intervention point. This approval can be also an internal user or a customer which should test the new image.\nThe main challenge in this process is the unknown approvers combined with a YAML pipeline and environments. Where in the classic pipeline a manual intervention task is available, it is recommended you should use a deployment job with environments in a YAML pipeline.\nIn the classic task you are able to put a variable into the recipient field. In an environment you have configure approvals and select an existing identity from the AD first which will be used in a pipeline.\nAzure DevOps components In this first part I will explain the needed DevOps components in a nutshell with the image process in mind.\nEnvironments The first component are environments. This is a collection of resources like virtual machine which can be targeted to deploy to from a pipeline. There are several extra options which can be configured before deploying an environment. It is also possible to configure these options without selecting resources.\nJob type Each type of jobs has its own options. A default job consists of tasks and steps, where a template points to a other YAML pipeline file. To target to an environment you have to configure deployment jobs in the YAML pipeline.\nWhen creating jobs in a YAML pipeline you have several types of jobs. (job, template or deployment)\nA deployment pipeline can look like below where the pipeline deploys to a test environment. From the steps part the default tasks will be configured.\njobs: -¬†deployment:¬†TestDeployJob environment:¬†TestEnvironment strategy: runOnce: deploy: steps: See the Microsoft documentation for more information about environments.\nApproval and checks Withing the approval and checks there are a lot of options which can be used.\nPipelines rely on resources such as environments, service connections, agent pools, variable groups, and secure files. Checks enable the resource owner to control if and when a stage in any pipeline can consume a resource. As an owner of a resource, you can define checks that must be satisfied before a stage consuming that resource can start. For example, a manual approval check on an environment would ensure that deployment to that environment only happens after the designated user(s) has reviewed the changes being deployed.\nMore information about approvals please check define approval and checks.\nAzure Resources To setup a dynamic approval the basic approval will not fill in the needs. This because of a basis approval needs an existing AD identity as an approver and will not accept variables. With the first introduction example in mind this means you have to setup an environment for every user and you will have to create one every time a new user comes in. From management perspective you have a lot of housekeeping.\nAzure Functions Because of it can be triggered from outside the function through an URL I use an Azure Function. There is also an option called Invoke REST API which will need a service connection with the correct permissions. To keep as less components as needed I decided to choose functions.\nAzure Logic App An another resource I used is an Azure Logic App. An Logic App has really nice preconfigured actions, accepts HTTP requests and has a way to authenticate to Microsoft Teams. With a few simple steps I‚Äôm able to send messages, to Teams in my scenario.\nMicrosoft Teams The past year the usage of Microsoft Teams has been grown exceptionally and there is no day I‚Äôm not using it. Because of that and its accessibility for every kind of user Microsoft Teams is a really good addition to your pipeline process.\nSo I decided to extend my DevOps pipelines to Microsoft Teams.\nAzure Pipelines with Microsoft Teams The first thing was to install the Azure Pipeline application into a Teams channel. This application is subscribed to projects or pipeline and follows every step. If an intervention is there the application will show a message like below.\nCheck the link for more information about the installation and the download location: https://docs.microsoft.com/en-us/azure/devops/pipelines/integrations/microsoft-teams?view=azure-devops\nAdaptive Cards Adaptive cards is a open card format which is possible to format in every way you like. It will allow you to exchange UI content across many applications. An adaptive card consist of JSON code. Luckily you don‚Äôt have to figure it out by yourself :). For creating cards you can take a look at: https://adaptivecards.io/designer/\nIn the logic app I‚Äôm using the Post an Adaptive Card to a Teams user and wait for a response action where the message contains the adaptive card JSON body.\nConnecting the dots The illustration below represents the process at top level. It start with the pipeline, of course ;). When the pipeline arrived at the manual intervention stage the deployment job will trigger the environment.\nStage configuration As mentioned earlier in this post you will need a deployment job type to ‚Äòtrigger‚Äô an environment (TestEnvironment in this scenario). If the approval and checks are successful then the tasks within the job will be executed.\njobs: -¬†deployment:¬†TestDeployJob environment:¬†TestEnvironment strategy: runOnce: deploy: steps: DevOps Environment I just created an empty environment name TestEnvironment. After creation I clicked the 3 dots to configure Approval and checks.\nIn the next screen click the ‚Äòsee all‚Äô link to show all the options. From the list I selected the Invoke Azure Function.\nTo send messages to dynamic users you will need an another option, Invoke Azure Function. This option allows you to send DevOps variables into the function. From there you are free to do anything with it. The configuration will looks like below.\nThe Azure Function URL is the first part without the authentication key. The key needs to be filled into the next field.\nI left the headers part untouched. In the body you are able to put text and variables in JSON format. There are a lot of predefined variables which can be used. It is also possible to send your own created variables, in that case you will have to put the variables into a variable group.\nThe body has the following content, where the keys are just text and the values are variables from DevOps. The Build. and System. are predefined variables. The other variable came from the variable group. (I created a task in the DevOps pipeline which will update the variable group content with the new content)\n{ \u0026#34;BuildRequestedBy\u0026#34; : \u0026#34;$(Build.RequestedFor)\u0026#34;, \u0026#34;BuildRequestedForEmail\u0026#34; : \u0026#34;$(Build.RequestedForEmail)\u0026#34;, \u0026#34;Status\u0026#34; : \u0026#34;Ready for manual actions\u0026#34;, \u0026#34;TeamProject\u0026#34; : \u0026#34;$(System.TeamProject)\u0026#34;, \u0026#34;BuildNumber\u0026#34; : \u0026#34;$(Build.BuildNumber)\u0026#34;, \u0026#34;DomainToJoin\u0026#34; : \u0026#34;$(DomainToJoin)\u0026#34;, \u0026#34;VMadminUsername\u0026#34; : \u0026#34;$(VMadminUsername)\u0026#34;, \u0026#34;VMadminPassword\u0026#34; : \u0026#34;$(VMadminPassword)\u0026#34;, \u0026#34;VirtualMachineName\u0026#34; : \u0026#34;$(VirtualMachineName)\u0026#34;, \u0026#34;PublicIp\u0026#34;: \u0026#34;$(PublicIp)\u0026#34; } At last make sure you have set the ApiResponse selected. This option will wait till the Logic App responded.\nFunction App Now the data is out of DevOps you will be able to send these data to anyone through mail by example (SendGrid). I decided to put the data to a Logic App. The function itself isn‚Äôt very complex. The first step is creating variables from the request input body.\nFor example the $Request.Body.BuildRequestedForEmail is the ‚ÄúBuildRequestedForEmail‚Äù key from the JSON code above. The BuildRequestedForEmail is the email of the user which has started the pipeline. Now the function app has this email you you can handle dynamic recipients from Devops environments.\nAfter creating all the variables I will format the body which will be send to the Logic App.\n$LogicAppUri = \u0026#34;url to Logic App\u0026#34; # These are the variables from the DevOps environment $Email = $Request.Body.BuildRequestedForEmail $Status = $Request.Body.Status $ProjectName = $Request.Body.TeamProject $BuildNumber = $Request.Body.BuildNumber $VMadminUsername = $Request.Body.VMadminUsername $VMadminPassword = $Request.Body.VMadminPassword $VirtualMachineName = $Request.Body.VirtualMachineName $PublicIp = $Request.Body.PublicIp $JsonBody = @{ account = @{ name = $Email } text = @{ Message = \u0026#34;Job $BuildNumber has status $Status.\u0026#34; Title = \u0026#34;Update from $ProjectName\u0026#34; VmName = $VirtualMachineName VmUserName = $VMadminUsername VmPassword = $VMadminPassword VmPublicIp = $PublicIp } } $Body = $JsonBody | Convertto-Json -depth 2 $Headers = @{ \u0026#39;Content-Type\u0026#39; = \u0026#39;application/json\u0026#39; } $Request = invoke-webrequest -URI $LogicAppUri -Body $Body -Method \u0026#39;Post\u0026#39; -Headers $Headers The full function app code I used can be found at my Github repository.\nLogic App The Azure Pipeline application is a great addition but for some scenario‚Äôs I don‚Äôt want to send messages to a complete Teams channel. That‚Äôs the point I decided to create a Logic App as well. Now I have dynamic data I‚Äôm able to send Teams messages to everyone I like.\nLogic App has a really nice integration with Microsoft Teams and a lot of predefined actions. They are still in preview but working quite well.\nIn the basic the Logic App has and HTTP Request, a post adaptive card and a response action.\nThe HTTP Request action will generate an unique URL which you need in the Function App. This is the public URL where the Logic App is listening. In the Request Body JSON Schema you tell the Logic App how a incoming request look like. This is the same as the body which will be send from the Function App.\n{ \u0026#34;properties\u0026#34;:¬†{ \u0026#34;account\u0026#34;:¬†{ \u0026#34;properties\u0026#34;:¬†{ \u0026#34;name\u0026#34;:¬†{ \u0026#34;type\u0026#34;:¬†\u0026#34;string\u0026#34; } }, \u0026#34;type\u0026#34;:¬†\u0026#34;object\u0026#34; }, \u0026#34;text\u0026#34;:¬†{ \u0026#34;properties\u0026#34;:¬†{ \u0026#34;Message\u0026#34;:¬†{ \u0026#34;type\u0026#34;:¬†\u0026#34;string\u0026#34; }, \u0026#34;Title\u0026#34;:¬†{ \u0026#34;type\u0026#34;:¬†\u0026#34;string\u0026#34; }, \u0026#34;VmName\u0026#34;:¬†{ \u0026#34;type\u0026#34;:¬†\u0026#34;string\u0026#34; }, \u0026#34;VmPassword\u0026#34;:¬†{ \u0026#34;type\u0026#34;:¬†\u0026#34;string\u0026#34; }, \u0026#34;VmPublicIp\u0026#34;:¬†{ \u0026#34;type\u0026#34;:¬†\u0026#34;string\u0026#34; }, \u0026#34;VmUserName\u0026#34;:¬†{ \u0026#34;type\u0026#34;:¬†\u0026#34;string\u0026#34; } }, \u0026#34;type\u0026#34;:¬†\u0026#34;object\u0026#34; } }, \u0026#34;type\u0026#34;:¬†\u0026#34;object\u0026#34; } The next action is the adaptive card action. It is a quite simple action which need two inputs. A recipient and a message. By adding the JSON code in the message body, Teams will generate a card from that code.\nCheck the adaptive card design page (https://adaptivecards.io/designer/) for creating an adaptive card JSON body.\nBecause of the request output from the step above is generated you will be able to select that output in the next step.\nIn the rest of the JSON body you are able to add dynamic content as well. This is also the output from the previous action.\nThe card will allow you to send back a response with a submit button. To make sure you have responded the card needs to be updated. Make sure to add extra parameters for updating the card in Teams.\nResponse The response needs a status code which will returned to the request (in this scenario the function app). A status code must be an HTTP statuscode like 202 (Accepted) or 400 (Bad Request). A status code is a code which will be returned by an action button in the adaptive card setup.\nIn this example I have a submit button with a OK value which will return 202. This status code will be returned to the function which will send this OK status to DevOps.\nThis is the most important part of the sequence. If the response is OK, the pipeline will continue.\n\u0026#34;actions\u0026#34;:¬†[ { \u0026#34;type\u0026#34;:¬†\u0026#34;Action.Submit\u0026#34;, \u0026#34;title\u0026#34;:¬†\u0026#34;OK\u0026#34;, \u0026#34;id\u0026#34;:¬†202 } ], Statuscode expression: outputs(‚ÄòPost_an_Adaptive_Card_to_a_Teams_user_and_wait_for_a_response‚Äô)[‚ÄòBody/SubmitActionId‚Äô]\nThe adaptive card code I used can be found at my Github repository.\nNow you are able to use dynamic recipients in DevOps environments. At the end a personal message from DevOps could be like this.\nTroubleshooting The most tricky part is the data handling in the Logic App. Especially in the beginning a lot went wrong. To troubleshoot some things I used (and still do) the Logic App run history. When navigating to the app at the main page you see the latest runs which tell you a lot what is going on in the app.\nTo find out the correct expression for the status code just open the response task and see what was the input.\nFor example the response code from the adaptive card must be a HTTP status code.\nI fixed it by adding an ID to the button which will be returned, otherwise the title is the response.\nUsed Resources Summary of the links I posted throughout the article.\nhttps://www.rozemuller.com/save-avd-image-with-sysprep-as-image-gallery-version/#overview https://docs.microsoft.com/en-us/azure/devops/pipelines/process/environments?view=azure-devops https://docs.microsoft.com/en-us/azure/devops/pipelines/process/approvals?view=azure-devops\u0026amp;tabs=check-pass https://adaptivecards.io/designer/ Thank you for reading my post about configuring dynamic recipients in DevOps environments :). Thank you for reading my blog how to configure dynamic recipients in devops environments. I hope you got a bit inspired. Enjoy your day and happy automating üëã ","date":"March 24, 2021","image":"http://localhost:1313/how-to-configure-dynamic-recipients-in-devops-environments/ms-teams-laptop-scaled.jpg","permalink":"/how-to-configure-dynamic-recipients-in-devops-environments/","title":"How to configure dynamic recipients in DevOps environments"},{"categories":["Azure Virtual Desktop","DevOps"],"contents":"Using Azure DevOps is a really nice way to deploy resources in Azure, so also for Windows Virtual Desktop. Before you are able to deploy resources into Azure with pipelines you will need to setup a project and a service connection first. In post I will explain how to create a DevOps Service Connection the automated way.\nIntroduction In this series about Prepare Azure DevOps for Windows Virtual Desktop deployment I will post a few small blogposts which will help you setting up a AVD prepared DevOps environment, fully automated. At the end of this series you will able to create a script which let you fill in an application name, a projectname and a PAT code for connecting to DevOps and will run all the needed steps to start with DevOps.\nThis series consists of the following subjects:\nApp registration in Azure Active Directory Create an Azure DevOps project Add a Service connection in the DevOps project Create a pipeline from a source project Using environments for manual image action dynamically At the start of this blog post you have created the following resources.\nAn Azure AD Service principal, with deployment permissions A DevOps organisation with a project Table Of Contents Introduction What is a service connection Manual configuration Types Methods Scopes Authentication Management group Subscription Automated configuration REST API Body Send request Response Creating custom body Body explained How to use Now it is time to create a DevOps service connection automated. Before creating a service connection it is good to know the basics.\nWhat is a service connection Service connections enable you to connect to customer tenants to execute tasks in a job. For example, you may need to connect to your Microsoft Azure subscription, to a different build server or file server, to an online continuous integration environment, or to services you install on remote computers.\nIt‚Äôs possible to define service connections in Azure Pipelines that are available for use in all your tasks. For example, you can create a service connection for your Azure subscription and use this service connection name in an Azure Web Site Deployment task in a release pipeline.\nIn this case we are using the service connection for deploying AVD resources like a AVD hostpool, NSG, sessionhosts, etc.\nMore info about service connections please check: https://docs.microsoft.com/en-us/azure/devops/pipelines/library/service-endpoints?view=azure-devops\u0026amp;tabs=yaml\nManual configuration Creating a service connection in the portal is very simple. Under Project Settings -\u0026gt; Pipelines -\u0026gt; Service connections you are able to create a new service connection.\nTypes Service connections consists of many different types, for example Azure Resource Manager, GitHub or Jenkins. When creating service connections you will need to know which resource type you want to connect to and which method you want to use. At last you need to choose on which scope you want to setup the connection.\nIn this post we will use the Azure Resource Manager. The reason why using the Azure Resource Manager type seems pretty clear since we like to deploy resources to Azure.\nMethods Scopes Choosing the right scope depends on the situation. In this post I will handle two of them, subscription and management group.\nChoosing the right scope level depends on how the tenant permissions are configured. If you are using management groups with permissions set you should select that one, otherwise you should select subscription level. If you have many subscriptions in one tenant you should consider creating management groups.\nMore about management groups check: https://docs.microsoft.com/nl-nl/azure/governance/management-groups/overview\nAuthentication Under authentication you can fill in the just created service principal with the secret and tenant id. By clicking the verify button the connection will be tested.\nManagement group If you selected the management group type make sure that the service principal has at least reader access to the management group. Go to management groups in the portal, click the management group and then click detail. Then choose Access Control (IAM) and set the correct permissions.\nSubscription If you like to connect to a subscription make sure your principal has the needed permissions on the subscription. Choose then the IAM settings at subscription level.\nAutomated configuration Although it is simple to create a service connection via the portal there are several reasons using an automated process for creating connections. Beside it is just fun exploring a new world it is very useful when you are able to prepare a complete DevOps environment with just one click. Let me explain how to create a DevOps service connection automated.\nREST API For the automated configuration we are going to use the DevOps API‚Äôs again.\nAs you can read in the previous post I‚Äôm using the API at organisation level and will create a project. In this post we need to go into the project and create a service connection. As far as I know this is the only way to create a service connection automated.\nIn the script we will call the API two times to achieve this goal. The first call to get the projectID, because we needed in the API call body. The second time to create a service connection at project level.\nBody With the manual configuration in mind we now know there are different Azure service connection methods, their different types and scopes. In this scenario we need to deploy Azure resources based on a service principal on a management group scope. The service principal was created at the first part of this series.\nAfter some research I was able to map the portal values to the API body values.\nThe way I used was creating a manual service connection first, after creation I did a GET API call and read the data. If you aren‚Äôt familiar with API the code below will help you finding the correct settings. This will also help with creating complete new connections with other types like GitHub. In that case I also will create a connection manual first to find out which parameters I need.\nSend request $personalToken¬†=¬†\u0026#34;verysecretcode\u0026#34; $organisation¬†=¬†\u0026#34;MyDevOpsOrganisation\u0026#34; $ProjectName¬†=¬†\u0026#34;The¬†DevOps¬†project\u0026#34; $URL¬†=¬†\u0026#34;https://dev.azure.com/$organisation/$ProjectName/_apis/serviceendpoint/endpoints?api-version=6.0-preview.4\u0026#34; $AzureDevOpsAuthenicationHeader¬†=¬†@{Authorization¬†=¬†\u0026#39;Basic¬†\u0026#39;¬†+¬†[Convert]::ToBase64String([Text.Encoding]::ASCII.GetBytes(\u0026#34;:$($personalToken)\u0026#34;))¬†} $Parameters¬†=¬†@{ Uri¬†=¬†$URL Method¬†=¬†\u0026#34;GET\u0026#34; Headers¬†=¬†$AzureDevOpsAuthenicationHeader Erroraction¬†=¬†\u0026#34;Stop\u0026#34; } $Result¬†=¬†Invoke-RestMethod¬†@Parameters $Result.value | ConvertTo-Json Just fill in the correct parameters and send the request.\nResponse { \u0026#34;data\u0026#34;: { \u0026lt;strong\u0026gt;\u0026#34;environment\u0026#34;: \u0026#34;AzureCloud\u0026#34;,\u0026lt;/strong\u0026gt; \u0026lt;--- \u0026lt;strong\u0026gt;\u0026#34;scopeLevel\u0026#34;: \u0026#34;Subscription\u0026#34;,\u0026lt;/strong\u0026gt; \u0026lt;--- \u0026#34;subscriptionId\u0026#34;: \u0026#34;xxxx\u0026#34;, \u0026#34;subscriptionName\u0026#34;: \u0026#34;xxxx\u0026#34;, \u0026#34;resourceGroupName\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;mlWorkspaceName\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;mlWorkspaceLocation\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;managementGroupId\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;managementGroupName\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;oboAuthorization\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;creationMode\u0026#34;: \u0026#34;Automatic\u0026#34;, \u0026#34;azureSpnRoleAssignmentId\u0026#34;: \u0026#34;xxxx\u0026#34;, \u0026#34;azureSpnPermissions\u0026#34;: \u0026#34;[{\\\u0026#34;roleAssignmentId\\\u0026#34;:\\\u0026#34;xxxx\\\u0026#34;,\\\u0026#34;resourceProvider\\\u0026#34;:\\\u0026#34;Microsoft.RoleAssignment\\\u0026#34;,\\\u0026#34;provisioned\\\u0026#34;:true}]\u0026#34;, \u0026#34;spnObjectId\u0026#34;: \u0026#34;xxxx\u0026#34;, \u0026#34;appObjectId\u0026#34;: \u0026#34;xxxx\u0026#34;, \u0026#34;resourceId\u0026#34;: \u0026#34;\u0026#34; }, \u0026#34;id\u0026#34;: \u0026#34;xxxx\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;AVDExpertToAccept\u0026#34;, \u0026lt;strong\u0026gt;\u0026#34;type\u0026#34;: \u0026#34;azurerm\u0026#34;,\u0026lt;/strong\u0026gt; \u0026lt;--- \u0026#34;url\u0026#34;: \u0026#34;https://management.azure.com/\u0026#34;, \u0026#34;createdBy\u0026#34;: { \u0026#34;displayName\u0026#34;: \u0026#34;Sander Rozemuller\u0026#34;, \u0026#34;url\u0026#34;: \u0026#34;xxxx\u0026#34;, \u0026#34;_links\u0026#34;: \u0026#34;@{avatar=}\u0026#34;, \u0026#34;id\u0026#34;: \u0026#34;xxxx\u0026#34;, \u0026#34;uniqueName\u0026#34;: \u0026#34;srozemuller@xxxx\u0026#34;, \u0026#34;imageUrl\u0026#34;: \u0026#34;xxxx\u0026#34;, \u0026#34;descriptor\u0026#34;: \u0026#34;aad.NGMyNDEwYTUtY2Y4OS03YzYxLThlNDEtMmMwN2UyM2M4MWQx\u0026#34; }, \u0026#34;description\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;authorization\u0026#34;: { \u0026#34;parameters\u0026#34;: \u0026#34;@{tenantid=xxxx; serviceprincipalid=xxxx; authenticationType=spnKey; scope=/subscriptions/xxxx/resourcegroups/xxxx}\u0026#34;, \u0026lt;strong\u0026gt;\u0026lt;em\u0026gt;\u0026#34;scheme\u0026#34;: \u0026#34;ServicePrincipal\u0026#34;\u0026lt;/em\u0026gt;\u0026lt;/strong\u0026gt; \u0026lt;--- }, \u0026#34;isShared\u0026#34;: false, \u0026#34;isReady\u0026#34;: true, \u0026#34;operationStatus\u0026#34;: { \u0026#34;state\u0026#34;: \u0026#34;Ready\u0026#34;, \u0026#34;statusMessage\u0026#34;: \u0026#34;\u0026#34; }, \u0026#34;owner\u0026#34;: \u0026#34;Library\u0026#34;, \u0026#34;serviceEndpointProjectReferences\u0026#34;: [ \u0026#34;@{projectReference=; name=AVDExpertToAccept; description=}\u0026#34; ] } ] By doing some research on the response body you will learn a lot how to setup the correct request body. The most important and static values has been marked with \u0026lt;‚Äî. These are the types, methods and scopes which are defined by Microsoft. The other parameters are user defined which I will show you in the next step.\nCreating custom body A lot of information from the response body isn‚Äôt necessary for the script. The body below is needed to create a service connection.\nAs you can see this body is smaller then the response body. This is a snippet from the script which you can find on my GitHub repository.\n$Body¬†=¬†@{ data¬†=¬†@{ managementGroupId¬†=¬†$managementGroupId managementGroupName¬†=¬†$managementGroupName environment¬†=¬†\u0026#34;AzureCloud\u0026#34; scopeLevel¬†=¬†\u0026#34;ManagementGroup\u0026#34; creationMode¬†=¬†\u0026#34;Manual\u0026#34; } name¬†=¬†$ConnectionName type¬†=¬†\u0026#34;AzureRM\u0026#34; url¬†=¬†\u0026#34;https://management.azure.com/\u0026#34; authorization¬†=¬†@{ parameters¬†=¬†@{ tenantid¬†=¬†$TenantInfo.Tenant.Id serviceprincipalid¬†=¬†$AADApplication.ApplicationId.Guid authenticationType¬†=¬†\u0026#34;spnKey\u0026#34; serviceprincipalkey¬†=¬†$PlainPassword } scheme¬†=¬†\u0026#34;ServicePrincipal\u0026#34; } isShared¬†=¬†$false isReady¬†=¬†$true serviceEndpointProjectReferences¬†=¬†@( @{ projectReference¬†=¬†@{ id¬†=¬†$ProjectID name¬†=¬†$ProjectName } name¬†=¬†$ConnectionName } ) } Body explained For the dynamic parts I‚Äôve created parameters.\nThings like environment, scopeLevel, creationMode and type are always the same for me, at this time.\nFor the leftovers I will explain what they are.\nFirst the data part, this is the same as the scope part in the manual configuration. In this scenario I‚Äôve chosen to use the management group scope level. By choosing management groups you will need a management group name and ID.\nIf you need a service connection at subscription scope level change the scope level to ‚ÄúSubscription‚Äú. Of course the ManagementgroupId and ManagementgroupName part should be replaced for SubscriptionId and SubscriptionName.\ndata =¬†@{ managementGroupId¬†=¬†$managementGroupId managementGroupName¬†=¬†$managementGroupName environment¬†=¬†\u0026#34;AzureCloud\u0026#34; scopeLevel¬†=¬†\u0026#34;ManagementGroup\u0026#34; creationMode¬†=¬†\u0026#34;Manual\u0026#34; } If you like to create a connection to a subscription instead of a management group then the script will use the parameters below.\ndata¬†=¬†@{ SubscritptionId¬†=¬†$SubscritptionId SubscriptionName¬†=¬†$SubscriptionName environment¬†=¬†\u0026#34;AzureCloud\u0026#34; scopeLevel¬†=¬†\u0026#34;Subscription\u0026#34; creationMode¬†=¬†\u0026#34;Manual\u0026#34; } The next part is pretty clear like a name, url and type. The type is the first thing you choose when creating a manual connection.\nname¬†=¬†$ConnectionName type¬†=¬†\u0026#34;AzureRM\u0026#34; url¬†=¬†\u0026#34;https://management.azure.com/\u0026#34; The authentication step (in manual) is the part where to fill in the tenantId, ApplicationId and secret. The scheme in this case is ServicePrincipal, the method.\nauthorization¬†=¬†@{ parameters¬†=¬†@{ tenantid¬†=¬†$TenantId serviceprincipalid¬†=¬†$ApplicationId authenticationType¬†=¬†\u0026#34;spnKey\u0026#34; serviceprincipalkey¬†=¬†$PlainPassword } scheme¬†=¬†\u0026#34;ServicePrincipal\u0026#34; } The last part is configuring the project reference. The place where to tell in which project the connection needs to be created. Because we like to automate things we seach for the projectId which can be found through an API call.\nThe URL you need is:\nhttps://dev.azure.com/$($organisation)/_apis/projects?api-version=6.0\nAfter querying the projects you will get a result like below.\nid : xxx name : Windows Virtual Desktop url : https://dev.azure.com/xxx/_apis/projects/xxx state : wellFormed revision : 414 visibility : private lastUpdateTime : 2/28/2020 2:47:03 PM Now you know the Id and Name, you are able to fill in every needed parameters to create a DevOps service connection the automated way.\nserviceEndpointProjectReferences¬†=¬†@( @{ projectReference¬†=¬†@{ id¬†=¬†$ProjectID name¬†=¬†$ProjectName } name¬†=¬†$ConnectionName } ) How to use In my case the script is a part of a sequence so a lot of parameters are allready known by task. The script at my repository can be used as a standalone script which can be used the following way:\n.\\\u0026lt;strong\u0026gt;create-DevOpsServiceConnection.ps1\u0026lt;/strong\u0026gt; -personalToken xxx -organisation DevOpsOrganisation -ProjectName AVD -ManagementGroupId MGTGROUP1 -ManagementGroupName \u0026#39;MGT GROUP 1\u0026#39; -TenantId xxx-xxx -ApplicationId xxx-xxx-xxx -ApplicationSecret \u0026#39;verysecret\u0026#39; More information about the service connection API please check: As I mentioned before in the blog I published some snippets. The complete script is stored at my GitHub page.\nHappy automating üòâ and thank you for reading. Thank you for reading my blog prepare azure devops for avd deployment ‚Äì create a service connection automated. I hope you got a bit inspired. Enjoy your day and happy automating üëã ","date":"March 11, 2021","image":"http://localhost:1313/prepare-azure-devops-for-avd-deployment-create-a-service-connection/slices.png","permalink":"/prepare-azure-devops-for-avd-deployment-create-a-service-connection/","title":"Prepare Azure DevOps for AVD deployment ‚Äì Create a Service Connection Automated"},{"categories":["Azure Virtual Desktop","Security"],"contents":"Security became important more and more. It is recommend, now the days arrived where the most people work from home and IT is moving to the public cloud, to use a good firewall with less complexity as possible. In case of AVD within the current situation an IT environment can be complex. In this article I will explain how configure an Azure Firewall for AVD automated with premium options, which came in public preview at 16 February 2021. At the end I will show how to deploy an Azure Firewall specially for a AVD environment with all the needed steps automated.\nThe most IT environments has a firewall at the office where you have to connect to first by VPN, IPSEC or something. From there you will be able to connect to the public cloud from the office. Properly with an another VPN or express route.\nTable Of Contents What is new in Azure Firewall Premium Transport Layer Security (TLS) Inspection Intrusion Detection and Prevention System (IDPS) URL Filtering Web categories Azure Firewall architecture Firewall policy Subnet Public IP Azure Firewall for AVD Automated Gather VNET information Add AzureFirewallSubnet Deploy public IP Deploy Azure Firewall Premium Firewall Policy for AVD Enable TLS Inspection Create an Azure Key Vault Enable IDPS Deploy the firewall policy Configuring the AVD policy URL Filtering Web categories Network rules Default routes Enable Log Analytics Other resources What is new in Azure Firewall Premium First thing which is good to know is the difference between the standard options and the premium options. Of course the price will increase but I am a tech guy so I will skip the pricing aspect.\nAnother important thing is the Microsoft‚Äôs SLA over the public features. Because it is still in public preview, not general available, it is recommended to keep it for test purposes for now.\nThat said it is now time to look at the new features.\nTransport Layer Security (TLS) Inspection Azure Firewall Premium terminates outbound and east-west TLS connections. Inbound TLS inspection is supported in conjunction with Azure Application Gateway allowing end-to-end encryption. Azure Firewall performs the required value-added security functions and re-encrypts the traffic which is sent to the original destination\nIntrusion Detection and Prevention System (IDPS) This new feature will help you monitor the network for malicious activity, will log these information, report about it and if configured it will block traffic. This can be very useful in a AVD environment since users will have the ability to install software in the host, which can be malicious.\nURL Filtering As the name says it can filter URLs and will block the page if needed. In relation to the standard option this filter has the possibility to filter parts of an URL like rozemuller.com/pages/testpage instead of filtering the whole page based on FQDN like rozemuller.com.\nWeb categories By configuring web categories you will be able to deny access to websites based on a category like social media or gambling. The standard feature also has this option. Difference between standard and premium in this case is the premium firewall will categorize a page based on the full URL where the standard firewall will check the FQDN part only.\nBased on the new features, in case of an end user environment like AVD, I think blocking specific URL‚Äôs will make things unnecessary complex. This because most of the web pages will fully represents a category. The inspection, detection and prevention features are really valuable protecting and end user environment.\nMore information about these features check the Azure Firewall Premium features overview\nAzure Firewall architecture Before automating things it is good to know how things are working. At first we all know a firewall is essential in every IT network. It will keep you network safe from hack attacks from outside your network. An Azure Firewall does the same like any other firewall.\nWhen configuring the Azure Firewall through the portal you will be asked in the first part for some basics like a name, region and resourcegroup. The next part is where you can setup the firewall. In this case we will create a new firewall and will choose the Premium tier.\nFirewall policy Every firewall has at least one policy called a Firewall Policy. Firewall Policy is an Azure resource that contains NAT, network, and application rule collections, and Threat Intelligence settings. It‚Äôs a global resource that can be used across multiple Azure Firewall instances in Secured Virtual Hubs and Hub Virtual Networks. Policies work across regions and subscriptions.\nIn the deployment later I will create several rule collections. Every rule collection represents rules with the same type, priority and action.\nSubnet The following step is connecting the firewall to a virtual network. This is the virtual network which the Azure Firewall will protect. Good to know is that the firewall needs an own subnet called AzureFirewallSubnet.\nIn the automation chapter I will use an existing VNET based on the AVD environment, but will create the needed subnet.\nPublic IP The next step is the public IP. This is the IP which is representing your internal network on the internet.\nMore information about the Azure Firewall features please check the feature list.\nAzure Firewall for AVD Automated Now we know which components we need it is time to deploy an Azure Firewall for AVD automated.\nAt the end of this chapter we have an Azure Firewall configured specially for a AVD environment. This means we have configured the new premium features and also added some rule to keep in contact with Microsoft. This is needed to stay supported.\nIn this first part I will deploy a premium firewall with PowerShell. As I mentioned before you will need information like a VNET and public IP.\nGather VNET information Because I will create a firewall in an existing VNET I will gather all that information which will serve as the basic information. Good to now that a firewall must be placed in the same resource group as the VNET.\n$VirtualNetworkName¬†= \u0026#39;vnet-avd-acc-westeu-001\u0026#39; $VirtualNetworkResoureGroup = \u0026#39;rg-avd-acc-001\u0026#39; $networkInfo¬†=¬†Get-AzVirtualNetwork¬†-Name¬†$VirtualNetworkName¬†-ResourceGroupName¬†$VirtualNetworkResoureGroup Add AzureFirewallSubnet Now we have the network information the first step is creating the needed AzureFirewallSubnet. In the first place I will add the subnet in memory then I will commit it to the virtual network.\nGood to know is that the subnet needs at least a /26 range.\n$networkInfo¬†|¬†Add-AzVirtualNetworkSubnetConfig¬†-Name¬†AzureFirewallSubnet¬†-AddressPrefix¬†10.2.0.192/26 $networkInfo¬†|¬†Set-AzVirtualNetwork Deploy public IP The next step is configuring deploying a public IP. This is the external IP which is representing your network on the Internet.\nTo avoid long commands first I will create a parameter table with all need information. As you can see the $networkinfo variable is used to deploy the public IP in the same region and resourcegroup. I will store the results in the $FWpip variable because we will need it in the next step.\n$FirewallPip¬†=¬†@{ Name¬†=¬†\u0026#34;fw-avd-pip-001\u0026#34; Location¬†=¬†$networkInfo.location AllocationMethod¬†=¬†\u0026#34;Static\u0026#34; Sku¬†=¬†\u0026#34;Standard\u0026#34; ResourceGroupName¬†=¬†$networkInfo.Resourcegroupname } $FWpip¬†=¬†New-AzPublicIpAddress¬†@FirewallPip Deploy Azure Firewall Premium Now the public IP is set we are able to deploy the firewall. In this parameter table I‚Äôm also using the $networkinfo again and the $FWpip from the previous step.\nMake a notice about the SkuTier which is set to Premium. Because ‚ÄúStandard‚Äù is default you will have to set this value.\n$firewallParameters¬†=¬†@{ Name¬†=¬†\u0026#34;fw-premium-avd\u0026#34; Location¬†=¬†$networkInfo.location PublicIpAddress¬†=¬†$FWpip VirtualNetwork¬†=¬†$networkInfo ResourceGroupName¬†=¬†$networkInfo.Resourcegroupname SkuTier¬†=¬†\u0026#34;Premium\u0026#34; } #¬†Create¬†the¬†firewall $Azfw¬†=¬†New-AzFirewall¬†@firewallParameters Now the firewall is deployed it is time the configure a firewall policy.\nFirewall Policy for AVD A firewall policy is an Azure resource that contains NAT, network, application rule collections, and Threat Intelligence settings. It‚Äôs a global resource that can be used across multiple Azure Firewall instances in Secured Virtual Hubs and Hub Virtual Networks. Policies work across regions and subscriptions.\nIn my case I will create a firewall policy for a AVD environment and will add it to the just created firewall. To make things clear a firewall policy is a independent Azure resource itself. After creating you have to ‚Äòconnect‚Äô it to the firewall.\nLets create a policy first. In the parameters below I‚Äôm using the networkInfo again, also make a notice about the ‚ÄòPremium‚Äô tier. Otherwise is standard and you will not have the new premium features.\nEnable TLS Inspection TLS inspection needs an Azure Key Vault with a valid certificate which is used to encrypt the data before it is send to the destination. You will need to enable TLS inspection first before you are able to use it within the application rules.\nThe code below will create a new Key Vault, a managed identity and will add it as a Reader to the Key Vault. This identity is used later when configuring TLS in the policy.\nCreate an Azure Key Vault Before enabling TLS you will need a valid certificate stored in an Azure Key Vault. With the code below you will create a new empty Key Vault with the correct permissions.\n$KeyVaultSettings¬†=¬†@{ Name¬†=¬†\u0026#39;kv-firewall-tls\u0026#39; ResourceGroupName¬†=¬†$networkInfo.Resourcegroupname Location¬†=¬†$networkInfo.location } $AzKeyVault¬†=¬†New-AzKeyVault¬†@KeyVaultSettings $KvManagedId¬†=¬†New-AzUserAssignedIdentity¬†-ResourceGroupName¬†$networkInfo.Resourcegroupname¬†-Name¬†fw-managedid-tls $objectId¬†=¬†Get-AzADServicePrincipal¬†-DisplayName¬†$KvManagedId.Name $AzKeyVault¬†|¬†New-AzRoleAssignment¬†-RoleDefinitionName¬†\u0026#34;Reader\u0026#34;¬†-objectId¬†$objectId.Id $AzKeyVault¬†|¬†Set-AzKeyVaultAccessPolicy¬†-objectId¬†$objectId.Id¬†-PermissionsToCertificates¬†\u0026#34;Get\u0026#34;,\u0026#34;List\u0026#34;¬†-PermissionsToSecrets¬†\u0026#34;Get\u0026#34;,\u0026#34;List\u0026#34; Now the Key Vault is in place and a managed identity all the needed information is present to fill in later.\nThe next step is creating a certificate and import it into the Key Vault. Mention the NotAfter which must be at least 12 months. In my demo I used a self signed certificate. Make sure your clients trust the root and will have the client certificate installed.\n$CertParameters¬†=¬†@{ Subject¬†=¬†\u0026#34;AVD¬†Intermediate¬†CertAuthority\u0026#34; FriendlyName¬†=¬†\u0026#34;AVD¬†Intermediate¬†CertAuthority\u0026#34; CertStoreLocation¬†=¬†\u0026#39;cert:\\LocalMachine\\My\u0026#39; DnsName¬†=¬†\u0026#34;avd.experts\u0026#34; KeyAlgorithm¬†=¬†\u0026#34;RSA\u0026#34; HashAlgorithm¬†=¬†\u0026#34;SHA256\u0026#34; KeyLength¬†=¬†4096 KeyUsage¬†=¬†@(\u0026#34;CertSign\u0026#34;,\u0026#34;DigitalSignature\u0026#34;) KeyUsageProperty¬†=¬†\u0026#34;Sign\u0026#34; NotAfter¬†=¬†(Get-Date).AddMonths(36) TextExtension¬†=¬†@(\u0026#34;2.5.29.19¬†=¬†{critical}¬†{text}ca=1\u0026amp;pathlength=1\u0026#34;) } $Cert¬†=¬†New-SelfSignedCertificate¬†@CertParameters $CertPassword¬†=¬†ConvertTo-SecureString¬†-String¬†\u0026#34;avd_demo_pass@!\u0026#34;¬†-Force¬†‚ÄìAsPlainText $Cert¬†|¬†Export-PfxCertificate¬†-FilePath¬†\u0026#34;C:\\avd-intermediate.pfx\u0026#34;¬†-Password¬†$CertPassword $tlsCert¬†=¬†$AzKeyVault¬†|¬†Import-AzKeyVaultCertificate¬†-Password¬†$CertPassword¬†-Name¬†\u0026#39;avd-intermediate-cert\u0026#39;¬†-FilePath¬†\u0026#34;C:\\avd-intermediate.pfx\u0026#34; More information about premium certificates check https://docs.microsoft.com/en-us/azure/firewall/premium-certificates\nEnable IDPS Enabling IDPS in basics is quite simple. In my example I only just setup an alert by executing the code below.\n$IdpsSettings¬†=¬†New-AzFirewallPolicyIntrusionDetection¬†-Mode¬†\u0026#34;Alert\u0026#34; Deploy the firewall policy Now every step is in place we are able to create the premium policy with TLS and IDPS enabled.\n$AzFwPolicySettings¬†=¬†@{ Name¬†=¬†\u0026#39;fw-policy-avd-premium\u0026#39; ResourceGroupName¬†=¬†$networkInfo.Resourcegroupname Location¬†=¬†$networkInfo.location SkuTier¬†=¬†\u0026#34;Premium\u0026#34; TransportSecurityName¬†=¬†\u0026#34;tls-avd\u0026#34; TransportSecurityKeyVaultSecretId¬†=¬†$tlsCert.SecretId UserAssignedIdentityId¬†=¬†$KvManagedId.Id IntrusionDetection¬†=¬†$IdpsSettings } $AzFwPolicy¬†=¬†New-AzFirewallPolicy¬†@AzFwPolicySettings At the moment of writing it looks like there is a bug with PowerShell in combination with the Azure portal. However you are creating a premium policy in the portal the policy is characterized as standard. In the policy itself the SKU is premium.\nConfiguring the AVD policy Before configuring the policy it is good to know how a policy is built up. To keep things clear is it recommended using groups called ‚Äòcollections‚Äô. In the firewall policy blade in the menu on the left side you see an option Rule Collections. Within that option you are able create different collection types, Application, DNAT and Network.\nMatching the portal to PowerShell commands in this case took some time.\nEvery rule collection has the same type of rules, priority and action. It is not possible to create an allow and deny rule in the same collection. With that in mind I will create for every type a new collection rules.\nGood to know is that you have to create collection rules with PowerShell.\nURL Filtering There are several commands for creating collections. In my first test flight I used the most likely command New-AzFirewallApplicationRuleCollection. It worked but this one will create a classic collection instead a policy collection.\nTo take benefit of the premium features you will need the premium policy. The next one I tried was the New.AzFirewallPolicyFilterRuleCollection. That one did the trick.\nI used the commands below to configure a AVD policy.\n$sourceAddress¬†=¬†\u0026#34;10.2.1.0/24\u0026#34; #¬†Create¬†a¬†rule¬†collection¬†group¬†first $RuleCollectionGroup¬†=¬†New-AzFirewallPolicyRuleCollectionGroup¬†-Name¬†AVD-APP-URL-ALLOW¬†-Priority¬†100¬†-FirewallPolicyObject¬†$AzFwPolicy #¬†Define¬†rules¬†//¬†part¬†of¬†the¬†safe-url¬†list¬†https://docs.microsoft.com/en-us/azure/virtual-desktop/safe-url-list $ApplicationRule1¬†=¬†New-AzFirewallPolicyApplicationRule¬†-Name¬†\u0026#39;avd-microsoft-com\u0026#39;¬†-Protocol¬†\u0026#34;http:80\u0026#34;,\u0026#34;https:443\u0026#34;¬†-TargetFqdn¬†\u0026#34;*.avd.microsoft.com\u0026#34;¬†-SourceAddress¬†$sourceAddress $ApplicationRule2¬†=¬†New-AzFirewallPolicyApplicationRule¬†-Name¬†\u0026#39;gcs-windows-net\u0026#39;¬†-Protocol¬†\u0026#34;http:80\u0026#34;,\u0026#34;https:443\u0026#34;¬†-TargetFqdn¬†\u0026#34;gcs.prod.monitoring.core.windows.net\u0026#34;¬†-SourceAddress¬†$sourceAddress $ApplicationRule3¬†=¬†New-AzFirewallPolicyApplicationRule¬†-Name¬†\u0026#39;diagnostics-windows-net\u0026#39;¬†-Protocol¬†\u0026#34;http:80\u0026#34;,\u0026#34;https:443\u0026#34;¬†-TargetFqdn¬†\u0026#34;production.diagnostics.monitoring.core.windows.net\u0026#34;¬†-SourceAddress¬†$sourceAddress #¬†TLS¬†Inspection¬†Rules $ApplicationRule4¬†=¬†New-AzFirewallPolicyApplicationRule¬†-Name¬†\u0026#39;microsoft-com\u0026#39;¬†-Protocol¬†\u0026#34;http:80\u0026#34;,\u0026#34;https:443\u0026#34;¬†-TargetFqdn¬†\u0026#34;*.microsoft.com\u0026#34;¬†-SourceAddress¬†$sourceAddress¬†-TerminateTLS $ApplicationRule5¬†=¬†New-AzFirewallPolicyApplicationRule¬†-Name¬†\u0026#39;windows-net\u0026#39;¬†-Protocol¬†\u0026#34;http:80\u0026#34;,\u0026#34;https:443\u0026#34;¬†-TargetFqdn¬†\u0026#34;*.windows.net\u0026#34;¬†-SourceAddress¬†$sourceAddress¬†-TerminateTLS¬†$ApplicationRuleCollection¬†=¬†@{ Name¬†=¬†\u0026#34;AVD-App-Rules-Allow\u0026#34; Priority¬†=¬†101¬†ActionType¬†=¬†\u0026#34;Allow\u0026#34; Rule¬†=¬†@($ApplicationRule1,¬†$ApplicationRule2,¬†$ApplicationRule3,$ApplicationRule4,$ApplicationRule5) } #¬†Create¬†a¬†app¬†rule¬†collection $AppRuleCollection¬†=¬†New-AzFirewallPolicyFilterRuleCollection¬†@ApplicationRuleCollection #¬†Deploy¬†to¬†created¬†rule¬†collection¬†group Set-AzFirewallPolicyRuleCollectionGroup¬†-Name¬†$RuleCollectionGroup.Name¬†-Priority¬†100¬†-RuleCollection¬†$AppRuleCollection¬†-FirewallPolicyObject¬†$AzFwPolicy Web categories The second option we have is creating rules based on web categories. In this example I will use a new rule collection group with a new application collection.\n$sourceAddress¬†=¬†\u0026#34;10.2.1.0/24\u0026#34; #¬†Create¬†a¬†rule¬†collection¬†category¬†group¬†first $RuleCatCollectionGroup¬†=¬†New-AzFirewallPolicyRuleCollectionGroup¬†-Name¬†AVD-APP-CATEGORY-Deny¬†-Priority¬†101¬†-FirewallPolicyObject¬†$AzFwPolicy $categoryrule1¬†=¬†New-AzFirewallPolicyApplicationRule¬†-WebCategory¬†\u0026#39;Gambling\u0026#39;¬†-Name¬†\u0026#39;Gambling\u0026#39;¬†-Protocol¬†\u0026#34;http:80\u0026#34;,\u0026#34;https:443\u0026#34;¬†-SourceAddress¬†$sourceAddress -TerminateTLS $categoryrule2¬†=¬†New-AzFirewallPolicyApplicationRule¬†-WebCategory¬†\u0026#39;Games\u0026#39;¬†-Name¬†\u0026#39;Games\u0026#39;¬†-Protocol¬†\u0026#34;http:80\u0026#34;,\u0026#34;https:443\u0026#34;¬†-SourceAddress¬†$sourceAddress -TerminateTLS #¬†Create¬†a¬†app¬†rule¬†collection $AppCategoryCollection¬†=¬†New-AzFirewallPolicyFilterRuleCollection¬†-Name¬†AVD-App-Categories¬†-Priority¬†101¬†-Rule¬†$categoryrule1,$categoryrule2¬†-ActionType¬†\u0026#34;Deny\u0026#34; #¬†Deploy¬†to¬†created¬†rule¬†collection¬†group Set-AzFirewallPolicyRuleCollectionGroup¬†-Name¬†$RuleCatCollectionGroup.Name¬†-Priority¬†101¬†-RuleCollection¬†$AppCategoryCollection¬†-FirewallPolicyObject¬†$AzFwPolicy Network rules To stay supported and keeps your AVD environment working it is necesarry to add some critical network rules.\nI used the code below to achieve my goal.\n$sourceAddress = \u0026#34;10.2.1.0/24\u0026#34; #AVD Subnet $RuleCollectionGroup¬†=¬†New-AzFirewallPolicyRuleCollectionGroup¬†-Name¬†AVD-NETWORK-ALLOW¬†-Priority¬†104¬†-FirewallPolicyObject¬†$AzFwPolicy $Rule1Parameters¬†=¬†@{ Name¬†=¬†\u0026#34;Allow-DNS\u0026#34; Protocol¬†=¬†\u0026#34;UDP\u0026#34; sourceAddress¬†=¬†$sourceAddress DestinationPort¬†=¬†\u0026#34;53\u0026#34; DestinationAddress¬†=¬†\u0026#34;*\u0026#34; } $Rule2Parameters¬†=¬†@{ Name¬†=¬†\u0026#34;Allow-KMS\u0026#34; Protocol¬†=¬†\u0026#34;TCP\u0026#34; sourceAddress¬†=¬†$sourceAddress DestinationPort¬†=¬†\u0026#34;1688\u0026#34; DestinationAddress¬†=¬†\u0026#34;23.102.135.246\u0026#34; }¬†$Rule3Parameters¬†=¬†@{ Name¬†=¬†\u0026#34;Allow-NTP\u0026#34; Protocol¬†=¬†\u0026#34;UDP\u0026#34; sourceAddress¬†=¬†$sourceAddress DestinationPort¬†=¬†\u0026#34;123\u0026#34; DestinationAddress¬†=¬†\u0026#34;51.105.208.173\u0026#34; } $rule1¬†=¬†New-AzFirewallPolicyNetworkRule¬†@Rule1Parameters $rule2¬†=¬†New-AzFirewallPolicyNetworkRule¬†@Rule2Parameters $rule3¬†=¬†New-AzFirewallPolicyNetworkRule¬†@Rule3Parameters $NetworkRuleCollection¬†=¬†@{ Name¬†=¬†\u0026#34;AVD-Network-Rules-Allow\u0026#34; Priority¬†=¬†102¬†ActionType¬†=¬†\u0026#34;Allow\u0026#34; Rule¬†=¬†@($rule1,¬†$rule2,¬†$rule3) } #¬†Create¬†a¬†app¬†rule¬†collection $NetworkRuleCategoryCollection¬†=¬†New-AzFirewallPolicyFilterRuleCollection¬†@NetworkRuleCollection #¬†Deploy¬†to¬†created¬†rule¬†collection¬†group Set-AzFirewallPolicyRuleCollectionGroup¬†-Name¬†$RuleCollectionGroup.Name¬†-Priority¬†104¬†-RuleCollection¬†$NetworkRuleCategoryCollection¬†-FirewallPolicyObject¬†$AzFwPolicy At the end the overall situation will have a rule collection at the top with a rule collection group and then an application rule collection in the group. (I haven‚Äôt spotted in the portal where to create a collection group yet. There is a default group out of the box). I recommend creating rule collection groups per type, per allow/deny. Because rule collection groups currently can only include single rule collection type.\nAt this moment there is no update command for updating the application rule collection. If you need to edit the collection then go to the portal. If the commands will be updated I will update this post as soon as possible.\nDefault routes At last we will make sure that all the traffic is going through the firewall. So we need to create a default route and add the route table to the AVD subnet. I consciously choose to add the default route as a last step. This will give me all the time I need to configure the firewall before getting live.\nIn the code below we first make a route table. After then we will add a route to that table. The last step will add the route to the subnet.\nAs you can see I‚Äôm still using the networkInfo parameter at the beginning of this post.\n# Create a route table $routeTableParameters = @{ Name = \u0026#39;fw-routetable-avd\u0026#39; Location = $networkInfo.Location ResourceGroupName = $networkInfo.ResourceGroupName DisableBgpRoutePropagation = $true } $routeTableDG = New-AzRouteTable @routeTableParameters # Create a route; 0-route will send all the traffic to the firewall $routeParameters = @{ Name = \u0026#34;avd-route\u0026#34; RouteTable = $routeTableDG AddressPrefix = 0.0.0.0/0 NextHopType = \u0026#34;VirtualAppliance\u0026#34; NextHopIpAddress = $azFw.IpConfigurations.privateIpAddress } Add-AzRouteConfig @routeParameters | Set-AzRouteTable # Associate the route table to the subnet $subnetParameters = @{ VirtualNetwork = $networkInfo Name = \u0026#34;DefaultSubnet\u0026#34; AddressPrefix = \u0026#34;10.2.1.0/24\u0026#34; #AVD Subnet RouteTable = $routeTableDG } Set-AzVirtualNetworkSubnetConfig @subnetParameters | Set-AzVirtualNetwork Enable Log Analytics By default no diagnostic settings are enabled at the Azure Firewall. To help you gathering information it is critical to setup the diagnosics settings.\nWith the code below you are able to enable the diagnostic settings to a new Log Analytics Workspace. Because a firewall can generate a lot of logs and metrics, and to avoid high costs, I have set the retention to 30 days.\n$WorkspaceParameters¬†=¬†@{ Name¬†=¬†\u0026#34;FW-AVD-Workspace\u0026#34; ResourceGroupName¬†=¬†$Azfw.ResourceGroupName Location¬†=¬†$Azfw.Location Sku¬†=¬†\u0026#34;Standard\u0026#34; } $Workspace¬†=¬†New-AzOperationalInsightsWorkspace¬†@WorkspaceParameters $Parameters¬†=¬†@{ Name¬†=¬†\u0026#34;FW-AVD-Diagnostics\u0026#34; ResourceId¬†=¬†$AzFw.Id WorkspaceId¬†=¬†$Workspace.ResourceId Enabled¬†=¬†$true RetentionEnable¬†=¬†$true RetentionInDays¬†=¬†30 } Set-AzDiagnosticSetting¬†@parameters Other resources Some other resources which helped me out.\nhttps://azure.microsoft.com/en-us/updates/azure-firewall-premium-now-in-public-preview https://docs.microsoft.com/en-us/azure/firewall/protect-windows-virtual-desktop https://www.linkedin.com/pulse/avd-azure-firewall-premium-web-content-filtering-marco-moioli https://docs.microsoft.com/en-us/azure/firewall-manager/policy-overview Thank you for reading my blog post about configuring an Azure Firewall Premium for AVD the automated way.\n","date":"February 26, 2021","image":"http://localhost:1313/configure-azure-firewall-premium-features-for-avd-automated/azure-security.jpg","permalink":"/configure-azure-firewall-premium-features-for-avd-automated/","title":"Configure Azure Firewall Premium features for AVD automated"},{"categories":["Azure Virtual Desktop","Monitoring"],"contents":"A lot of people in the Microsoft community are using a fancy gadget called LaMetric Time. It is a smart clock which can show time, receiving notifications from a lot of different environments and more. In this blog I will show how to use LaMetric Time to monitor a AVD environment.\nWelcome to this fun with functions blog about using LaMetric Time to monitor AVD environment based on an Azure Function. In this post I will explain how to setup the Azure Function and create dynamic content for the LaMetric time application.\nTable Of Contents LaMetric Time Developer account Select type Creating the application Look-and-feel Communication Parameters Data format Azure Function Function app Function Permissions The code Result LaMetric Time Before setting up the application it is good to know where to start, which types there are and how they act.\nDeveloper account Before creating a LaMetric application you will need a developer account. This account can be created at https://developer.lametric.com/. This is very easy by clicking the Sign Up button.\nSelect type After signing in you will see a create button. After clicking you will get a page where you need to select a type. In this case I used the indicator type. The reason why is I don‚Äôt need actions. I more like the notification type but in that case I have to do some port forwarding from the internet to my device. This because of the function app (later in this article) will send information to the device.\nAfter selecting the application type the next page will guide you through every step.\nCreating the application Look-and-feel Now it is time to create the application. An application consists of frames. Every frame is display on the LaMetric device. In the create user interface you are able to setup the look-and-feel. Under the frames part you are able to configure an icon and some text.\nCommunication The next part is the communication type and where the application can get his information. In my case I used the poll type. Depending on the type the form below will ask for the required information. By selecting the poll type the form will ask you for a server URL where he can find the data. This will be the function app URL which we will create in the next part.\nParameters When the application has been installed on the LaMetric device it would be nice to configure things like the AVD hostpool you like to monitor. These parameters can be configured by adding fields.\nThe Id is the parameter which will be send with the request. The title is the display name in the application. While sending parameters with the URL, the URL will look like https://url.azurewebsite.net\u0026amp;textidvalue=value\u0026amp;nexttextidvalue=value.\nData format By configuring frames, icons and text the data format will change automatically. In this section you will see the correct data format which the application will use for displaying the correct info on the LaMetric device.\nAzure Function With the LaMetric data format in mind it is time to create an Azure Function. The poll type will request the URL (Webhook) with a GET method. That means you will have to create an Azure Function with a HTTP trigger.\nFunction app Before creating a function you will need a function app first. A function app is the host which will execute the function. Because of using function apps you won‚Äôt need resource types like a VM.\nPlease follow the Microsoft Docs for creating an Azure Function App. Make sure while creating a function app you choose the PowerShell runtime.\nFunction When the function app has been created it is time to create a function. Functions has different types of templates. Most common are HTTP Trigger and Timer Trigger.\nIn this case we need a HTTP Trigger. This means the function will be executed when the function URL has been triggered (by the LaMetric application).\nAfter selecting the template a basic function is created. Because of security reasons I set the HTTP Methods to GET only.\nPermissions Before the function will run successfully make sure the function app has the correct permissions. Because of security reasons I will configure as less permissions as needed.\nTo configure the correct permissions first you need to setup an identity. By enable the identity status the function will become an Azure AD object. When configuring permissions the function will appear in the list of object.\nNow there are special roles I have chosen the Desktop Virtualization Host Pool Reader and LogAnalyics Reader.\nThe code Basically, the code consists on four parts (regions), receiving and handling the request, finding sessionhost availabillity, gathering LogAnalytics information and throw out the JSON format where the LaMetric app will pick up the body.\nLet me explain the most important things of the script.\nThe request query parameters are the parameters which are send from the LaMetric app. The given Id must be the same (case sensitive) in the LaMetric app as in the function.\nSecond are the LaMetric icon numbers. These are the numbers which can be found at https://developer.lametric.com/icons\nusing¬†namespace¬†System.Net #¬†Input¬†bindings¬†are¬†passed¬†in¬†via¬†param¬†block. param($Request) #¬†Write¬†to¬†the¬†Azure¬†Functions¬†log¬†stream. Write-Host¬†\u0026#34;PowerShell¬†HTTP¬†trigger¬†function¬†processed¬†a¬†request.\u0026#34; #region¬†request¬†query¬†parameters // these are the LaMetric Ids. $avdHostpool¬†=¬†$Request.Query.WvdHostpool $resourceGroup¬†=¬†$Request.Query.ResourceGroup $days¬†=¬†$Request.Query.Days #endregion $hosts¬†=¬†Get-AzWvdSessionHost¬†-HostPoolName¬†$avdHostpool¬†-resourcegroupname¬†$ResourceGroup $failedHosts¬†=¬†($hosts.Status¬†|¬†Where¬†{$_¬†-ne¬†\u0026#34;Available\u0026#34;}) $fail¬†=¬†$false $LametricWvdIcon¬†=¬†\u0026#34;i39991\u0026#34; #region¬†sessionhosts if¬†($failedHosts)¬†{ $fail¬†=¬†$true $percentage¬†=¬†($failedHosts.count¬†/¬†$hosts.count)*100 } else¬†{ $fail¬†=¬†$false $percentage¬†=¬†100 } if¬†($fail){ Write-Host¬†\u0026#34;Fail:¬†Percentage¬†is¬†$percentage\u0026#34; $LametricWvdStatusIcon¬†=¬†\u0026#34;a43522\u0026#34;¬†#Animated¬†fail¬†icon } else¬†{ Write-Host¬†\u0026#34;OK:¬†Percentage¬†is¬†$percentage\u0026#34; $LametricWvdStatusIcon¬†=¬†\u0026#34;a43527\u0026#34;¬†#Animated¬†ok¬†icon } #endregion #region¬†sessions $hostpool¬†=¬†Get-AzWvdHostPool¬†-name¬†$avdHostpool¬†-ResourceGroupName¬†$ResourceGroup $Diagsettings¬†=¬†Get-AzDiagnosticSetting¬†-ResourceId¬†$hostpool.id¬†|¬†Select¬†Name,¬†@{Label=‚ÄùWorkSpace¬†Name‚Äù;Expression={($_.WorkspaceId.Split(\u0026#34;/\u0026#34;)[-1])}} try¬†{ $WorkspaceInfo¬†=¬†Get-AzResource¬†-ResourceId¬†$Diagsettings.WorkspaceId $Workspace¬†=¬†Get-AzOperationalInsightsWorkspace¬†-Name¬†$WorkspaceInfo.Name¬†-resourcegroupname¬†$WorkspaceInfo.ResourceGroupName $query¬†=¬†\u0026#34;AVDConnections¬†|¬†where¬†State¬†==¬†\u0026#39;Started\u0026#39;¬†and¬†split(_ResourceId,\u0026#39;/\u0026#39;)[-1]¬†==¬†\u0026#39;$avdHostpool\u0026#39;¬†|¬†make-series¬†total=count()¬†default=0¬†on¬†TimeGenerated¬†from¬†ago($days`d)¬†to¬†now()¬†step¬†1d¬†by¬†now()¬†|¬†summarize¬†mylist¬†=¬†make_list(total)¬†\u0026#34; $results¬†=¬†Invoke-AzOperationalInsightsQuery¬†-Workspace¬†$Workspace¬†-Query¬†$query $resultValues¬†=¬†@(($results.Results.mylist).Split(\u0026#34;,\u0026#34;)¬†-replace¬†\u0026#34;\\D\u0026#34;)¬†|¬†%¬†{$_¬†-as¬†[Int32]} } catch¬†{ Write-Warning¬†\u0026#34;$_¬†\u0026#34; } #endregion $JSONBody¬†=¬†new-object¬†psobject $frames¬†=¬†@( @{index¬†=¬†\u0026#39;0\u0026#39;;¬†text¬†=¬†$avdHostpool;¬†icon¬†=¬†$LametricWvdIcon;¬†}, @{index¬†=¬†\u0026#39;1\u0026#39;;¬†text¬†=¬†\u0026#39;%¬†Avail\u0026#39;;¬†icon¬†=¬†$LametricWvdIcon;¬†}, @{index¬†=¬†\u0026#39;2\u0026#39;;¬†text¬†=¬†$percentage;¬†icon¬†=¬†$LametricWvdStatusIcon;¬†} @{index¬†=¬†\u0026#39;3\u0026#39;;¬†text¬†=¬†\u0026#39;lst¬†days\u0026#39;;¬†icon¬†=¬†$LametricWvdIcon;¬†} @{index¬†=¬†\u0026#39;4\u0026#39;;¬†chartData¬†=¬†$resultValues;} ) $JSONBody¬†|¬†Add-Member¬†-MemberType¬†NoteProperty¬†-Name¬†\u0026#39;frames\u0026#39;¬†-Value¬†$frames¬†-Force $Body¬†=¬†$JSONBody¬†|¬†ConvertTo-Json¬†-Depth¬†3 #¬†Associate¬†values¬†to¬†output¬†bindings¬†by¬†calling¬†\u0026#39;Push-OutputBinding\u0026#39;. Push-OutputBinding¬†-Name¬†Response¬†-Value¬†([HttpResponseContext]@{ StatusCode¬†=¬†[HttpStatusCode]::OK Body¬†=¬†$Body }) Good to know is that the function is querying the Log Analytics workspace. Make sure you have AVD monitoring enabled. You could take a look at my page about [enabling AVD monitoring automated](https://www.rozemuller.com/deploy-azure-monitor-for-windows-virtual-desktop-automated). Result Last good thing to know is that the LaMetric app should be stay in private visibility. A public app will not work because you are using a function URL which will only work for you environment.\nFollowing on this post I`m working an a Graph API version with authentication. Stay tuned and for now happy playing with the LaMetric Time to monitor an AVD environment :-).\nThank you for reading my blog using lametric time to monitor an avd environment. I hope you got a bit inspired. Enjoy your day and happy automating üëã ","date":"February 9, 2021","image":"http://localhost:1313/using-lametric-time-to-monitor-an-avd-environment/IMG_1665.jpg","permalink":"/using-lametric-time-to-monitor-an-avd-environment/","title":"Using LaMetric Time to monitor an AVD environment"},{"categories":["Azure Virtual Desktop","Monitoring"],"contents":"Microsoft has provided a list with URL‚Äôs which you need for running a Azure Virtual Desktop environment. In this blog post I will explain how to monitor any issues related to the AVD required URL‚Äôs. and how to setup the monitoring environment.\nIntroduction Most of the time you are able to connect to the Internet over port 443. But there could be several reasons you don‚Äôt. In case of a Azure Virtual Desktop environment it is critical you can reach Microsoft, otherwise you are unsupported. So it is very important to monitor AVD required URL‚Äôs. To help configuring the firewall Microsoft came up with an URL list.\nIn this case I blocked every traffic to the Internet, with an exception to the Azure Virtual Desktop needed URL‚Äôs.\nTable Of Contents Introduction Windows Event log Azure portal Log Analytics Add session hosts to Log Analytics Workspace Finding the Log Analytics Workspace Monitor workbook Alerting Powershell Azure Monitor Action groups Action rule Automated Prevent issues Network Security Group There are several ways to notice there is something wrong connecting to one of the required URL‚Äôs. In the next chapters I will handle some of them.\nWindows Event log The most common and important place is the Windows event log on the sessionhosts. If there is an issue you will see events like below. A lot of application errors with source AVD-Agent and eventId 3019.\nFor every URL which cannot be reached there will be an event.\nThere also could be some warnings on eventId 3702. You will see errors like this:\nAzure portal An another place where to find issues is on the Azure Portal itself under the Azure Virtual Desktop blade.\nLog Analytics A second place where you can find events is the Log Analytics Workspace. Important thing to know is that Log Analytics is not working out of the box. For using Log Analytics is you will need a Log Analytics workspace. Also make sure you installed the MicrosoftMonitoringAgent extension on each AVD session host.\nI wrote a blog post about enabling Azure Monitor for Azure Virtual Desktop. In that post I explain how to enable the Azure Monitor in basics for AVD including setting up a Log Analytics workspace with the correct settings.\nThe blog post below will continue at the last section when adding session host to the workspace.\nAdd session hosts to Log Analytics Workspace If you have configured the Azure Monitor you will need to add every sessionhost to Log Analytics. Make sure you add this step into your AVD deployment sequence for future installations.\nFinding the Log Analytics Workspace There are more way to figure out the Log Analytics Workspace. The first route is through the Azure portal by going to the Azure Virtual Desktop blade, select the correct hostpool and search for diagnostic settings.\nA second way is by PowerShell using the command below. Use the provided workspace name.\n$HostpoolName = \u0026#34;Test-AVD-Host\u0026#34; $ResourceGroup = \u0026#34;Test-AVD-ResourceGroup\u0026#34; $hostpool¬†=¬†Get-AzWvdHostPool¬†-name¬†$HostpoolName¬†-ResourceGroupName¬†$ResourceGroup Get-AzDiagnosticSetting -ResourceId $hostpool.id | Select Name, @{Label=‚ÄùWorkSpace Name‚Äù;Expression={($_.WorkspaceId.Split(\u0026#34;/\u0026#34;)[-1])}} If you have all the needed information use the PowerShell below for installing the OMSExtension.\n$HostpoolName = \u0026#34;Test-AVD-Host\u0026#34; $ResourceGroup = \u0026#34;Test-AVD-ResourceGroup\u0026#34; $Workspace = \u0026#34;LA-Workspace-xxxxx\u0026#34; # region install Log Analytics Agent on Virutal Machine $ResourceGroup = ($hostpool).id.split(\u0026#34;/\u0026#34;)[4] $sessionhosts = Get-AzWvdSessionHost -HostpoolName $HostpoolName -ResourceGroupName $ResourceGroup $virtualMachines = @($sessionhosts.ResourceId.Split(\u0026#34;/\u0026#34;)[-1]) $workspaceKey = ($Workspace | Get-AzOperationalInsightsWorkspaceSharedKey).PrimarySharedKey $TemplateParameters = @{ workspaceId = $Workspace.CustomerId workspaceKey = $workspaceKey virtualMachines = $virtualMachines extensionNames = @(\u0026#34;OMSExtenstion\u0026#34;) } New-AzResourceGroupDeployment -ResourceGroupName $ResourceGroup -TemplateUri \u0026#34;https://raw.githubusercontent.com/srozemuller/Windows-Virtual-Desktop/master/Azure-Monitor/deploy-lawsagent.json\u0026#34; -TemplateParameterObject $TemplateParameters #endregion At the end, if you have installed the Log Analytics extension on the sessionhosts, the Windows events will appear in the Log Analytics Workspace. I used the Kusto Query below for searching for AVD-Agent events.\nEvent |¬†where¬†TimeGenerated¬†\u0026gt;¬†ago(6h) and¬†Source¬†==¬†\u0026#34;AVD-Agent\u0026#34; |¬†sort¬†by¬†TimeGenerated¬†desc |¬†project¬†TimeGenerated,¬†Computer,¬†EventLog,¬†EventID¬†,¬†EventLevelName,¬†RenderedDescription,¬†_ResourceId More information about Log Analytics and Windows event log check Collect Windows event log data sources with Log Analytics agent.\nMonitor workbook Using a monitor workbook can help creating a great overview. Although creating a complete workbook from scratch is not a part of the scope in this article, I will describe the basics from where to start.\nFirst we need some dashboard parameters like subscription and hostpool name.\nGo to Azure Monitor, click Workbook and click an Empty workbook\nIn the workbook click Add and choose Add Parameters\nFrom there you will be able to create dynamic parameters by executing queries. For example when requesting all subscriptions you will need to execute an Azure Resource Graph query.\nAfter creating the parameters add the query boxes via the Add button below and choose Add query. In this case I‚Äôve used two simple queries.\nEvent |¬†where¬†Source¬†==¬†\u0026#34;AVD-Agent\u0026#34; |¬†summarize¬†count()¬†by¬†Computer,¬†EventID Event |¬†where¬†Source¬†==¬†\u0026#34;AVD-Agent\u0026#34; |¬†sort¬†by¬†TimeGenerated¬†desc |¬†project¬†TimeGenerated,¬†Computer,¬†EventLog,¬†EventID¬†,¬†EventLevelName,¬†RenderedDescription,¬†_ResourceId Alerting There are some options for getting alerts I like to show. The first option is by using PowerShell.\nPowershell The script will need a AVD hostpool name and its resource group. All other information will found automatically. If there are results found the script will let you know.\n$HostpoolName = \u0026#34;Test-AVD-Host\u0026#34; $ResourceGroup = \u0026#34;Test-AVD-ResourceGroup\u0026#34; Try¬†{ $sessionHosts¬†=¬†Get-AzWvdSessionHost¬†-ResourceGroupName¬†$ResourceGroup -HostPoolName¬†$HostpoolName } Catch¬†{ Throw¬†\u0026#34;Error¬†getting¬†sessionhosts¬†$_\u0026#34; } if¬†($sessionHosts)¬†{ Write-Host¬†\u0026#34;Found¬†session¬†hosts¬†$sessionHosts\u0026#34; $VirtualMachine¬†=¬†Get-AzResource¬†-ResourceId¬†$sessionHosts[0].ResourceId $ExtentionResult¬†=¬†(Get-AzVMExtension¬†-VMName¬†$VirtualMachine.Name¬†-ResourceGroupName¬†$VirtualMachine.ResourceGroupName¬†-Name¬†\u0026#34;OMSExtenstion\u0026#34;) $WorkspaceId¬†=¬†($ExtentionResult.PublicSettings¬†|¬†ConvertFrom-Json).workspaceId $query¬†=¬†@\u0026#34; Event |¬†where¬†TimeGenerated¬†\u0026gt;¬†ago(6h)¬†and¬†Source¬†==¬†\u0026#39;AVD-Agent\u0026#39;¬†|¬†sort¬†by¬†TimeGenerated¬†desc¬†|¬†project¬†TimeGenerated,¬†Computer,¬†EventLog,¬†EventID,¬†EventLevelName,¬†RenderedDescription,¬†_ResourceId \u0026#34;@ $Results¬†=¬†Invoke-AzOperationalInsightsQuery¬†-WorkspaceId¬†$WorkspaceId¬†-Query¬†$Query if¬†($Results)¬†{ $Count¬†=¬†$($Results.Results).count Write-Warning¬†\u0026#34;AVD¬†Agent¬†errors¬†found!¬†Total:¬†$Count¬†\u0026#34; } else¬†{ Write-Host¬†\u0026#34;No¬†errors¬†found\u0026#34; } } else¬†{ Write-Error¬†\u0026#34;No¬†hosts¬†found¬†!\u0026#34; } Azure Monitor Alert rules are rulesets in the Azure Monitor. Rules can be configured based on scopes (resource types). In this configuration I use the Log Analytics resource. Every resource has it own condition sets. When using Log Analytics you will able to run a custom log search, a user defined query.\nAction groups The common parts at every rule is an action group. This can be a webhook, email or ITSM tool. In this case I will configure an action group based on an e-mailaddress.\nI used the code below to deploy an action group first based on an ARM template.\n$TemplateFileLocation¬†=¬†\u0026#39;https://github.com/srozemuller/Windows-Virtual-Desktop/blob/master/Azure-Monitor/deploy-actiongroup.json\u0026#39; $TemplateParameters¬†=¬†@{ actionGroupName¬†=¬†\u0026#34;Ag-MailToAVDAdmin\u0026#34; actionGroupShortName¬†=¬†\u0026#34;AVD\u0026#34; receiverName¬†=¬†\u0026#34;AVD¬†Admin\u0026#34; receiverEmailAddress¬†=¬†\u0026#34;admin@wvd-ilike.it\u0026#34; tags¬†=¬†@{AVD¬†=¬†\u0026#34;TestEnvironment\u0026#34;} } $ActionGroup¬†=¬†New-AzResourceGroupDeployment¬†-ResourceGroupName¬†$HostpoolResourceGroup¬†-Name¬†\u0026#39;Deploy-ActionGroup-FromPowerShell\u0026#39;¬†-TemplateUri¬†$TemplateFileLocation¬†@TemplateParameters More information about creating action groups in the portal check the Microsoft docs.\nAction rule After the alert group is configured all the needed parts are there to create the alert rule. As mentioned before I will use the Log Analytics resource and will configure a custom log search as condition.\nIn this case I use a simple query which search for Error events with a source AVD-Agent.\nEvent¬†|¬†where¬†EventLevelName¬†==¬†\u0026#39;Error\u0026#39;¬†and¬†Source¬†==¬†\u0026#39;AVD-Agent\u0026#39;¬†|¬†sort¬†by¬†TimeGenerated The last part is configuring the action. After clicking Add action groups, select the just created action group. At the end I configured an alert rule which will send me an e-mail if the result is above 0.\nWhen finished the configuration the setup will look something like below.\nAutomated An option is to create an alert rule through the Azure portal. An another option is to create a rule automated, with PowerShell in this example. For a good start check the Microsoft docs about creating a new schedule query rule. From there all the needed commands will explained. To get all the information I had to do some reverse engineering which results in a script like below.\n$HostpoolName = \u0026#34;Test-AVD-Host\u0026#34; $ResourceGroup = \u0026#34;Test-AVD-ResourceGroup\u0026#34; $hostpool¬†=¬†Get-AzWvdHostPool¬†-name¬†$HostpoolName¬†-ResourceGroupName¬†$HostpoolResourceGroup $Query¬†=¬†\u0026#34;Event¬†|¬†where¬†EventLevelName¬†==¬†\u0026#39;Error\u0026#39;¬†and¬†Source¬†==¬†\u0026#39;AVD-Agent\u0026#39;¬†|¬†sort¬†by¬†TimeGenerated\u0026#34; $DataSourceId¬†=¬†(Get-AzDiagnosticSetting¬†-ResourceId¬†$hostpool.id).WorkspaceId $Source¬†=¬†New-AzScheduledQueryRuleSource¬†-Query¬†\u0026#34;Event¬†|¬†where¬†EventLevelName¬†==¬†\u0026#39;Error\u0026#39;¬†and¬†Source¬†==¬†\u0026#39;AVD-Agent\u0026#39;¬†|¬†sort¬†by¬†TimeGenerated\u0026#34;¬†-DataSourceId¬†$DataSourceId $Schedule¬†=¬†New-AzScheduledQueryRuleSchedule¬†-FrequencyInMinutes¬†5¬†-TimeWindowInMinutes¬†5 # use the actiongroup variable from the previous step $AznsAction¬†=¬†New-AzScheduledQueryRuleAznsActionGroup¬†-ActionGroup¬†$ActionGroup.Outputs.actionGroupResourceId.value¬†-EmailSubject¬†\u0026#34;AVD-Agent¬†Eventlog¬†Errors\u0026#34;¬†-CustomWebhookPayload¬†\u0026#34;{}\u0026#34; $TriggerCondition¬†=¬†New-AzScheduledQueryRuleTriggerCondition¬†-ThresholdOperator¬†\u0026#34;GreaterThan\u0026#34;¬†-Threshold¬†0 $AlertingAction¬†=¬†New-AzScheduledQueryRuleAlertingAction¬†-AznsAction¬†$AznsAction¬†-Severity¬†1¬†-Trigger¬†$TriggerCondition $QueryRuleParameters¬†=¬†@{ ResourceGroupName¬†=¬†$HostpoolResourceGroup Location¬†=¬†\u0026#34;West¬†Europe\u0026#34; Enabled¬†=¬†$true Name¬†=¬†\u0026#34;AVD-Agent¬†Eventlog¬†Errors\u0026#34; Description¬†=¬†\u0026#34;Gets¬†AVD-Agent¬†Eventlog¬†Errors¬†from¬†sessionhosts\u0026#34; Source¬†=¬†$Source Schedule¬†=¬†$Schedule Action¬†=¬†$alertingAction } New-AzScheduledQueryRule¬†@QueryRuleParameters Prevent issues In case of better safe then sorry the best way is to setup the Network Security Group (NSG) correctly. Most of the time outgoing secure traffic (HTTPS, port 443) is permitted. But in case of some limitations this is a way how to open the firewall with less destinations as possible.\nNetwork Security Group In this part I will show a possible way how to configure the NSG in case of there are outgoing limitations.\nDuring the time the NSG had a lot of improvement. The option using destination service tags is one of them. Now AVD is a part for the Azure family a new service tag is available, WindowsVirtualDesktop. All other services tags are available as well. Using them will things make things a lot easier.\nI wrote a PowerShell function which will help added NSG rules automatically. The function will check the first deny rule and will place the needed rules above them.\n$NSGName = \u0026#34;NSG\u0026#34; $ResourceGroupName = \u0026#34;ResourceGroup\u0026#34; function¬†add-firewallRule($NSG,¬†$port,¬†$ServiceTag)¬†{ #¬†Pick¬†random¬†number¬†for¬†setting¬†priority.¬†It¬†will¬†exclude¬†current¬†priorities. $FirstDenyRule¬†=¬†($NSG¬†|¬†Get-AzNetworkSecurityRuleConfig¬†|¬†where¬†{$_.access¬†-eq¬†\u0026#34;Deny\u0026#34;}¬†|¬†select¬†Priority).priority[0] $InputRange¬†=¬†100..($FirstDenyRule-1) $priority¬†=¬†Get-Random¬†-InputObject¬†$InputRange¬†$nsgParameters¬†=¬†@{ Name¬†=¬†\u0026#34;Allow-$ServiceTag-over-$port\u0026#34; Description¬†=¬†\u0026#34;Allow¬†port¬†$port¬†to¬†$ServiceTag\u0026#34; Access¬†=¬†\u0026#39;Allow\u0026#39; Protocol¬†=¬†\u0026#34;Tcp\u0026#34;¬†Direction¬†=¬†\u0026#34;Outbound\u0026#34;¬†Priority¬†=¬†$priority¬†SourceAddressPrefix¬†=¬†\u0026#34;*\u0026#34; SourcePortRange¬†=¬†\u0026#34;*\u0026#34; DestinationAddressPrefix¬†=¬†$ServiceTag¬†DestinationPortRange¬†=¬†$port } if¬†($NSG.SecurityRules.Name.Contains($nsgParameters.Name)){ Write-Host¬†\u0026#34;Rule¬†already¬†exists.\u0026#34; } else¬†{$NSG¬†|¬†Add-AzNetworkSecurityRuleConfig¬†@NSGParameters¬†|¬†Set-AzNetworkSecurityGroup¬†} } $NSG¬†=¬†Get-AzNetworkSecurityGroup¬†-name¬†$NSGName¬†-ResourceGroupName¬†$ResourceGroupName add-firewallRule¬†-NSG¬†$NSG¬†-ServiceTag¬†WindowsVirtualDesktop¬†-port¬†443 At the end you are able to monitor AVD required URL‚Äôs.\nMore info about the safe URL list, Network Security Group, Azure Monitor check the links below:\nSafe URL list: https://docs.microsoft.com/en-us/azure/virtual-desktop/safe-url-list\nProtect AVD: https://docs.microsoft.com/en-us/azure/firewall/protect-windows-virtual-desktop\nAzure Monitor: https://docs.microsoft.com/nl-nl/azure/azure-monitor/overview\nThank you for reading my blog monitor azure virtual desktop required url\u0026amp;#8217;s with log analytics workspace. I hope you got a bit inspired. Enjoy your day and happy automating üëã ","date":"January 22, 2021","image":"http://localhost:1313/monitor-azure-virtual-desktop-required-urls-with-log-analytics-workspace/monitoring-required-urls.png","permalink":"/monitor-azure-virtual-desktop-required-urls-with-log-analytics-workspace/","title":"Monitor Azure Virtual Desktop required URL\u0026#8217;s with Log Analytics Workspace"},{"categories":["Azure"],"contents":"Past week (at Monday 11 January) I spoke at he Dutch Azure Virtual Desktop User Group. The subject was automation in a Azure Virtual Desktop combined with the Azure Shared Image Gallery. The session is now available at YouTube.\nCheck the video below.\nIf you like more video‚Äôs from the Dutch WVD community please subscribe to the Dutch Azure Virtual Desktop User Group channel\nMore information about automation and Azure Virtual Desktop please check my series about Azure Virtual Desktop Image Management Automated\n","date":"January 17, 2021","image":"http://localhost:1313/session-available-dutch-windows-virtual-desktop-user-group/dutch-avd-community.jpg","permalink":"/session-available-dutch-windows-virtual-desktop-user-group/","title":"Session available Dutch Azure Virtual Desktop User Group"},{"categories":["Graph API","Azure Virtual Desktop","DevOps"],"contents":"An Azure DevOps environment consists of projects. Before create anything in DevOps like a board, repositories or pipelines you first need a project. In this second part of the series Prepare Azure DevOps for Azure Virtual Desktop we are diving into that DevOps part, a project. At the end of this post you will be able to deploy WVD with DevOps automated.\nIntroduction In this series about Prepare Azure DevOps for Azure Virtual Desktop deployment I will post some small blogposts which will help you setting up a prepared DevOps environment fully automated. This to achieve the goal to deploy WVD with DevOps. At the end of this series you will able to create a script which let you fill in an application name, a projectname and a PAT code for connecting to DevOps and will run all the needed steps to start with DevOps.\nThis series consists of the following subjects:\nApp registration in Azure Active Directory Create an Azure DevOps project Add a Service connection in the DevOps project Create a pipeline from a source project Table Of Contents Introduction Parameters Basic settings JSON Body Create the project Parameters The script consists of some parameters you will need. In this paragraph I will explain which parameters and what they mean.\nOrganisation: This is the Azure DevOps organisation and first thing you need. Please check About organization management in Azure DevOps about creating an Azure DevOps organisation or go to https://dev.azure.com.\nPersonalToken: A personal access token (PAT) is used as an alternate password to authenticate into Azure DevOps. You are able to create a PAT under you own personal account. Want to learn how to create, use, modify, and revoke PATs for Azure DevOps please check Authenticate with personal access tokens.\nProjectName \u0026amp; ProjectDescription as the name says, just a name and a description.\nVisibility: Is the project visible or not, accepted values are private or public.\nParam( [Parameter(Mandatory¬†=¬†$True)] [string]$Organisation, [Parameter(Mandatory¬†=¬†$True)] [string]$PersonalToken, [Parameter(Mandatory¬†=¬†$True)] [string]$ProjectName, [Parameter(Mandatory¬†=¬†$True)] [string]$ProjectDescription,¬†[ValidateSet(\u0026#34;private\u0026#34;,\u0026#34;public\u0026#34;)] [Parameter(Mandatory¬†=¬†$True)] [string]$Visibility ) Basic settings After you filled in the correct values the script will generate an authentication token based on the provided personal access token and an organisation URL like https://dev.azure.com/MyOrganisation\n$orgUrl¬†=¬†\u0026#34;https://dev.azure.com/$($organisation)\u0026#34; Write-Host¬†\u0026#34;Initialize¬†authentication¬†context\u0026#34;¬†-ForegroundColor¬†Yellow $token¬†=¬†[System.Convert]::ToBase64String([System.Text.Encoding]::ASCII.GetBytes(\u0026#34;:$($personalToken)\u0026#34;)) $header¬†=¬†@{authorization¬†=¬†\u0026#34;Basic¬†$token\u0026#34;¬†} JSON Body The next part is creating a JSON body for the POST request. As you can see most of the parameters are used in the JSON body. This will actually do the job. After setting the body the ConvertTo-Json will take care about the correct JSON format.\nCheck Create Azure DevOps Projects for more about creating projects with REST API.\n$jsonBody¬†=¬†@{ name¬†=¬†$ProjectName description¬†=¬†$ProjectDescription visibility¬†=¬†$Visibility capabilities¬†=¬†@{ versioncontrol¬†=¬†@{ sourceControlType¬†=¬†\u0026#34;Git\u0026#34; } processTemplate¬†=¬†@{ templateTypeId¬†=¬†\u0026#34;adcc42ab-9882-485e-a3ed-7678f01f66bc\u0026#34; } } } $newJSONBody¬†=¬†($jsonBody¬†|¬†ConvertTo-Json¬†-Depth¬†3) Create the project At last the project will be created in DevOps.\n$projectsUrl¬†=¬†\u0026#34;$orgUrl/_apis/projects?api-version=5.1\u0026#34; #¬†Create¬†new¬†project Write-Host¬†\u0026#34;Creating¬†project¬†$($ProjectName)\u0026#34;¬†-ForegroundColor¬†blue Invoke-RestMethod¬†-Uri¬†$projectsUrl¬†-Method¬†Post¬†-ContentType¬†\u0026#34;application/json\u0026#34;¬†-Headers¬†$header¬†-Body¬†$newJSONBody In the end you are able to run the script like:\nNew-DevOpsProject -Organisation \u0026#34;DemoDevOpsAutomation\u0026#34; -Personaltoken $token -ProjectName \u0026#34;Test new customer\u0026#34; -ProjectDescription \u0026#34;New description\u0026#34; -Visibility Private More about REST API and DevOps please check Get started with Azure DevOps and API\nFollow this series that help you deploy AVD with DevOps\nThank you for reading my blog prepare azure devops for azure virtual desktop deployment ‚Äì create devops project. I hope you got a bit inspired. Enjoy your day and happy automating üëã ","date":"January 15, 2021","image":"http://localhost:1313/prepare-azure-devops-for-azure-virtual-desktop-deployment-create-devops-project/man_working-scaled.jpg","permalink":"/prepare-azure-devops-for-azure-virtual-desktop-deployment-create-devops-project/","title":"Prepare Azure DevOps for Azure Virtual Desktop deployment ‚Äì Create DevOps project"},{"categories":["Graph API","Automation","Azure Virtual Desktop","Powershell"],"contents":"Using Azure DevOps is a really nice way to deploy resources in Azure, so also for Azure Virtual Desktop. Before you are able to deploy resources into Azure with pipelines you will need to setup a project and a service connection first. While configuring a service connection you will be asked for some specific tenant details.\nIn this short series I will explain every step you need to prepare DevOps for AVD deployment.\nIntroduction In this series about Prepare Azure DevOps for Azure Virtual Desktop deployment I will post three small blogposts which will help you setting up a prepared DevOps environment fully automated. At the end of this series you will able to create a script which let you fill in an application name, a projectname and a PAT code for connecting to DevOps and will run all the needed steps to start with DevOps.\nThis series consists of the following subjects:\nApp registration in Azure Active Directory Create an Azure DevOps project Add a Service connection in the DevOps project Create a pipeline from a source project Table Of Contents Introduction Register the application Connect to the tenant Role Based Access The script Create DevOps Service Connection (manual) Validation More info about application registrations Register the application In this first part we will take care about the application registration in the Azure Active Directory. The application is an Active Directory object just like a user. Depending on the application permissions you are able to do tasks into the tenant.\nAt the end of this post you will have an application registered in the tenants Active Directory with the correct Azure Virtual Desktop permissions.\nConnect to the tenant The first step is to login into the Azure tenant with PowerShell. Make sure you have the Az.Accounts and Az.Resources modules installed .Use the commands below.\nInstall-Module Az.Accounts Import-Module Az.Accounts Install-Module Az.Resources Import-Module Az.Resources Use Az.* to import all the Azure PowerShell modules.\nFor logging in into the tenant use the Connect-AzAccount. Since PowerShell 7 there will be a url provided and a code. Follow the instructions on the screen. TIP: if u are using VSCode the popup can be placed on the background, use alt-tab.\nFill in the code which provided by PowerShell\nChoose or fill in the correct account.\nYou may close the tab or browser now.\nRole Based Access After being connected lets start with the application registration. Because of security reasons the application will only have the roles assigned which are needed to prepare DevOps for AVD deployment.\nIn this case the application will be assigned to the Azure Virtual Desktop Contributor and User Access Administrator role.\nMore info about build-in roles please check Built-in roles for Azure Virtual Desktop.\nThe script The script will take care about registering the application and the service principal in the Azure AD. Also there will be a password generated based on a random guid.\nAs mentioned the application will get the Desktop Virtualisation Contributor role, after the registration was successful the additional User Access Administrator role will be added.\n$ApplicationName = \u0026#34;AVD DevOps Connection\u0026#34; try { if ($null -eq (Get-AzADApplication -DisplayName $ApplicationName)) { $IdentifierUri = ($ApplicationName).Replace(\u0026#34; \u0026#34;, $null) $PlainPassword = ([System.Convert]::ToBase64String([System.Text.Encoding]::UTF8.GetBytes((New-Guid)))) + \u0026#34;=\u0026#34; $SecureStringPassword = ConvertTo-SecureString -String $PlainPassword -AsPlainText -Force $AADApplication = New-AzADApplication -DisplayName $ApplicationName -IdentifierUris \u0026#34;http://$IdentifierUri\u0026#34; -Password $SecureStringPassword $AADServicePrincipal = New-AzADServicePrincipal -DisplayName $ApplicationName -Role \u0026#34;Desktop Virtualization Contributor\u0026#34; -ApplicationId $AADApplication.applicationId $AADServicePrincipal | New-AzRoleAssignment -RoleDefinitionName \u0026#34;User Access Administrator\u0026#34; Write-Host \u0026#34;Updated $ApplicationName to Service Principal\u0026#34; } else { Write-Host \u0026#34;$ApplicationName exists\u0026#34; } } catch { $ErrorMessage = $_ | ConvertFrom-Json Throw \u0026#34;Could not create application: $($ErrorMessage.message)\u0026#34; } $TenantInfo = Get-AzContext Write-Output \u0026#34;Subscription Id: $($TenantInfo.Subscription)\u0026#34; Write-Output \u0026#34;Subscription Name: $($TenantInfo.SubscriptionName)\u0026#34; Write-Output \u0026#34;Service Principal Id: $($AADApplication.ApplicationId.Guid)\u0026#34; Write-Output \u0026#34;Service Principal Key: $PlainPassword\u0026#34; Write-Output \u0026#34;Tenant Id: $($TenantInfo.Tenant)\u0026#34; At the end the script will output some values which are needed to create a service connection in DevOps. For now the service connection needs to be created manually. Later in this series we are using these values to create service connection automatically.\nCreate DevOps Service Connection (manual) Let‚Äôs say we will create a service connection manually now. Assuming you already have a DevOps environment and a project. Go to Project Settings -\u0026gt; Pipelines -\u0026gt; Service Connections.\nChoose New Service Connection on the right top and type Azure Resource Manager. In the next step choose Service Principal (Manual).\nValidation In the next step fill in the parameters you‚Äôve got from the script. At last validate the parameters.\nThen save the connection. You‚Äôre all set now.\nAs I mentioned before in the next part of this series I will show how to automate this last step. At the end you will know everything you need to prepare DevOps for AVD deployment.\nMore info about application registrations If you like more information about application registration please check the following pages.\nQuickstart: Register an application with the Microsoft identity platform How and why applications are added to Azure AD Steps to add a role assignment {{ bye }}\n","date":"January 3, 2021","image":"http://localhost:1313/prepare-azure-devops-for-windows-virtual-desktop-deployment-app-registration/devops.png","permalink":"/prepare-azure-devops-for-windows-virtual-desktop-deployment-app-registration/","title":"Prepare Azure DevOps for Azure Virtual Desktop deployment - App registration"},{"categories":["Automation","Azure","Powershell"],"contents":"In this quick blog post I will explain a way how to backup Azure resources and how to restore them with PowerShell, in JSON format, to an Azure Storage Account which is ‚Äúdeployment ready‚Äù.\nTable Of Contents Main idea Backup Azure Resources Restore Main idea The main idea is to export the resource configuration into a JSON file, with the parameters included. In case of emergency or when you like to setup the same resource to an another resourcegroup or subscription the JSON file can be download and deployed.\nBackup Azure Resources Before creating backups we need a backup location. Of course the file can be stored at every place you like. I‚Äôm using an Azure Storage Account. Because I like to have my backups all at the same place I will put all the resources into the same Storage Account. I recommend using low costs storage with Cool access tier and Standard performance. (in this example it is Hot)\nIn this case I am using a Network Security Group (NSG) for example. First I will check if there is already a container for this resource type, if not it will be created.\nThe next step is creating a JSON output for each NSG. The -IncludeParameterDefaultValue switch in the Export-AzResourceGroup PowerShell command will take care of the resource specific parameters. Without this switch only an empty ARM template will be returned. So you better be use it :).\nThe export automatically download the file into the current folder where the script is at. Next I will create a blobname (filename at the storage account). The last step is to upload the file to the storage account. This can be achieved by executing the Set-AzStorageBlobContent command.\n$Resources¬†=¬†Get-AzNetworkSecurityGroup if¬†($Resources)¬†{ $StorageAccount¬†=¬†get-AzStorageAccount¬†|¬†?¬†{¬†$_.StorageAccountName¬†-eq¬†\u0026#34;satestsrbackup\u0026#34;¬†} $ContainerName¬†=¬†($Resources.Id).Split(\u0026#34;/\u0026#34;)[-2].ToLower() $StorageAccountContainer¬†=¬†$StorageAccount¬†|¬†Get-AzStorageContainer¬†|¬†Where¬†{¬†$_.Name¬†-match¬†$ContainerName¬†} if¬†($null¬†-eq¬†$StorageAccountContainer){ $StorageAccount¬†|¬†New-AzStorageContainer¬†$ContainerName.ToLower() } foreach¬†($Resource¬†in¬†$Resources)¬†{ $BlobName¬†=¬†$($Resource).Name¬†+¬†\u0026#34;.json\u0026#34; $Export¬†=¬†$Resource¬†|¬†Export-AzResourceGroup¬†-IncludeParameterDefaultValue¬†-Force Set-AzStorageBlobContent¬†-File¬†$Export.Path¬†-Container¬†$ContainerName¬†-Blob¬†$BlobName¬†-Context¬†$StorageAccount.context¬†-Force } } Restore When a restore is needed just download the file from the storage account and use it as input for the New-AzResourceGroupDeployment command.\nNew-AzResourceGroupDeployment -TemplateParameterFile [file] Now you are able to backup Azure resources and to restore them fully automatically.\nThank you for reading my blog automatically backup azure resources to an azure storage account in json. I hope you got a bit inspired. Enjoy your day and happy automating üëã ","date":"December 21, 2020","image":"http://localhost:1313/automatically-backup-azure-resources-to-an-azure-storage-account-in-json/image-16.png","permalink":"/automatically-backup-azure-resources-to-an-azure-storage-account-in-json/","title":"Automatically backup Azure resources to an Azure Storage Account in JSON"},{"categories":["Azure Virtual Desktop","Security"],"contents":"Working remotely has a lot of advantages like less travel time, more focus to work (when the house is not full of kids :)), which contributes to working efficiently. But there are some concerns about security, and rightly so I think. Especially when you are used to ‚Äòsee‚Äô what happens on the screen in real life which help at least you think you are in control of your data. In this article I will show how to enable AVD screen protection automated.\nIntroduction In the world of working remotely a lot has been changed. Everyone is used to go to the office and see, speak and work together with colleagues. Now most of the time the only thing you see is your own screen without the knowledge what is going on on the other screens.\nThat‚Äôs the point where people gets nervous about a big thing: SECURITY. Where is the data, who is sharing it and how to avoid data leaks.\nIn this article I‚Äôm showing how to stop one of most easiest and common ways of the data leaks: Screen Capture. I will explain how to enable AVD Screen protection automated.\nThere are different ways how to enable the Screen Capture Protection and there also are different environment scenario‚Äôs. In this article I explain two environment scenario‚Äôs and two deployment scenario‚Äôs\nTable Of Contents Introduction Enable AVD Screen protection Prerequisites Finding AVD session hosts Run remote PowerShell command on the session host Enable AVD Screen Protection via ARM template Enable AVD Screen protection The screen capture protection feature prevents sensitive information from being captured on the client endpoints. When you enable this feature, remote content will be automatically blocked or hidden in screenshots and screen shares. Also, the Remote Desktop client will hide content from malicious software that may be capturing the screen. To enable this feature automated, please check the post below.\nPrerequisites Currently, only the Windows Desktop client supports screen capture protection and only full desktops are supported. Install the Az.Avd PowerShell module\nInstall-Module Az.Avd Import-Module Az.Avd Finding AVD session hosts In the first place we need to get all the Azure Virtual Desktop session hosts and their virtual machine names. To achieve that goal I use the Az.Avd PowerShell module.\n#¬†Get¬†one¬†of¬†the¬†current¬†production¬†VM\u0026#39;s¬†for¬†getting¬†the¬†share¬†image¬†gallery¬†info $sessionHosts¬†=¬†Get-Avdsessionhost -hostpool avd-hostpool -ResourceGroupName rg-demo-avd-01 Run remote PowerShell command on the session host One of the options is executing a local PowerShell script at the Windows Virtual Desktop sessionhost with the Invoke-AzVMRunComand command. This command allows you executing a local PowerShell script on the remote machine.\nNow we know every existing session host we are able to execute the PowerShell which enables the Screen Capture Protection\nforeach¬†($sessionHost¬†in¬†$sessionHosts.value){ $VirtualMachineName¬†=¬†($sessionHosts.Name.Split(\u0026#34;/\u0026#34;)[-1]).Split(\u0026#34;.\u0026#34;)[0] Get-AzVM¬†-Name¬†$VirtualMachineName¬†|¬†Invoke-AzVMRunCommand¬†-CommandId¬†\u0026#39;RunPowerShellScript\u0026#39;¬†-ScriptPath¬†[PathToLocalScript] } After executing the command you will see something like this.\nEnable AVD Screen Protection via ARM template For the ARM template lovers I created a simple extension. It is possible to deploy the extension to the virtual machine with the New-AzResourceGroupDeployment command. The ARM templates are stored in my Github repository.\nNew-AzResourceGroupDeployment¬†-ResourceGroupName¬†ResourceGroupName¬†-TemplateUri¬†https://raw.githubusercontent.com/srozemuller/Windows-Virtual-Desktop/master/Security/Extensions/deploy-WvdScpExtension.json¬†-vmName¬†cust-wvd-1 After the configuration has been set there is no way to screen capture a Azure Virtual Desktop session.\nNow you can see how easy it is to enable AVD screen protection.\nAdditional to the post more information is available at the following url: https://docs.microsoft.com/en-us/azure/virtual-desktop/screen-capture-protection.\nThank you for reading my blog enable screen capture protection for azure virtual desktop automated. I hope you got a bit inspired. Enjoy your day and happy automating üëã ","date":"December 16, 2020","image":"http://localhost:1313/enable-screen-capture-protection-for-azure-virtual-desktop-automated/image-1368.png","permalink":"/enable-screen-capture-protection-for-azure-virtual-desktop-automated/","title":"Enable Screen Capture Protection for Azure Virtual Desktop Automated"},{"categories":["Azure Virtual Desktop","Monitoring"],"contents":"Monitoring user environments will help you get sight of what is going on and will help you solve problems. An Azure Virtual Desktop environment isn‚Äôt an exception. It is recommended to configure a monitoring environment. In this post, I will show how to deploy Azure Monitor for AVD fully automated.\nIn this article, I will describe how to configure a monitoring environment from scratch fully automated. Especially when you have to manage more than one AVD environment this article will help you set up monitoring for all environments.\nTable Of Contents Prepare Log Analytics Workspace Parameters Functions Configuring LAWS Enable diagnostic settings on the Azure Virtual Desktop host pool Add session hosts to LAWS Workbooks How to use Update (25-03-2021) Prepare Log Analytics Workspace Preparing the Log Analytics Workspace (LAWS) is the first step. In these steps, we will create the workspace and will add the needed Windows events en Windows performance counters.\nThe needed events and counters are in two separate JSON files which need to be imported into a PowerShell script. The files are stored on my GitHub repository.\nParameters The template files and workspace names are mandatory. The script will create a workspace if no workspace name is provided.\nparam( [parameter(mandatory = $false)][string]$EventsTemplate, [parameter(mandatory = $false)][string]$CountersTemplate, [parameter(mandatory = $true)][string]$ResourceGroup, [parameter(mandatory = $true)][string]$Location, [parameter(mandatory = $false)][string]$WorkspaceName ) Import-Module Az.OperationalInsights if ($null -eq $WorkspaceName) { Write-Host \u0026#34;No Log Analytics Workspace name provided, creating new Workspace\u0026#34; $WorkspaceName = \u0026#34;log-analytics-wvd-\u0026#34; + (Get-Random -Maximum 99999) # workspace names need to be unique across all Azure subscriptions - Get-Random helps with this for the example code # Create the workspace New-AzOperationalInsightsWorkspace -Location $Location -Name $WorkspaceName -Sku Standard -ResourceGroupName $ResourceGroup } Write-Host \u0026#34;Created workspace $WorkspaceName\u0026#34; $WindowsEvents = Get-Content $EventsTemplate | ConvertFrom-Json $PerformanceCounters = Get-Content $CountersTemplate | ConvertFrom-Json Functions To make things easier I have created two functions. These will help to set the correct event levels and will be added as a dynamic switch statement.\nThe next function creates an Azure-friendly name because a slash (‚Äú/‚Äù) is not allowed. I‚Äôm using the event and counter name also as counter names to keep things clear.\nConfiguring LAWS Now it is time to configure the workspace with events and counters. Two for-each loops will take care of applying the Windows events and Windows performance counters.\nThe code below is a snippet from a complete script. Make sure you are using the complete PowerShell scripts (https://github.com/srozemuller/Windows-Virtual-Desktop/blob/master/Azure-Monitor/deploy-laws.ps1, and https://github.com/srozemuller/Windows-Virtual-Desktop/blob/master/Azure-Monitor/deploy-wvdhostpool-diagsettings.ps1).\nIf ($EventsTemplate) { foreach ($WindowsEventLog in $WindowsEvents.WindowsEvent.EventLogNames) { $Level = Get-CorrectEventLevels -EventLevels $WindowsEventLog.EventTypes $Name = Make-NameAzureFriendly -Name $WindowsEventLog.Value # Windows Event New-AzOperationalInsightsWindowsEventDataSource -ResourceGroupName $ResourceGroup -WorkspaceName $WorkspaceName -EventLogName $WindowsEventLog.Value -Name $Name @Level } } If ($CountersTemplate) { foreach ($CounterObject in $PerformanceCounters.WindowsPerformanceCounter) { $CounterObject foreach ($Counter in $CounterObject.Counters) { $Name = Make-NameAzureFriendly -Name $Counter.name $Parameters = @{ ObjectName = $CounterObject.Object InstanceName = $Counter.InstanceName CounterName = $Counter.CounterName IntervalSeconds = $Counter.IntervalSeconds Name = $Name } $Parameters New-AzOperationalInsightsWindowsPerformanceCounterDataSource -ResourceGroupName $ResourceGroup -WorkspaceName $WorkspaceName @parameters } } } After running the PowerShell you will notice the needed configuration had been set.\nEnable diagnostic settings on the Azure Virtual Desktop host pool Before Azure Monitor works for Azure Virtual Desktop, you need to enable diagnostic settings for the AVD Hostpool. To enable diagnostics, I use the Az.Avd PowerShell module. In the example below, I enable the SessionHostManagement category.\nThe script I‚Äôm using needs two parameters, the host pool and the workspace.\nCheck first if the Microsoft.Insights provider is registered in the subscription. Otherwise, the script will install the Microsoft.Insights provider.\nMake sure you use at least beta version 2.4.4-beta.21: https://github.com/srozemuller/AzAvd/releases/tag/v2.4.4-beta.22\nparam( [parameter(mandatory = $true)][string]$HostPoolName, [parameter(mandatory = $true)][string]$ResourceGroupName, [parameter(mandatory = $true)][string]$WorkspaceName, [parameter(mandatory = $true)][string]$WorkspaceResourceGroup, ) Import-Module Az.Avd try { # Check if the insightsprovide is registered otherwise register If (!(Register-AzResourceProvider -ProviderNamespace microsoft.insights).RegistrationState.Contains(\u0026#34;Registered\u0026#34;)){ Register-AzResourceProvider -ProviderNamespace microsoft.insights } while (!(Register-AzResourceProvider -ProviderNamespace microsoft.insights).RegistrationState.Contains(\u0026#34;Registered\u0026#34;)){ Write-Host \u0026#34;Resource provider microsoft.insights is not registered yet\u0026#34; Start-Sleep 1 } } catch { Throw \u0026#34;Not able to register insights provider, $_\u0026#34; } $avdParams = @{ hostpoolName = $HostPoolName resourceGroupName = $ResourceGroupName } $hostpool = Get-AvdHostPool @avdParams $parameters = @{ lAWorkspace = $WorkspaceName laResourceGroupName = $WorkspaceResourceGroup Categories = @(\u0026#34;SessionHostManagement\u0026#34;) } $hostpool | Enable-AvdDiagnostics -DiagnosticsName \u0026#39;avd-diagnostics\u0026#39; @parameters -Verbose Add session hosts to LAWS The last step is adding the session hosts to the workspace. Achieve this by installing the MicrosoftMonitoringAgent extension with an ARM template. This is the last section of the script.\n# region install Log Analytics Agent on Virutal Machine $sessionhosts = Get-AvdSessionHostResources -HostpoolName $HostpoolName -ResourceGroupName $ResourceGroupName $virtualMachines = @($sessionhosts.vmResources.name) $workspaceKey = ($Workspace | Get-AzOperationalInsightsWorkspaceSharedKey).PrimarySharedKey $TemplateParameters = @{ workspaceId = $Workspace.CustomerId workspaceKey = $workspaceKey virtualMachines = $virtualMachines extensionNames = @(\u0026#34;OMSExtenstion\u0026#34;) } New-AzResourceGroupDeployment -ResourceGroupName $ResourceGroup -TemplateUri \u0026#34;https://raw.githubusercontent.com/srozemuller/Windows-Virtual-Desktop/master/Azure-Monitor/deploy-lawsagent.json\u0026#34; -TemplateParameterObject $TemplateParameters #endregion Workbooks At the end when opening the workbook you will get something like below. The workbook in the example below is a default workbook from Microsoft. This workbook is stored under the workbook blade on the left of Azure Virtual Desktop.\nAll needed files can be downloaded from my GitHub repository.\nHow to use You need to run the scripts like the example below. After you downloaded them from the repository, use the following commands.\nFirst, make sure you are connected to Azure.\nConnect-AzAccount A browser will appear or, in case of PowerShell 7, you have to open a link and provide a code which PowerShell gives you.\nNext make sure you have selected the correct subscription.\nSet-AzContext -Subscriptionid [id] First run the deploy-laws.ps1 script. If the workspaceName parameter left empty the script will create one.\n./deploy-laws.ps1 -EventsTemplate ./events-configuration.json -CountersTemplate ./performance-counters-configuration.json -ResourceGroup [rg-laws-wvd] -Location westeurope Update (25-03-2021) Microsoft recommends removing the counters below to reduce data ingestion, and so costs. Based on the article below I moved these performance counters to a separate JSON file.\nIf you need these counters use the command below to add them.\nWe have removed 5 per-process performance counters from the default configuration, which has a minimal impact on UI and should reduce data ingestion by over 80% depending on your environment size and usage:\nProcess(*)\\% Processor Time Process(*)\\% User Time Process(*)\\ Thread count Process(*)\\ IO Write Operations/second Process(*)\\ IO Read Operations/second To still add these use the command below.\n./deploy-laws.ps1 -CountersTemplate ./high-ingestion-counters.json -ResourceGroup [rg-laws-wvd] -Location westeurope Microsoft announcement: https://techcommunity.microsoft.com/t5/windows-virtual-desktop/updated-guidance-on-azure-monitor-for-wvd/m-p/2236173\nTo deploy the diagnostic settings to WVD use the second script deploy-wvdhostpool-diagsettings.ps1. This script needs the WVD hostpool name and the exising or just created WorkspaceName.\n./deploy-wvdhostpool-diagsettings.ps1 -HostPoolName wvdhostpool -WorkspaceName la-workspace Thank you for reading my blog deploy azure monitor for avd automated. I hope you got a bit inspired. Enjoy your day and happy automating üëã ","date":"December 4, 2020","image":"http://localhost:1313/deploy-azure-monitor-for-windows-virtual-desktop-automated/monitoring-real.jpg","permalink":"/deploy-azure-monitor-for-windows-virtual-desktop-automated/","title":"Deploy Azure Monitor for AVD automated"},{"categories":["Azure","Azure Virtual Desktop"],"contents":"Since Windows Virtual Desktop is generally available a lot of improvements has been done. Think about ARM template deployment, MSIX app attach through the Azure portal and performance improvements in the WVD client and latency improvements at the most of the regions.\nSince 16 November 2020 a nice improvement has been added to the list, Windows Virtual Desktop RDP Short path. In this blogpost I will show you how to enable RDP Shortpath in WVD with some automation tasks.\nTable Of Contents Introduction Prerequisites Table of contents Create new virtual machine Local tasks on the virtual machine Configure Network Security Group Finalizing the virtual machine Verify connection Introduction First some basic background information about Windows Virtual Desktop and default network connectivity. A Windows Virtual Desktop environment consists of host pools, session hosts, workspaces and application groups. If everything has been configured correctly you will be able to connect with a domain account to https://rdweb.wvd.microsoft.com/arm/webclient/index.html. After connecting you will able to connect workspaces and/or applications.\nThe user validation, securing and other connection needs will be handled by the Windows Virtual Desktop gateway and broker. Because I am an automation guy and not a networking guy I will stop talking about networking now :). For a complete networking overview please check the Windows Virtual Desktop network connectivity page.\nRDP Shortpath is a feature of Windows Virtual Desktop that establishes a direct UDP-based transport between Remote Desktop Client and Session host. RDP uses this transport to deliver Remote Desktop and RemoteApp while offering better reliability and consistent latency. For a complete overview please check the Shortpath overview.\nPrerequisites Before continuing you will need:\nA Share Image Gallery with an image Session hosts booted from the image To support RDP Shortpath, the Windows Virtual Desktop client needs a direct line of sight to the session host Table of contents For enabling RDP ShortPath in a new image version you do the following steps:\nCreate a new virtual machine based on the latest image version Local tasks on the virtual machine Configure Network Security Group Finalizing the virtual machine Verify connection Create new virtual machine For creating a new virtual I worte a blogpost earlier. Please check my blogpost Windows Virtual Desktop Image Management Automated ‚Äì Part 1 ‚Äì Create WVD image version based on existing config with PowerShell. In that post I will explain how to create a new virtual machine based on a existing sessionhost. The only thing you need know is the Windows Virtual Desktop hostpool.\nAfter following that post at the end you will have an output somthing like this:\n![image-version-output](https://www.rozemuller.com/wp-content/uploads/2020/10/Image-1159.png)Use the information for the rest of this article. Local tasks on the virtual machine For enabling the RDP shortpath feature you will need to execute some command on the local virtual machine. Because I don‚Äôt want to login I‚Äôm using remote PowerShell which allows me to run commands with login into the virtual machine with RDP.\nFirst of all I created a PowerShell script for the local machine. Before you are able to setup a remote PowerShell connection you will need some parameters first. These are the parameters you received from the create new virtual machine part. Beside that you also need some modules and the add-firewallRule function.\nparam( [parameter(mandatory = $true, ValueFromPipelineByPropertyName)]$virtualMachineName, [parameter(mandatory = $true, ValueFromPipelineByPropertyName)]$resourceGroupName, [parameter(mandatory = $true, ValueFromPipelineByPropertyName)]$password, [parameter(mandatory = $true, ValueFromPipelineByPropertyName)]$username ) import-module az.desktopvirtualization import-module az.network import-module az.compute function add-firewallRule() { param( [parameter(mandatory = $true, ValueFromPipelineByPropertyName)]$NSG, [parameter(mandatory = $true, ValueFromPipelineByPropertyName)]$Name, [parameter(mandatory = $true, ValueFromPipelineByPropertyName)]$Port, [parameter(mandatory = $true, ValueFromPipelineByPropertyName)]$Protocol, [parameter(mandatory = $false, ValueFromPipelineByPropertyName)]$Source ) if ($null -eq $Source){$Source = \u0026#34;*\u0026#34;;$SourceReadable=\u0026#34;Any\u0026#34;} # Pick random number for setting priority. It will exclude current priorities. $InputRange = 100..200 $Exclude = ($NSG | Get-AzNetworkSecurityRuleConfig | select Priority).priority $RandomRange = $InputRange | Where-Object { $Exclude -notcontains $_ } $priority = Get-Random -InputObject $RandomRange $nsgParameters = @{ Name = $Name Description = $Name Access = \u0026#39;Allow\u0026#39; Protocol = $Protocol Direction = \u0026#34;Inbound\u0026#34; Priority = $priority SourceAddressPrefix = $Source SourcePortRange = \u0026#34;*\u0026#34; DestinationAddressPrefix = \u0026#34;*\u0026#34; DestinationPortRange = $Port } $NSG | Add-AzNetworkSecurityRuleConfig @NSGParameters | Set-AzNetworkSecurityGroup } The code below will take care about enabling remote PowerShell on the virtual machine by installing the EnableRemotePS extention. It will also create a NSG rule which allows you to connect. For security reasons I recommend providing a source from where the remote PowerShell connection will be made instead of any. In this example I‚Äôm using 127.0.0.1, you will need to change that source of course. $vm = Get-AzVM -name $virtualMachineName -ResourceGroupName $resourceGroupName $virtualNetworkSubnet = (Get-AzNetworkInterface -ResourceId $vm.NetworkProfile.NetworkInterfaces.id).IpConfigurations.subnet.id $NSG = Get-AzNetworkSecurityGroup | ? { $_.subnets.id -eq $virtualNetworkSubnet }# Enabling Powershell Remote Extention $virtualMachinePublicIp = (Get-AzPublicIpAddress | where { $_.name -match $VirtualMachineName }).IpAddress $connectionUri = \u0026#34;https://\u0026#34; + $virtualMachinePublicIp + \u0026#34;:5986\u0026#34; [securestring]$secStringPassword = ConvertTo-SecureString $password -AsPlainText -Force [pscredential]$creds = New-Object System.Management.Automation.PSCredential ($userName, $secStringPassword) $session = $null # Adding Remote PowerShell Extention and NSG configuration Write-Output \u0026#34;Enabling Powershell Remote Extention\u0026#34; Invoke-AzVMRunCommand -CommandId \u0026#34;EnableRemotePS\u0026#34; -VM $vm add-firewallRule -NSG $NSG -port 5986 -Source 127.0.0.1 -Protocol \u0026#34;TCP\u0026#34; -Name \u0026#34;Allow-port-5986\u0026#34; After the extention has been installed and the NSG has been configured you can setup a remote PowerShell connection. while (!($session)) { $session = New-PSSession -ConnectionUri $connectionUri -Credential $creds -SessionOption (New-PSSessionOption -SkipCACheck -SkipCNCheck -SkipRevocationCheck) Write-Output \u0026#34;Creating Remote Powershell session\u0026#34; $session } After the session has been established we are able to execute remote commands.\nIn the first place I will set the execution policy to unrestricted so my script will run. Next the script will test if there is a C:\\Scripts directory allready, if it does not exist we will create one.\nThe next part will download the real script which will configure the needed register keys and the local Windows Firewall. At last it will execute the script.\n# Configure base needs Invoke-Command -Session $session -ScriptBlock { Set-ExecutionPolicy -ExecutionPolicy Unrestricted } Invoke-Command -Session $session -ScriptBlock { if(!(Test-Path C:\\Scripts)){New-Item -ItemType \u0026#34;directory\u0026#34; -Path \u0026#34;C:\\Scripts\u0026#34; } } # RDP Short path Invoke-Command -Session $session -ScriptBlock { Invoke-WebRequest -uri https://raw.githubusercontent.com/srozemuller/Windows-Virtual-Desktop/master/SessionHost-Management/Prepare-ForRDPShortPath.ps1 -OutFile C:\\Scripts\\Perpare-ForRDPShortPath.ps1} Invoke-Command -Session $session -ScriptBlock { C:\\Scripts\\Perpare-ForRDPShortPath.ps1 } Disconnect-PSSession -Session $session Remove-PSSession -Session $session After the script has run locally you will see the prepare script in C:\\Scripts. The Prepare-ForRDPShortpath.ps1 content will take care of the register keys and the local Windows Firewall\n$WinstationsKey = \u0026#39;HKLM:\\SYSTEM\\CurrentControlSet\\Control\\Terminal Server\\WinStations\u0026#39; if(Test-Path $WinstationsKey){ New-ItemProperty -Path $WinstationsKey -Name \u0026#39;fUseUdpPortRedirector\u0026#39; -ErrorAction:SilentlyContinue -PropertyType:dword -Value 1 -Force New-ItemProperty -Path $WinstationsKey -Name \u0026#39;UdpPortNumber\u0026#39; -ErrorAction:SilentlyContinue -PropertyType:dword -Value 3390 -Force } New-NetFirewallRule -DisplayName \u0026#39;Remote Desktop - Shortpath (UDP-In)\u0026#39; -Action Allow -Description \u0026#39;Inbound rule for the Remote Desktop service to allow RDP traffic. [UDP 3390]\u0026#39; -Group \u0026#39;@FirewallAPI.dll,-28752\u0026#39; -Name \u0026#39;RemoteDesktop-UserMode-In-Shortpath-UDP\u0026#39; -PolicyStore PersistentStore -Profile Domain, Private -Service TermService -Protocol udp -LocalPort 3390 -Program \u0026#39;%SystemRoot%\\system32\\svchost.exe\u0026#39; -Enabled:True Restart-Computer $env:computername -Force The local Windows Firewall has been set.\nThe needed registry keys.\nConfigure Network Security Group After the local settings has been done the next step is to add port 3390 to the correct NSG. In this step we will execute the add-firewallRule function again, this time without a source.\n# Adding the role for RDP Shortpath add-firewallRule -NSG $NSG -port 3390 -Protocol \u0026#34;UDP\u0026#34; -Name \u0026#34;Allow-3390\u0026#34; Finalizing the virtual machine The second last step is Sysprep the virtual machine and create a new version into the Shared Image Gallery. I talked about how to Sysprep and create a new version automatically in part 2 of Windows Virtual Desktop Image Management Automated.\nThe very last step is adding the new virtual machines to the Windows Virtual Desktop hostpool. How to achieve that goal fully automatically please check part 3 Create WVD sessionhosts on image version\nVerify connection At the end after connecting with the Windows Remote Client (download via Windows desktop client page) and the Workspace tab you are able to view the connection settings.\n","date":"November 22, 2020","image":"http://localhost:1313/enable-rdp-shortpath-for-windows-virtual-desktop-on-an-image-version-automated/rdp-shortpath-connections.png","permalink":"/enable-rdp-shortpath-for-windows-virtual-desktop-on-an-image-version-automated/","title":"Enable RDP ShortPath for WVD on an image version automated"},{"categories":["Automation","Azure","Powershell"],"contents":"By default the Azure Key Vault has softdelete enabled with a 90 day retention. This option will protect Key Vault items when deleted by accident. When deleted you are able to restore that item through the portal or PowerShell.\nBut what if someone has deleted the Key Vault itself with all the items and softdeleted items included. There is no option to restore a Key Vault. In this article I will describe a way how to backup and restore a Key Vault when deleted.\nTable Of Contents Azure Key Vault Backup strategy Backup items function Create a backup resource group Backup Azure Key Vault Upload to the storage account Restore the Azure Key Vault Azure Key Vault Backup strategy You have an Azure Key Vault with items, softdelete enabled and want to backup the items. The main idea is to backup every single item into a storage account with PowerShell from an Azure Function. Each Key Vault has its one storage account container. If the storage account is missing the script will create a storage account from an ARM template. The container will be created into to storage account you provided. When created the storage account- and container name will be added to the Key Vault configuration as tags.\nThe function will run every day.\nBackup items function To backup a Key Vault item we need to use the AZ.KeyVault PowerShell module. The module has backup commands available which let you choose the backup location. I made a choice to upload all the items once at the end so first we will backup the items to a temporary location.\nInitial we need to set some parameters like backup location and template location first.\n$storageAccountTemplateFile = \u0026#34;https://raw.githubusercontent.com/srozemuller/Azure/main/AzureStorageAccount/azuredeploy.json\u0026#34; $storageAccountTemplateParameters = \u0026#34;https://raw.githubusercontent.com/srozemuller/Azure/main/AzureStorageAccount/azuredeploy.parameters.json\u0026#34; $backupFolder = \u0026#34;$env:Temp\\KeyVaultBackup\u0026#34; $location = \u0026#34;West Europe\u0026#34; $backupLocationTag = \u0026#34;BackupLocation\u0026#34; $backupContainerTag = \u0026#34;BackupContainer\u0026#34; $global:parameters = @{ resourceGroupName = \u0026#34;RG-PRD-Backups-001\u0026#34; location = $location } For the backup part I wrote a function which accepts the Key Vault name to keep the code nice and clean. The function will take care about the item backup and write it to the temporary location. Later in the script we will call the function.\nfunction backup-keyVaultItems($keyvaultName) { #######Parameters #######Setup backup directory If ((test-path $backupFolder)) { Remove-Item $backupFolder -Recurse -Force } ####### Backup items New-Item -ItemType Directory -Force -Path \u0026#34;$($backupFolder)\\$($keyvaultName)\u0026#34; | Out-Null Write-Output \u0026#34;Starting backup of KeyVault to a local directory.\u0026#34; ###Certificates $certificates = Get-AzKeyVaultCertificate -VaultName $keyvaultName foreach ($cert in $certificates) { Backup-AzKeyVaultCertificate -Name $cert.name -VaultName $keyvaultName -OutputFile \u0026#34;$backupFolder\\$keyvaultName\\certificate-$($cert.name)\u0026#34; | Out-Null } ###Secrets $secrets = Get-AzKeyVaultSecret -VaultName $keyvaultName foreach ($secret in $secrets) { #Exclude any secrets automatically generated when creating a cert, as these cannot be backed up if (! ($certificates.Name -contains $secret.name)) { Backup-AzKeyVaultSecret -Name $secret.name -VaultName $keyvaultName -OutputFile \u0026#34;$backupFolder\\$keyvaultName\\secret-$($secret.name)\u0026#34; | Out-Null } } #keys $keys = Get-AzKeyVaultKey -VaultName $keyvaultName foreach ($kvkey in $keys) { #Exclude any keys automatically generated when creating a cert, as these cannot be backed up if (! ($certificates.Name -contains $kvkey.name)) { Backup-AzKeyVaultKey -Name $kvkey.name -VaultName $keyvaultName -OutputFile \u0026#34;$backupFolder\\$keyvaultName\\key-$($kvkey.name)\u0026#34; | Out-Null } } } Create a backup resource group Because I want to limit permissions to our backup engineers only I create a specific backup resourece group. In a later stadium I can use this resource group for other backup situations. Setting permissions is out of scope in this article.\nI‚Äôve set the parameter in the beginning of the script.\nIf the resource group does not exist the script will create one. If you already have a destination group, just change the parameter value.\nif ($null -eq (get-AzResourceGroup $global:parameters.resourceGroupName -ErrorAction SilentlyContinue)) { New-AzResourceGroup @global:parameters } The next step will check if any Key Vault has the tag ‚ÄúBackupLocation‚Äù. By setting this tag I know the Key Vault will backed up. If the tag is missing we first need to create a storage location. When the storage account is created the name will be used as the BackupLocation value.\nBecause the storage account name is globally unique we need to create a unique name. In the template I set a variable which will generate a unique value. I use a prefix in combination with the resourceGroupId. I haven‚Äôt seen issues with this combination yet since the a resourceGroupId is unique by itself.\n\u0026#34;variables\u0026#34;: { \u0026#34;storageAccountName\u0026#34;: \u0026#34;[toLower( concat( parameters(\u0026#39;storageNamePrefix\u0026#39;), uniqueString(resourceGroup().id) ) )]\u0026#34;, \u0026#34;location\u0026#34;: \u0026#34;[resourceGroup().location]\u0026#34; }, $keyvaults = Get-AzKeyVault if ($null -eq ($keyvaults | ? { $_.Tags.Keys -match \u0026#34;BackupLocation\u0026#34; })) { # if no backuplocation tags is available at any of the keyVaults we will create one first $deployment = New-AzResourceGroupDeployment -ResourceGroupName $global:parameters.resourceGroupName -TemplateUri $storageAccountTemplateFile -TemplateParameterUri $storageAccountTemplateParameters $backupLocation = $deployment.outputs.Get_Item(\u0026#34;storageAccount\u0026#34;).value } After creating a storage account we will update the Key Vault tags by merging with existing tags. Based on the Key Vaults name a container name will be created as well. This name will be used later.\nif ($deployment.ProvisioningState -eq \u0026#34;Succeeded\u0026#34;) { foreach ($keyvault in $keyvaults) { $containerName = $keyvault.VaultName.Replace(\u0026#34;-\u0026#34;, $null).ToLower() if (!(Get-aztag -ResourceId $keyvault.ResourceId | ? { $_.Tags.Keys -match $BackupLocationTag } )) { Update-AzTag $keyvault.ResourceId -operation Merge -Tag @{BackupLocation = $backupLocation; BackupContainer = $containerName } } } } Backup Azure Key Vault Now we have a backup resource group and every Key Vault knows about it, its time to create the backups. The script will read the BackupLocation and BackupContainer tag for every Key Vault and uses that combination to determine the backup location. If the container does not exist the script will create a container first.\nelse { foreach ($keyvault in $keyvaults) { $backupLocation = (get-azkeyvault -VaultName $keyvault.vaultname | ? { $_.Tags.Keys -match $BackupLocationTag}).tags.Get_Item($BackupLocationTag) $storageAccount = get-AzStorageAccount | ? { $_.StorageAccountName -eq $backupLocation } if ($null -eq (Get-aztag -ResourceId $keyvault.ResourceId | ? { $_.Tags.Keys -match $BackupContainerTag } )) { $containerName = $keyvault.VaultName.Replace(\u0026#34;-\u0026#34;, $null).ToLower() Update-AzTag $keyvault.ResourceId -operation Merge -Tag @{BackupContainer = $containerName } } $containerName = (get-azkeyvault -VaultName $keyvault.vaultname | ? { $_.Tags.Keys -match $backupContainerTag }).tags.Get_Item($backupContainerTag) if ($null -eq (Get-AzStorageContainer -Name $containerName -Context $storageAccount.context)) { New-AzStorageContainer -Name $containerName -Context $storageAccount.context } backup-keyVaultItems -keyvaultName $keyvault.VaultName } } Upload to the storage account After the items are backed up to the temporary location it is time to upload the file to the storage account container.\nforeach ($file in (get-childitem \u0026#34;$($backupFolder)\\$($keyvault.VaultName)\u0026#34;)) { Set-AzStorageBlobContent -File $file.FullName -Container $containerName -Blob $file.name -Context $storageAccount.context -Force } Restore the Azure Key Vault If you need to restore the Key Vault first you need to create a Key Vault itself. Mostly you will create a Key Vault with the same name. Due our namingconvention I‚Äôm able to determine the original Key Vault name based on the storage account container name.\nBy using the following commands you are able to restore items into a Key Vault.\n$resourceGroup = \u0026#34;RG-PRD-Backups-001\u0026#34; $storageAccountName = \u0026#34;kvbck5a6yyxhxttvtu\u0026#34; $storageAccountContainer = \u0026#34;kvprdmascustappinfo\u0026#34; $context = Get-AzStorageAccount -Name $storageAccountName -ResourceGroupName $resourceGroup $files = Get-AzStorageBlob -Container $storageAccountContainer -Context $context.Context $keyVaultName = \u0026#39;test-restore\u0026#39; foreach ($file in $files){ $tempDestination = \u0026#34;c:\\temp\\$($file.name).blob\u0026#34; Get-AzStorageBlobContent -Container kvprdmascustappinfo -Blob $file.name -Context $context.Context -Destination $tempDestination if ($file.name -match \u0026#34;key-\u0026#34; ){Restore-AzKeyVaultKey -VaultName $keyVaultName -InputFile $tempDestination} if ($file.name -match \u0026#34;secret-\u0026#34; ){Restore-AzKeyVaultSecret -VaultName $keyVaultName -InputFile $tempDestination} if ($file.name -match \u0026#34;certificate-\u0026#34; ){Restore-AzKeyVaultCertificate -VaultName $keyVaultName -InputFile $tempDestination} } At the end I created an Azure Function to take care of the backup part.\nI stored the file into my GitHub repository\nThank you for reading my blog disaster recovery plan for azure key vault using tags, powershell and azure function. I hope you got a bit inspired. Enjoy your day and happy automating üëã ","date":"November 17, 2020","image":"http://localhost:1313/disaster-recovery-plan-for-azure-key-vault-using-tags-powershell-and-azure-function/backup.png","permalink":"/disaster-recovery-plan-for-azure-key-vault-using-tags-powershell-and-azure-function/","title":"Disaster recovery plan for Azure Key Vault using tags, PowerShell and Azure Function"},{"categories":["Azure Virtual Desktop","Image Management","Monitoring"],"contents":"When using the Azure Virtual Desktop for a longer time and created images several times you will noticed you are not able to remove old images directly when a new version has been deployed. Or in case of a MSP when you need to manage more then one image after some time you will lose sight on images and versions. An image overview would be nice to have. In this article I will show how to do some image version control on a AVD environment.\nIn this part I will describe some options how to create an image overview with Azure Monitor creating a Workbook based on Kusto Query. Main goal is to do some image version control on a AVD environment.\nThis post is a part of the series Azure Virtual Desktop Image Management Automated.\nCreate AVD image version based on existing config with PowerShell ‚Äì Part 1 Save AVD image with Sysprep as Image Gallery version ‚Äì Part 2 Create AVD Sessionhosts based on Shared Image Gallery version ‚Äì Part 3 AVD housekeeping, removing all unused sessionhosts, disks and images ‚Äì Part 4 Monitor Image Versions with Azure Monitor ‚Äì Part 5 Table Of Contents Create dashboard parameters Latest image version per image Session host image version overview Final Kusto query Create dashboard parameters First we need some dashboard parameters like subscription and image name. With parameters the report will find the information out of Azure Resource Graph.\nAlthough creating a complete workbook from scratch is not a part of the scope in this article, I will describe the basics from where to start.\nGo to Azure Monitor, click Workbook and click an Empty workbook\nIn the workbook click Add and choose Add Parameters\nFrom there you will be able to create dynamic parameters by executing queries. For example when requesting all subscriptions you will need to execute an Azure Resource Graph query.\nsummarize Count = count() by subscriptionId | project value = strcat(\u0026#39;/subscriptions/\u0026#39;, subscriptionId), label = subscriptionId, Selected = Count \u0026gt;= 0 The parameters you need are Subscription, SubscritptionId and ImageName.\nI‚Äôve used the following queries to achieve that goal.\nresourcecontainers | where type =~ \u0026#34;microsoft.resources/subscriptions\u0026#34; | summarize Count = count() by subscriptionId | project Label = subscriptionId, Id = subscriptionId, Selected = Count \u0026gt;= 0 And for the image name parameter\nresources | where type =~ \u0026#34;microsoft.compute/virtualmachines\u0026#34; and isnotnull(properties.storageProfile.imageReference.exactVersion) | mvexpand imageVersion = properties.storageProfile.imageReference.exactVersion | extend imageInfo = split(properties.storageProfile.imageReference.id,\u0026#34;/\u0026#34;) | summarize Count=count() by Image=tostring(imageInfo[10]) | project Label =Image At the end it will properly looks like this\nLatest image version per image After creating parameters you can add modules by clicking the Add button (in the Workbook) and click Add Query.\nIn the first place I want to know which is the last image version per image. This will result in the following overview.\nFor creating this image overview i‚Äôve used the following Azure Resource Graph Kusto query.\nresources | where type contains \u0026#34;microsoft.compute/galleries/images/versions\u0026#34; | extend GalleryInfo = split(id,\u0026#34;/\u0026#34;) | project GalleryName=GalleryInfo[8], ImageName=GalleryInfo[10], ImageVersion=GalleryInfo[12], GalleryInfo[2] | summarize LastVersion=max(tostring(ImageVersion)) by tostring(GalleryName), tostring(ImageName) Session host image version overview The second overview will show the Azure Virtual Machines which are not on the latest version. It responds on the selected Image parameter.\nThe needed information is scattered over more resource types, to get the information together we need to join all the resources together. At first we query for Azure virtual machines which has an image version and extending the imageId and the exact version the virtual machine is using. Before creating a join we need to project the information first. This information will be used by the join statement.\nresources | where type =~ \u0026#34;microsoft.compute/virtualmachines\u0026#34; and isnotempty(properties.storageProfile.imageReference.exactVersion) | extend currentImageVersion = properties.storageProfile.imageReference.exactVersion | extend imageName=split(properties.storageProfile.imageReference.id,\u0026#34;/\u0026#34;)[10] | project tostring(imageName), tostring(currentImageVersion), vmId=id The the most tricky and fun part is to join other resource types. This query is quite complex because of multiple joins.\nThe main join will get the versions per image. The second join will check if the result is the last version and will put the result in a new column. This has been achieved by the summarize. If it is the latest version the result is yes, otherwise no üôÇ\nI only return the latest version to avoid getting the no-results as well even when an Azure Virutual Machine is on the latest version.\n| join kind=inner( resources | where type=~\u0026#39;microsoft.compute/galleries/images/versions\u0026#39; | extend imageName=split(id,\u0026#34;/\u0026#34;)[10] | project id, name, tostring(imageName) | join kind=inner ( resources | where type =~ \u0026#39;microsoft.compute/galleries/images/versions\u0026#39; | extend versionDetails=split(id,\u0026#34;/\u0026#34;) | project id, name, imageName=versionDetails[10], imageGallery=versionDetails[8], resourceGroup, subscriptionId | summarize LastVersion=max(tostring(name)) by tostring(imageName) , tostring(imageGallery), resourceGroup, subscriptionId ) on imageName | extend latestVersion = case(name != LastVersion, \u0026#34;No\u0026#34;, \u0026#34;Yes\u0026#34;) | where latestVersion == \u0026#34;Yes\u0026#34; ) on imageName Now we know the image version status the last part is matching the current Azure virtual machine version with the latest version.\n| extend vmLatestVersion = case(currentImageVersion != LastVersion, \u0026#34;No\u0026#34;, \u0026#34;Yes\u0026#34;) | extend vmDetails = split(vmId,\u0026#34;/\u0026#34;) | project vmLatestVersion,imageName, currentImageVersion, vmId, imageGallery, resourceGroup, subscriptionId, LastVersion This query will return the complete list with Azure Virtual Machines with latest versions yes and no. If you like to seperate the results you can add a where statement which shows the status you want. This could be usefull when counting rows by state.\n| where vmLatestVersion =~ \u0026#34;No\u0026#34; Final Kusto query The final Kusto query for Azure Resource Graph will result in the following\nresources | where type =~ \u0026#34;microsoft.compute/virtualmachines\u0026#34; and isnotempty(properties.storageProfile.imageReference.exactVersion) | extend currentImageVersion = properties.storageProfile.imageReference.exactVersion | extend imageName=split(properties.storageProfile.imageReference.id,\u0026#34;/\u0026#34;)[10] | project tostring(imageName), tostring(currentImageVersion), vmId=id | join kind=inner( resources | where type=~\u0026#39;microsoft.compute/galleries/images/versions\u0026#39; | extend imageName=split(id,\u0026#34;/\u0026#34;)[10] | project id, name, tostring(imageName) | join kind=inner ( resources | where type =~ \u0026#39;microsoft.compute/galleries/images/versions\u0026#39; | extend versionDetails=split(id,\u0026#34;/\u0026#34;) | project id, name, imageName=versionDetails[10], imageGallery=versionDetails[8], resourceGroup, subscriptionId | summarize LastVersion=max(tostring(name)) by tostring(imageName) , tostring(imageGallery), resourceGroup, subscriptionId ) on imageName | extend latestVersion = case(name != LastVersion, \u0026#34;No\u0026#34;, \u0026#34;Yes\u0026#34;) | where latestVersion == \u0026#34;Yes\u0026#34; ) on imageName | extend vmLatestVersion = case(currentImageVersion != LastVersion, \u0026#34;No\u0026#34;, \u0026#34;Yes\u0026#34;) | extend vmDetails = split(vmId,\u0026#34;/\u0026#34;) | project vmLatestVersion,imageName, currentImageVersion, vmId, imageGallery, resourceGroup, subscriptionId, LastVersion Thank you for reading this post about how to do some image version control for a AVD environment.\n{{ bye }}\n","date":"October 28, 2020","image":"http://localhost:1313/azure-virtual-desktop-image-management-automated-part-5-monitor-image-versions-with-azure-monitor/azure-monitor.png","permalink":"/azure-virtual-desktop-image-management-automated-part-5-monitor-image-versions-with-azure-monitor/","title":"Azure Virtual Desktop Image Management Automated ‚Äì Part 5 Monitor Image versions with Azure Monitor"},{"categories":["Azure Virtual Desktop","Image Management"],"contents":"The current situation, we have created new disks, snapshots, virtual machines, networks, images and session hosts. All the resources has been added to the existing AVD hostpool. Now it is time to cleanup the old resources, to keep everything nice and clean. In this part we will take care of removing components related to the old image version.\nThis post is a part of the series Azure Virtual Desktop Image Management Automated.\nCreate AVD image version based on existing config with PowerShell ‚Äì Part 1 Save AVD image with Sysprep as Image Gallery version ‚Äì Part 2 Create AVD Sessionhosts based on Shared Image Gallery version ‚Äì Part 3 AVD housekeeping, removing all unused sessionhosts, disks and images ‚Äì Part 4 Monitor Image Versions with Azure Monitor ‚Äì Part 5 Enroll MSIX packages automated ‚Äì Part 6 ‚Äì (coming soon) Table Of Contents Tags, tags, tags Get the resources Cleanup What if you don‚Äôt use tags Tags, tags, tags After deploying the entire environment you have created a new disk, a snapshot, Azure virtual machines, network interfaces, an image and session hosts. This a quite a lot of resources and you will need to do a good job keeping all these resources up-to-date. Well, there is a better housekeeping solution.\nI work a lot with tags. By tagging resources consequent you can find the relationship between resources by just one single click on a tag.\nLet‚Äôs say you have the Azure VM resources with the ImageVersion. By clicking the tag you will get every resources which is in relation with that specific ImageVersion.\nBy clicking the tag you will see the related disks, virtual machines, network interfaces and other resources with this tag value.\nNow we know how to get all the related resources based on a tag lets fill the automation part.\nGet the resources In the Azure portal you can click the tag you need and do your job (per tag). Because PowerShell doesn‚Äôt know the latest version, we need to find the latest images version used in the AVD hostpool.\nAVD Hostpool By knowing the AVD hostpool every other component can be find due some reverse engineering.\nparam( [parameter(mandatory = $true, ValueFromPipelineByPropertyName)]$hostpoolName, [parameter(mandatory = $true, ValueFromPipelineByPropertyName)]$deleteResources ) import-module az.desktopvirtualization import-module az.network import-module az.compute # Getting the hostpool first $hostpool = Get-AzWvdHostPool | ? { $_.Name -eq $hostpoolname } if ($null -eq $hostpool){ \u0026#34;Hostpool $hostpoolname not found\u0026#34; exit; } # Creating VM configuration based on existing VM\u0026#39;s in specific hostpool, selecting first one $hostpoolRg = ($hostpool).id.split(\u0026#34;/\u0026#34;)[4] Write-Output \u0026#34;Hostpool resourcegroup is $hostpoolRg\u0026#34; Shared Image Gallery To get the Shared Image Gallery information we need to know the sessionhost first. Sessionhost (actually an Azure VM) information contains storage information where the image information is stored.\n# Get one of the current production VM\u0026#39;s for getting the share image gallery info $sessionHosts = Get-AzWvdSessionHost -ResourceGroupName $hostpoolRg -HostPoolName $hostpool.name $existingSessionHost = ($sessionHosts.Name.Split(\u0026#34;/\u0026#34;)[-1]).Split(\u0026#34;.\u0026#34;)[0] $productionVm = Get-AzVM -Name $existingSessionHost The next part will use the Azure VM information to get the Shared Image Gallery information and will search for the oldest version. By sorting on published date and grabbing the first result you will get the oldest version.\n# Source: https://docs.microsoft.com/en-us/azure/virtual-machines/image-version-managed-image-powershell # Creating image version based on the image created few steps ago $imageReference = ((get-azvm -Name $productionVm[-1].name -ResourceGroupName $productionVm[-1].ResourceGroupName).storageprofile.ImageReference).id $galleryImageDefintion = get-AzGalleryImageDefinition -ResourceId $imageReference $galleryName = $imageReference.Split(\u0026#34;/\u0026#34;)[-3] $gallery = Get-AzGallery -Name $galleryName $versions = Get-AzGalleryImageVersion -ResourceGroupName $gallery.ResourceGroupName -GalleryName $gallery.Name -GalleryImageDefinitionName $galleryImageDefintion.Name $oldestVersion = $($versions | Sort-Object PublishedDate).name[0] \u0026#34;Found version $oldestVersion\u0026#34; Cleanup Knowing the oldest version we can search for any resource with the ImageVersion Tag where the value has the oldest version in it.\nIf the parameter $deleteResources is $true the resources will be deleted. Else the WhatIf will be used.\n$tag = @{ImageVersion = $oldestVersion} foreach ($resource in (Get-AzResource -Tag $tag)) { $resource if ($deleteResources) { Remove-AzResource -ResourceId $resource.ResourceId -Force } else{ Remove-AzResource -ResourceId $resource.ResourceId -WhatIf } } What if you don‚Äôt use tags Every resource related to tag ImageVersion has been deleted. I can imagine you don‚Äôt use tags at this moment. Don‚Äôt worry, there is a way to remove resources without the use of tags. In the foreach loop the script searches for a specific tag. If you don‚Äôt use tags this way searching for them is quite difficult :), so we need to find an another way.\nThere is a way by searching Azure virtual machines on a specific storage profile. The ImageReference object contains an image version which can be used. By filtering that object you will get virtual machine only. By using the objects in the virtual machine you can find\nforeach ($resource in (get-azvm | ? {$_.storageprofile.ImageReference.ExactVersion -eq $oldestVersion})) { $resource if ($deleteResources) { Remove-AzResource -ResourceId $resource.ResourceId -Force } else{ Remove-AzResource -ResourceId $resource.ResourceId -WhatIf } } Good luck and take care. This script will **DELETE **resources, i‚Äôm not responsible for any accidentally delete.\nThank you for reading my blog azure virtual desktop image management automated ‚Äì part 4 avd clean up unused resources. I hope you got a bit inspired. Enjoy your day and happy automating üëã ","date":"October 18, 2020","image":"http://localhost:1313/avd-clean-up-unused-resources/compute-single-vm.png","permalink":"/avd-clean-up-unused-resources/","title":"Azure Virtual Desktop Image Management Automated ‚Äì Part 4 AVD Clean up unused resources"},{"categories":["Azure","Security"],"contents":"At October 14, 2020 Mircosoft announced the public preview of ARM templates for adding secrets to Azure Key Vault. In this article I will explain a way how to use these templates.\nTable Of Contents Key Vault overview The templates The KeyVault parameter file PowerShell command Adding Key Vault items Splatting item parameters The main reason why using ARM templates is the less of PowerShell modules you need. Using less modules will result in less PowerShell module updates and commands which will break. Beside that using templates will avoid editing scripts.\nKey Vault overview The Azure Key Vault is a central location where you can securely store keys, secrets and certificates. Using the Key Vault will help you to store secret information outside of script or application. By configuring an access policy you are able to configure permissions for a user, group or service principal and to monitor access and use of Key Vault items.\nThe templates First I separated the Key Vault resource from the secret resource. This because the objectId parameter is mandatory. Based on the separate files I‚Äôve created a paramater file for the Azure Key Vault resource with only the mandatory parameters.\n{ \u0026#34;$schema\u0026#34;: \u0026#34;https://schema.management.azure.com/schemas/2019-04-01/deploymentTemplate.json#\u0026#34;, \u0026#34;contentVersion\u0026#34;: \u0026#34;1.0.0.0\u0026#34;, \u0026#34;parameters\u0026#34;: { \u0026#34;keyVaultName\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;metadata\u0026#34;: { \u0026#34;description\u0026#34;: \u0026#34;Specifies the name of the key vault.\u0026#34; } }, \u0026#34;location\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;defaultValue\u0026#34;: \u0026#34;[resourceGroup().location]\u0026#34;, \u0026#34;metadata\u0026#34;: { \u0026#34;description\u0026#34;: \u0026#34;Specifies the Azure location where the key vault should be created.\u0026#34; } }, \u0026#34;enabledForDeployment\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;bool\u0026#34;, \u0026#34;defaultValue\u0026#34;: false, \u0026#34;allowedValues\u0026#34;: [ true, false ], \u0026#34;metadata\u0026#34;: { \u0026#34;description\u0026#34;: \u0026#34;Specifies whether Azure Virtual Machines are permitted to retrieve certificates stored as secrets from the key vault.\u0026#34; } }, \u0026#34;enabledForDiskEncryption\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;bool\u0026#34;, \u0026#34;defaultValue\u0026#34;: false, \u0026#34;allowedValues\u0026#34;: [ true, false ], \u0026#34;metadata\u0026#34;: { \u0026#34;description\u0026#34;: \u0026#34;Specifies whether Azure Disk Encryption is permitted to retrieve secrets from the vault and unwrap keys.\u0026#34; } }, \u0026#34;enabledForTemplateDeployment\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;bool\u0026#34;, \u0026#34;defaultValue\u0026#34;: false, \u0026#34;allowedValues\u0026#34;: [ true, false ], \u0026#34;metadata\u0026#34;: { \u0026#34;description\u0026#34;: \u0026#34;Specifies whether Azure Resource Manager is permitted to retrieve secrets from the key vault.\u0026#34; } }, \u0026#34;tenantId\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;defaultValue\u0026#34;: \u0026#34;[subscription().tenantId]\u0026#34;, \u0026#34;metadata\u0026#34;: { \u0026#34;description\u0026#34;: \u0026#34;Specifies the Azure Active Directory tenant ID that should be used for authenticating requests to the key vault. Get it by using Get-AzSubscription cmdlet.\u0026#34; } }, \u0026#34;objectId\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;metadata\u0026#34;: { \u0026#34;description\u0026#34;: \u0026#34;Specifies the object ID of a user, service principal or security group in the Azure Active Directory tenant for the vault. The object ID must be unique for the list of access policies. Get it by using Get-AzADUser or Get-AzADServicePrincipal cmdlets.\u0026#34; } }, \u0026#34;keysPermissions\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;array\u0026#34;, \u0026#34;defaultValue\u0026#34;: [ \u0026#34;list\u0026#34; ], \u0026#34;metadata\u0026#34;: { \u0026#34;description\u0026#34;: \u0026#34;Specifies the permissions to keys in the vault. Valid values are: all, encrypt, decrypt, wrapKey, unwrapKey, sign, verify, get, list, create, update, import, delete, backup, restore, recover, and purge.\u0026#34; } }, \u0026#34;secretsPermissions\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;array\u0026#34;, \u0026#34;defaultValue\u0026#34;: [ \u0026#34;list\u0026#34; ], \u0026#34;metadata\u0026#34;: { \u0026#34;description\u0026#34;: \u0026#34;Specifies the permissions to secrets in the vault. Valid values are: all, get, list, set, delete, backup, restore, recover, and purge.\u0026#34; } }, \u0026#34;skuName\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;defaultValue\u0026#34;: \u0026#34;Standard\u0026#34;, \u0026#34;allowedValues\u0026#34;: [ \u0026#34;Standard\u0026#34;, \u0026#34;Premium\u0026#34; ], \u0026#34;metadata\u0026#34;: { \u0026#34;description\u0026#34;: \u0026#34;Specifies whether the key vault is a standard vault or a premium vault.\u0026#34; } } }, \u0026#34;resources\u0026#34;: [ { \u0026#34;type\u0026#34;: \u0026#34;Microsoft.KeyVault/vaults\u0026#34;, \u0026#34;apiVersion\u0026#34;: \u0026#34;2019-09-01\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;[parameters(\u0026#39;keyVaultName\u0026#39;)]\u0026#34;, \u0026#34;location\u0026#34;: \u0026#34;[parameters(\u0026#39;location\u0026#39;)]\u0026#34;, \u0026#34;properties\u0026#34;: { \u0026#34;enabledForDeployment\u0026#34;: \u0026#34;[parameters(\u0026#39;enabledForDeployment\u0026#39;)]\u0026#34;, \u0026#34;enabledForDiskEncryption\u0026#34;: \u0026#34;[parameters(\u0026#39;enabledForDiskEncryption\u0026#39;)]\u0026#34;, \u0026#34;enabledForTemplateDeployment\u0026#34;: \u0026#34;[parameters(\u0026#39;enabledForTemplateDeployment\u0026#39;)]\u0026#34;, \u0026#34;tenantId\u0026#34;: \u0026#34;[parameters(\u0026#39;tenantId\u0026#39;)]\u0026#34;, \u0026#34;accessPolicies\u0026#34;: [ { \u0026#34;objectId\u0026#34;: \u0026#34;[parameters(\u0026#39;objectId\u0026#39;)]\u0026#34;, \u0026#34;tenantId\u0026#34;: \u0026#34;[parameters(\u0026#39;tenantId\u0026#39;)]\u0026#34;, \u0026#34;permissions\u0026#34;: { \u0026#34;keys\u0026#34;: \u0026#34;[parameters(\u0026#39;keysPermissions\u0026#39;)]\u0026#34;, \u0026#34;secrets\u0026#34;: \u0026#34;[parameters(\u0026#39;secretsPermissions\u0026#39;)]\u0026#34; } } ], \u0026#34;sku\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;[parameters(\u0026#39;skuName\u0026#39;)]\u0026#34;, \u0026#34;family\u0026#34;: \u0026#34;A\u0026#34; }, \u0026#34;networkAcls\u0026#34;: { \u0026#34;defaultAction\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;bypass\u0026#34;: \u0026#34;AzureServices\u0026#34; } } } ] } The KeyVault parameter file The parameter file has the Key Vault name, permissions and objectId only. The objectId is an Azure AD object like a user, group or service principal. This id will get permission to the Key Vault.\n{ \u0026#34;$schema\u0026#34;: \u0026#34;https://schema.management.azure.com/schemas/2019-04-01/deploymentParameters.json#\u0026#34;, \u0026#34;contentVersion\u0026#34;: \u0026#34;1.0.0.0\u0026#34;, \u0026#34;parameters\u0026#34;: { \u0026#34;keyVaultName\u0026#34;: { \u0026#34;value\u0026#34;: \u0026#34;tst-kv-tst1\u0026#34; }, \u0026#34;objectId\u0026#34;: { \u0026#34;value\u0026#34;: \u0026#34;d1a2aa35-4e4b-4b94-a9f0-137027085535\u0026#34; }, \u0026#34;keysPermissions\u0026#34;: { \u0026#34;value\u0026#34;: [ \u0026#34;list\u0026#34;, \u0026#34;read\u0026#34; ] }, \u0026#34;secretsPermissions\u0026#34;: { \u0026#34;value\u0026#34;: [ \u0026#34;list\u0026#34;, \u0026#34;read\u0026#34; ] } } } PowerShell command New-AzResourceGroupDeployment -TemplateFile ./azuredeploy.json -TemplateParameterFile ./azuredeploy.parameters.json -ResourceGroupName groupname Adding Key Vault items After the Key Vault has been created you can fill it with items with an ARM template. I separated the resources otherwise you will be asked for an objectId every time.\n{ \u0026#34;$schema\u0026#34;: \u0026#34;https://schema.management.azure.com/schemas/2019-04-01/deploymentTemplate.json#\u0026#34;, \u0026#34;contentVersion\u0026#34;: \u0026#34;1.0.0.0\u0026#34;, \u0026#34;parameters\u0026#34;: { \u0026#34;keyVaultName\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;metadata\u0026#34;: { \u0026#34;description\u0026#34;: \u0026#34;Specifies the name of the key vault.\u0026#34; } }, \u0026#34;location\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;defaultValue\u0026#34;: \u0026#34;[resourceGroup().location]\u0026#34;, \u0026#34;metadata\u0026#34;: { \u0026#34;description\u0026#34;: \u0026#34;Specifies the Azure location where the key vault should be created.\u0026#34; } }, \u0026#34;secretName\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;metadata\u0026#34;: { \u0026#34;description\u0026#34;: \u0026#34;Specifies the name of the secret that you want to create.\u0026#34; } }, \u0026#34;secretValue\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;securestring\u0026#34;, \u0026#34;metadata\u0026#34;: { \u0026#34;description\u0026#34;: \u0026#34;Specifies the value of the secret that you want to create.\u0026#34; } } }, \u0026#34;resources\u0026#34;: [ { \u0026#34;type\u0026#34;: \u0026#34;Microsoft.KeyVault/vaults/secrets\u0026#34;, \u0026#34;apiVersion\u0026#34;: \u0026#34;2019-09-01\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;[concat(parameters(\u0026#39;keyVaultName\u0026#39;), \u0026#39;/\u0026#39;, parameters(\u0026#39;secretName\u0026#39;))]\u0026#34;, \u0026#34;location\u0026#34;: \u0026#34;[parameters(\u0026#39;location\u0026#39;)]\u0026#34;, \u0026#34;dependsOn\u0026#34;: [ ], \u0026#34;properties\u0026#34;: { \u0026#34;value\u0026#34;: \u0026#34;[parameters(\u0026#39;secretValue\u0026#39;)]\u0026#34; } } ] } Splatting item parameters For security reasons I recommend using parameter objects instead of parameter files. Parameter files contains plain text and often stored at a not save location. The technique which can be used is called splatting.\nA parameter object is a PowerShell hashtable which can be inserted as object.\n$templateParameters = @{ keyVaultName = \u0026#34;keyvault-name\u0026#34; secretName = \u0026#34;test-secret-4\u0026#34; secretValue = \u0026#34;test-secret-4-value\u0026#34; } New-AzResourceGroupDeployment -TemplateFile ./azuredeploy.json -TemplateParameterObject $templateParameters -ResourceGroupName resoursegroup The files can be found at my [GitHub](https://github.com/srozemuller/Azure/tree/main/Azure%20Keyvault) repository Thank you for reading my blog how to use key vault arm templates and deal with sensitive parameters. I hope you got a bit inspired. Enjoy your day and happy automating üëã ","date":"October 18, 2020","image":"http://localhost:1313/how-to-use-keyvault-arm-templates-and-deal-with-sensitive-parameters/arm-with-keyvault.png","permalink":"/how-to-use-keyvault-arm-templates-and-deal-with-sensitive-parameters/","title":"How to use Key Vault ARM templates and deal with sensitive parameters"},{"categories":["Azure Virtual Desktop","Image Management","Microsoft 365"],"contents":"This article is serie of posts about AVD Image Management Automated. In this part we are going to add new sessionhosts to an existing AVD hostpool based on a ARM template.\nThis post is a part of the series Azure Virtual Desktop Image Management Automated.\nCreate AVD image version based on existing config with PowerShell ‚Äì Part 1 Save AVD image with Sysprep as Image Gallery version ‚Äì Part 2 Create AVD Sessionhosts based on Shared Image Gallery version ‚Äì Part 3 AVD housekeeping, removing all unused sessionhosts, disks and images ‚Äì Part 4 Monitor Image Versions with Azure Monitor ‚Äì Part 5 Enroll MSIX packages automated ‚Äì Part 6 ‚Äì (coming soon) Table Of Contents Introduction Let‚Äôs start Hostpool registration key AVD Hostpool information AVD Sessionhost information Tags Splatting parameters Deploy with ARM Drainmode Introduction The situation so far: we have created a new image, did a Sysprep, set the image as a version into the Shared Image Gallery.\nIn this part we deploy new AVD sessionhosts and add it to an existing hostpool.\nRequirements There are some few requirements which are important.\nBefore continuing make sure have at least module version 2.5.1 of AZ.Resources in PowerShell. There is a bug fixed about passing a securestring into a templateParameterObject. (See github issue securitystring broken)\nI assume you know how to update PowerShell modules.\nLet‚Äôs start Information we need first:\nhostpoolname, administrator credentials (needed to join the sessionhosts into the domain), the number of instances you like to create (optional) drainmode to on at the old sessionshosts. param( [parameter(mandatory = $true)][string]$hostpoolName, [parameter(mandatory = $true)][string]$administratorAccountUsername, [parameter(mandatory = $true)][securestring]$administratorAccountPassword, [parameter(mandatory = $false)][int]$sessionHostsNumber, [parameter(mandatory = $true)][boolean]$setDrainModeToOn ) import-module az.desktopvirtualization import-module az.network import-module az.compute import-module az.resources #At least version 2.5.1 Hostpool registration key You need a registration key before you are able to add sessionhosts to a hostpool. Otherwise you will be notified.\nTo make code nice and clean I wrote a function for creating a AVD Hostpool registration key. After running the function you will receive the whole registration info.\nfunction create-wvdHostpoolToken($hostpoolName,$resourceGroup,$hostpoolSubscription) { $now = get-date # Create a registration key for adding machines to the AVD Hostpool $registered = Get-AzWvdRegistrationInfo -SubscriptionId $hostpoolSubscription -ResourceGroupName $resourceGroup -HostPoolName $hostpoolName if (($null -eq $registered.ExpirationTime) -or ($registered.ExpirationTime -le ($now))) { $registered = New-AzWvdRegistrationInfo -SubscriptionId $hostpoolSubscription -ResourceGroupName $resourceGroup -HostPoolName $hostpool.Name -ExpirationTime $now.AddHours(4) } if ($registered.Token) { } return $registered } AVD Hostpool information The function needs parameters and then continues.\nThe script depends on existing session hosts. If there are no hosts, the script will not continue.\n# Get the hostpool information $hostpool = Get-AzWvdHostPool | ? { $_.Name -eq $hostpoolName } $resourceGroup = ($hostpool).id.split(\u0026#34;/\u0026#34;)[4].ToUpper() $hostpoolSubscription = ($hostpool).id.split(\u0026#34;/\u0026#34;)[2] # Get current sessionhost information $sessionHosts = Get-AzWvdSessionHost -ResourceGroupName $resourceGroup -HostPoolName $hostpool.name # Doing some checks beforce continuing if ($null -eq $sessionHosts) { Write-Host \u0026#34;No sessionhosts found in hostpool $hostpoolname, exiting script\u0026#34; exit; } $hostPoolRegistration = create-wvdHostpoolToken -hostpoolName $hostpoolName -resourceGroup $resourceGroup -hostpoolSubscription $hostpoolSubscription if ($hostPoolRegistration) { $hostPoolToken = (ConvertTo-SecureString -AsPlainText -Force ($hostPoolRegistration).Token) } AVD Sessionhost information With all the needed information the script will continue to the final check about the number of session hosts that will be created. If the variable $sessionHostNumber is empty the script will count the existing AVD session hosts and will use that number.\nif ($null -eq $sessionHostsNumber) { $sessionHostsNumber = $sessionHosts.count Write-Host \u0026#34;No sessionHostsNumber provided, creating $sessionHostsNumber hosts\u0026#34; } In the next part all other variables will be filled based on a existing sessionhost. This is almost the same part I described in part one.\n(in the future I will create a module or function for that)\n# Get current sessionhost configuration, used in the next steps $existingHostName = $sessionHosts[-1].Id.Split(\u0026#34;/\u0026#34;)[-1] $prefix = $existingHostName.Split(\u0026#34;-\u0026#34;)[0] $currentVmInfo = Get-AzVM -Name $existingHostName.Split(\u0026#34;.\u0026#34;)[0] $vmInitialNumber = [int]$existingHostName.Split(\u0026#34;-\u0026#34;)[-1].Split(\u0026#34;.\u0026#34;)[0] + 1 $vmNetworkInformation = (Get-AzNetworkInterface -ResourceId $currentVmInfo.NetworkProfile.NetworkInterfaces.id) $virtualNetworkName = $vmNetworkInformation.IpConfigurations.subnet.id.split(\u0026#34;/\u0026#34;)[-3] $virutalNetworkResoureGroup = $vmNetworkInformation.IpConfigurations.subnet.id.split(\u0026#34;/\u0026#34;)[4] $virtualNetworkSubnet = $vmNetworkInformation.IpConfigurations.subnet.id.split(\u0026#34;/\u0026#34;)[-1] # Get the image gallery information for getting latest image $imageReference = ($currentVmInfo.storageprofile.ImageReference).id $galleryImageDefintion = get-AzGalleryImageDefinition -ResourceId $imageReference $galleryName = $imageReference.Split(\u0026#34;/\u0026#34;)[-3] $gallery = Get-AzGallery -Name $galleryName $latestImageVersion = (Get-AzGalleryImageVersion -ResourceGroupName $gallery.ResourceGroupName -GalleryName $gallery.Name -GalleryImageDefinitionName $galleryImageDefintion.Name)[-1] Tags To make things easier to find later I‚Äôm using tags which are deployed to every sessionhost component like networkinterface, virtual machine, disk, etc. By adding extra values into the hashtable you are able to extent the tags.\n$tags = @{ ImageVersion = $latestImageVersion.Name HostPool = $hostpoolName } Splatting parameters After setting all the variables the template parameter will be created. In most situations you will use a JSON template file in combination with a JSON parameter file. It is unnecessary filling a lot of parameters which are mostly known by the system, with all its consequences.\n$templateParameters = @{ resourceGroupName = $resourceGroup hostpoolName = $hostpoolName administratorAccountUsername = $administratorAccountUsername administratorAccountPassword = (ConvertTo-SecureString $administratorAccountPassword -AsPlainText -Force) createAvailabilitySet = $false hostpooltoken = $hostPoolToken vmInitialNumber = $vmInitialNumber vmResourceGroup = ($resourceGroup).ToUpper() vmLocation = $currentVmInfo.Location vmSize = $currentVmInfo.HardwareProfile.vmsize vmNumberOfInstances = $sessionHostsNumber vmNamePrefix = $prefix vmImageType = \u0026#34;CustomImage\u0026#34; vmDiskType = $currentVmInfo.StorageProfile.osdisk.ManagedDisk.StorageAccountType vmUseManagedDisks = $true existingVnetName = $virtualNetworkName existingSubnetName = $virtualNetworkSubnet virtualNetworkResourceGroupName = $virutalNetworkResoureGroup usePublicIP = $false createNetworkSecurityGroup = $false vmCustomImageSourceId = $imageReference availabilitySetTags = $tags networkInterfaceTags = $tags networkSecurityGroupTags = $tags publicIPAddressTags = $tags virtualMachineTags = $tags imageTags = $tags } Because of splatting parameters instead of using a parameter file you will need at least PowerShell module Az.Resources version 2.5.1. (As I mentioned at the beginning).\nAt the total end we use the new-AzResourceGroupDeployment command for deploying the environment by an ARM template.\nDeploy with ARM $deploy = new-AzresourcegroupDeployment -TemplateUri \u0026#34;https://raw.githubusercontent.com/srozemuller/Windows-Virtual-Desktop/master/Image%20Management/deploy-sessionhost-template.json\u0026#34; @templateParameters -Name \u0026#34;deploy-version-$($latestImageVersion.Name)\u0026#34; Drainmode After deployment was successful the ‚Äúold‚Äù session hosts drainmode should be set to ON. This will ensure no new sessions will be accepted. Actually you force new connections to new session hosts.\nif (($deploy.ProvisioningState -eq \u0026#34;Succeeded\u0026#34;) -and ($setDrainModeToOn)) { foreach ($sessionHost in $sessionHosts) { $sessionHostName = $sessionHost.name.Split(\u0026#34;/\u0026#34;)[-1] Update-AzWvdSessionHost -HostPoolName $Hostpoolname -ResourceGroupName $ResourceGroup -Name $sessionHostName -AllowNewSession:$false } } At my Github you will find the script and template file.\nThank you for reading my blog azure virtual desktop image management automated - part 3 create avd sessionhosts on image version with arm. I hope you got a bit inspired. Enjoy your day and happy automating üëã ","date":"October 14, 2020","image":"http://localhost:1313/create-avd-sessionhosts-based-on-shared-image-gallery-version-with-arm/azure-virtual-desktop.jpg","permalink":"/create-avd-sessionhosts-based-on-shared-image-gallery-version-with-arm/","title":"Azure Virtual Desktop  Image Management Automated - Part 3 Create AVD sessionhosts on image version with ARM"},{"categories":["Microsoft 365"],"contents":"Azure KeyVault is the security key management system in Azure where you can store keys, secrets and certificates. From that place you can use the items everywhere you like.\nTable Of Contents The main idea The foreach loop DevOps The main idea We using the certificates thumbprint for connecting to an Azure AD. While logged in we like to change application permissions based on a JSON file input. After changing that file the continuous integration (CI) proces in DevOps will take care about the application permission change at all of our customers.\nWhy using certificates?\nJust for a simple reason that a customer does not always have an Azure subscription. You will need a subscription or management group to create a project service connection in DevOps. So we need using certificates in combination with a service principal.\nAn Azure KeyVault can be used perfectly storing certificates at a save place.\nBefore getting logged in you need to get the certificates from the KeyVault and need to install the certificate in a local store first.\nAfter all a DevOps task is just running on VM with PowerShell. That means that an agent job is able to store things locally on the machine, like certificates in a certificate store. We can use that technology storing the certificates that makes us allow to connect to an Azure AD by certificate thumbprint.\n(Yes there is a Azure KeyVault task available for downloading items into seperate variables, actually in this case we need to iterate the whole KeyVault, not just one variable)\nDownload certificates from KeyVault\nThe script will accept two parameters, the vaultName and a tempStoreLocation. The vaultName parameter needs no introduction, the tempStoreLocation is the location where the certificates are stored first.\nparam( [parameter(mandatory = $true)][string]$vaultName, [parameter(mandatory = $true)][string]$tempStoreLocation ) The foreach loop In case there are more certificates needed I created a foreach loop. In the loop each certificate will be exported from the KeyVault to the temp staging location. In order to prevent certificate abuse we will set a password on the certificate, also stored in the KeyVault.\nI recommend a naming convention, this will make life a lot easier. To know the password and certificate combination we use that naming convention, [customersname]-[type].\nAt the $prefix variable the -type convention will be deleted, so the customer name will be left.\n$prefix = ($certificate.name).replace(\u0026#34;-certificate\u0026#34;, $null) In the loop\nIn the foreach loop the download and import will be executed\nforeach ($certificate in (Get-AzureKeyVaultCertificate -VaultName $vaultName)) { $prefix = ($certificate.name).replace(\u0026#34;-certificate\u0026#34;, $null) \u0026#34;Importing certificate $certificate\u0026#34; $cert = Get-AzureKeyVaultSecret -VaultName $vaultName -name $certificate.name $certBytes = [System.Convert]::FromBase64String($cert.SecretValueText) $certCollection = New-Object System.Security.Cryptography.X509Certificates.X509Certificate2Collection $certCollection.Import($certBytes, $null, [System.Security.Cryptography.X509Certificates.X509KeyStorageFlags]::Exportable) $password = (Get-AzkeyVaultSecret -VaultName $vaultName -Name \u0026#34;$prefix-password\u0026#34;).SecretValueText $secure = ConvertTo-SecureString -String $password -AsPlainText -Force $protectedCertificateBytes = $certCollection.Export([System.Security.Cryptography.X509Certificates.X509ContentType]::Pkcs12, $password) $pfxPath = \u0026#34;$tempStoreLocation\\$prefix.pfx\u0026#34; [System.IO.File]::WriteAllBytes($pfxPath, $protectedCertificateBytes) Import-PfxCertificate -FilePath \u0026#34;$tempStoreLocation\\$prefix.pfx\u0026#34; Cert:\\CurrentUser\\My -Password $secure } Finally we will check the import by executing a dir command\nGet-ChildItem Cert:\\CurrentUser\\My DevOps After finishing the PowerShell script lets make the DevOps task which will import the needed certificates. This is the YAML from the import certificates task.\nMake sure you have the vaultname variable created as pipeline variable.\nsteps: - task: AzurePowerShell@5 displayName: \u0026#39;import certificates\u0026#39; inputs: azureSubscription: ToMicrosoft365Customers ScriptPath: \u0026#39;$(System.DefaultWorkingDirectory)/_Microsoft365/import-certificatesFromKeyvault.ps1\u0026#39; ScriptArguments: \u0026#39;-vaultname $(vaultname) -tempStoreLocation \u0026#34;D:\\a\\_temp\\\u0026#34;\u0026#39; errorActionPreference: continue azurePowerShellVersion: LatestVersion pwsh: true From this place the certificates are in the local certificate store at the agent. Make sure you add the certificate dependent tasks in the same agent jobs. This because every agent job (with tasks) runs in an isolated environment.\nThank you for reading my blog using keyvault certificates in azure devops. I hope you got a bit inspired. Enjoy your day and happy automating üëã ","date":"October 13, 2020","image":"http://localhost:1313/using-keyvault-certificates-in-azure-devops/arm-with-keyvault.png","permalink":"/using-keyvault-certificates-in-azure-devops/","title":"Using KeyVault certificates in Azure DevOps"},{"categories":["Azure Virtual Desktop","Image Management"],"contents":"This is part two of a serie posts about AVD disk management. In this blogpost I will explain how to finish a disk with sysprep and deploy it as a version into the Azure Shared Image Gallery automated.\nThis post is a part of the series Azure Virtual Desktop Image Management Automated.\nCreate AVD image version based on existing config with PowerShell ‚Äì Part 1 Save AVD image with Sysprep as Image Gallery version ‚Äì Part 2 Create AVD Sessionhosts based on Shared Image Gallery version ‚Äì Part 3 WVD housekeeping, removing all unused sessionhosts, disks and images ‚Äì Part 4 Monitor Image Versions with Azure Monitor ‚Äì Part 5 Before continuing I will strongly recommend reading part 1 first. This part will continue based on part one and you will need the information from that part.\nTable Of Contents Steps overview Sysprep The technique Stop first Create a before sysprep snapshot Remote PowerShell Add port to firewall Remote Sysprep Shared Image Gallery Steps overview Basically there are 2 main steps. 1) Sysprep the machine, 2) creating a new version from a snapshot and put it as a version into the Shared Image Gallery.\nSysprep After your work is done you will need to run a Sysprep on the machine. Microsoft explains a Sysprep (Generalize) as following:\nSysprep removes all your personal account and security information, and then prepares the machine to be used as an image. For information about Sysprep, see sysprep overview.\nAs the article says you can Sysprep an image to the max of 8. To avoid reaching the limit we first going to make a clean Before Snapshot (BS).\nUpdate from Microsoft. It is now possible to run a sysprep up to 1001 times. With this in mind you should be able to skip the create before sysprep snapshot part.\nhttps://docs.microsoft.com/en-us/windows-hardware/manufacture/desktop/sysprep‚Äìgeneralize‚Äìa-windows-installation#limits-on-how-many-times-you-can-run-sysprep\n![image-1](image-1.png) #### Create version into the Share Image Gallery A little introduction about the Shared Image Gallery.\nShared Image Gallery is a service that helps you build structure and organization around your images. Shared Image Galleries provide:\nGlobal replication of images. Versioning and grouping of images for easier management. Highly available images with Zone Redundant Storage (ZRS) accounts in regions that support Availability Zones. ZRS offers better resilience against zonal failures. Premium storage support (Premium_LRS). Sharing across subscriptions, and even between Active Directory (AD) tenants, using RBAC. Scaling your deployments with image replicas in each region. Using a Shared Image Gallery you can share your images to different users, service principals, or AD groups within your organization. Shared images can be replicated to multiple regions, for quicker scaling of your deployments. For a full explanation check Shared Image Gallery overview.\nThe technique Lets do the official part like parameters and importing the needed modules first :). The required parameters are virtual machine, the one you have created at part one, the resource group where the virtual machine is in and of course the AVD hostpool.\nparam( [parameter(mandatory = $true, ValueFromPipelineByPropertyName)]$virtualMachineName, [parameter(mandatory = $true, ValueFromPipelineByPropertyName)]$resourceGroupName, [parameter(mandatory = $true, ValueFromPipelineByPropertyName)]$hostpoolName, [parameter(mandatory = $true, ValueFromPipelineByPropertyName)]$password, [parameter(mandatory = $true, ValueFromPipelineByPropertyName)]$username ) import-module az.compute $date = get-date -format \u0026#34;yyyy-MM-dd\u0026#34; $version = $date.Replace(\u0026#34;-\u0026#34;, \u0026#34;.\u0026#34;) Stop first Before creating a clean snapshot we first shutdown the virtual machine. After the virtual machineis shutdown completely we can create the snapshot. For checking the virtual machine status I have created a function.\nfunction test-VMstatus($virtualMachineName) { $vmStatus = Get-AzVM -name $virtualMachineName -resourcegroup $resourceGroupName -Status return \u0026#34;$virtualMachineName status \u0026#34; + (($vmstatus.Statuses | ? { $_.code -match \u0026#39;Powerstate\u0026#39; }).DisplayStatus) } # Stopping VM for creating clean snapshot Stop-AzVM -name $virtualMachineName -resourcegroup $resourceGroupName -Force -StayProvisioned do { $status = test-vmStatus -virtualMachineName $virtualMachineName $status } until ( $status -match \u0026#34;stopped\u0026#34;) The Stop-VM will use the -StayProvisioned flag. This will help keeping the virtual machine available at all time otherwise the virtual machine will be deallocated and will take much longer to start again. (and we will loose the public IP)\nThe do-until loop checks the virtual machine status as long it isn‚Äôt stopped yet. If the status is stopped the loop will break and continue.\nCreate a before sysprep snapshot Now lets gather the needed information and store it into variables.\n# If VM is stopped, create snapshot Before Sysprep $vm = Get-AzVM -name $virtualMachineName -ResourceGroupName $resourceGroupName $snapshot = New-AzSnapshotConfig -SourceUri $vm.StorageProfile.OsDisk.ManagedDisk.Id -Location $vm.location -CreateOption copy $snapshotName = ($vm.StorageProfile.OsDisk.name).Split(\u0026#34;-\u0026#34;) $snapshotName = $snapshotName[0] + \u0026#34;-\u0026#34; + $snapshotName[1] + \u0026#34;-\u0026#34; + $date + \u0026#34;-BS\u0026#34; Write-Output \u0026#34;Creating snapshot $snapshotName for $virtualMachineName\u0026#34; $createSnapshot = New-AzSnapshot -Snapshot $snapshot -SnapshotName $snapshotName -ResourceGroupName $resourceGroupName After the snapshot is finished we need to start the virtual machine again and test the status with the same function test-VmStatus. The loop will now continue after the virtual machine has a status Started.\n\u0026lt;# Source: https://docs.microsoft.com/nl-nl/azure/virtual-machines/windows/capture-image-resource # If snapshot is created start VM again and run a sysprep #\u0026gt; if ($null -eq $createSnapshot) { Write-Error \u0026#34;No snapshot created\u0026#34; break; } Start-AzVM -name $virtualMachineName -resourcegroup $resourceGroupName Write-Output \u0026#34;Snapshot created, starting machine.\u0026#34; do { $status = test-vmStatus -virtualMachineName $virtualMachineName $status } until ($status -match \u0026#34;running\u0026#34;) Remote PowerShell In the steps we need to use a remote PowerShell session to run commands on the virtual machine. Beforce we can setup a session we need to setup some important parts and have to request the virtual machine public IP. Also we need to enable the remote PowerShell extention.\n# Get the VM\u0026#39;s public IP $virtualMachinePublicIp = (Get-AzPublicIpAddress | where { $_.name -match $VirtualMachineName }).IpAddress $virtualNetworkSubnet = (Get-AzNetworkInterface -ResourceId $vm.NetworkProfile.NetworkInterfaces.id).IpConfigurations.subnet.id $NSG = Get-AzNetworkSecurityGroup | ? { $_.subnets.id -eq $virtualNetworkSubnet }# Enabling Powershell Remote Extention Write-Output \u0026#34;Enabling Powershell Remote Extention\u0026#34; Invoke-AzVMRunCommand -CommandId \u0026#34;EnableRemotePS\u0026#34; -VM $vm Add port to firewall The second part is adding a firewall rule for port 5986, which is the ssl port for remote PowerShell connection url.\nNow we need the PowerShell function add-firewallRule from part 1 again. After adding the port we will setup a remote PowerShell session.\n#Adding the role add-firewallRule -NSG $NSG -localPublicIp $localPublicIp -port 5986 $connectionUri = \u0026#34;https://\u0026#34; + $virtualMachinePublicIp + \u0026#34;:5986\u0026#34; [securestring]$secStringPassword = ConvertTo-SecureString $password -AsPlainText -Force [pscredential]$creds = New-Object System.Management.Automation.PSCredential ($userName, $secStringPassword) $session = $null while (!($session)) { $session = New-PSSession -ConnectionUri $connectionUri -Credential $creds -SessionOption (New-PSSessionOption -SkipCACheck -SkipCNCheck -SkipRevocationCheck) Write-Output \u0026#34;Creating Remote Powershell session\u0026#34; $session } Remote Sysprep When the session has been established we are able to run the Sysprep command remotely.\n$setRegCommand = \u0026#34;Set-Itemproperty -path \u0026#39;HKLM:\\SYSTEM\\Setup\\Status\\SysprepStatus\u0026#39; -Name \u0026#39;GeneralizationState\u0026#39; -value 3\u0026#34; $sysprep = \u0026#39;C:\\Windows\\System32\\Sysprep\\Sysprep.exe\u0026#39; $arg = \u0026#39;/generalize /oobe /shutdown /quiet\u0026#39; Invoke-Command -Session $session -ScriptBlock { param($sysprep, $arg) Start-Process -FilePath $sysprep -ArgumentList $arg } -ArgumentList $sysprep, $arg Write-Output \u0026#34;Sysprep command started on $virtualMachineName, now waiting till the vm is stopped.\u0026#34; Now the big wait will start till the VM has been shutdown. Again we will run a do-while loop till the virtual machine has been deallocated. Because of a Sysprep can take a while we will add a pause of 5 minutes into the loop.\n# After sysprep check if vm is stopped and deallocated do { $status = test-vmStatus -virtualMachineName $virtualMachineName $status Start-Sleep 300 } until ($status -like \u0026#34;stopped\u0026#34;) #\u0026gt; # When the VM is stopped it is time to generalize the VM Set-AzVm -ResourceGroupName $resourceGroupName -Name $virtualMachineName -Generalized Shared Image Gallery We are going back to the AVD environment and will, just as in part 1, start at the AVD hostpool. From that point we will do some reverse engineering to get all the critical information.\n# Testing if there is allready a AVD VM with an update status $hostpool = Get-AzWvdHostPool | ? { $_.Name -eq $hostpoolname } # Creating VM configuration based on existing VM\u0026#39;s in specific hostpool, selecting first one $hostpoolResourceGroup = ($hostpool).id.split(\u0026#34;/\u0026#34;)[4] Write-Output \u0026#34;Hostpool resourcegroup is $hostpoolResourceGroup \u0026#34; # Get one of the current production VM\u0026#39;s for getting the share image gallery info $sessionHosts = Get-AzWvdSessionHost -ResourceGroupName $hostpoolResourceGroup -HostPoolName $hostpool.name $existingSessionHost = ($sessionHosts.Name.Split(\u0026#34;/\u0026#34;)[-1]).Split(\u0026#34;.\u0026#34;)[0] $productionVm = Get-AzVM -Name $existingSessionHost # Get the VM for creating new image based on connected disk $diskName = $vm.StorageProfile.OsDisk.name # Replace the Before Sysprep to After Sysprep $imageName = $diskname.Replace(\u0026#34;BS\u0026#34;, \u0026#34;AS\u0026#34;) $image = New-AzImageConfig -Location $vm.location -SourceVirtualMachineId $vm.Id # Create the image based on the connected disk on the update VM Write-Output \u0026#34;Creating image $imageName based on $($vm.name)\u0026#34; New-AzImage -Image $image -ImageName $imageName -resourcegroupname $productionVm.ResourceGroupName $managedImage = Get-AzImage -ImageName $imageName -resourcegroupname $productionVm.ResourceGroupName # Source: https://docs.microsoft.com/en-us/azure/virtual-machines/image-version-managed-image-powershell # Creating image version based on the image created few steps ago $imageReference = ((get-azvm -Name $productionVm[-1].name -ResourceGroupName $productionVm[-1].ResourceGroupName).storageprofile.ImageReference).id $galleryImageDefintion = get-AzGalleryImageDefinition -ResourceId $imageReference $galleryName = $imageReference.Split(\u0026#34;/\u0026#34;)[-3] $gallery = Get-AzGallery -Name $galleryName # Configuring paramaters $imageVersionParameters = @{ GalleryImageDefinitionName = $galleryImageDefintion.Name GalleryImageVersionName = $version GalleryName = $gallery.Name ResourceGroupName = $gallery.ResourceGroupName Location = $gallery.Location Source = $managedImage.id.ToString() } # Doing the job New-AzGalleryImageVersion @imageVersionParameters $bodyValues = [Ordered]@{ hostPool = $hostpoolName virtualMachineName = $VirtualMachineName resourceGroupName = $resourceGroupName virtualMachinePublicIp = $virtualMachinePublicIp username = $username password = $password } $bodyValues Creating an image can take a couple of minutes.\nAfter all when the PowerShell is finished you will get an image overview. At that same moment you will notice the image version provisioning state has been succeeded.\nI have added the full script at my AVD Github repository\nThank you for reading my blog azure virtual desktop image management automated - part 2 save avd image with sysprep as image gallery version. I hope you got a bit inspired. Enjoy your day and happy automating üëã ","date":"October 6, 2020","image":"http://localhost:1313/save-avd-image-with-sysprep-as-image-gallery-version/shared-image-gallery.png","permalink":"/save-avd-image-with-sysprep-as-image-gallery-version/","title":"Azure Virtual Desktop Image Management Automated - Part 2 Save AVD image with Sysprep as Image Gallery version"},{"categories":["Microsoft 365","Monitoring"],"contents":"You have some Microsoft 365 (M365) customers which you like to monitor. Every day you are looking at the customer specific M365 portal looking for Microsoft event.\nLogging in at the customers portal isn‚Äôt a very efficient way. Microsoft provides a lot of API‚Äôs which can be used for monitoring. By combining some of them you are able to do some really nice and smart things.\nTable Of Contents Getting the messages Filtering Tenant Info Bring the parts together As you may properly know Microsoft provide a ServiceHealth API for monitoring the Microsoft 365 services. By requesting API you get all messages even from services you don‚Äôt use. Well there is a way to filter only the services you use in your tenant.\nGetting the messages In my blogpost about authenticating against the API with PowerShell i‚Äôve wrote in basics how to get an authentication token which you need for requesting an API.\nHow to use RESTAPI with Powershell\nFirst you need to request the ServiceHealth message API. This API shows every message for the past few weeks.\nGET https://manage.office.com/api/v1.0/contoso.com/ServiceComms/Messages To get the right token you need to authenticate to manage.office.com. Authenticating against the wrong resource will result in a ‚Äúinvalid audience‚Äù error\nIn PowerShell the request looks like this:\n$tenantId = xxx $resource = \u0026#34;https//manage.office.com\u0026#34; $clientId = \u0026#34;the application id\u0026#34; $clientSecret = \u0026#34;The secret you have created under secrets\u0026#34; $body = @{grant_type = \u0026#34;client_credentials\u0026#34;; resource = $resource; client_id = $ClientId; client_secret = $ClientSecret } $oauth = Invoke-RestMethod -Method Post -Uri \u0026#34;https://login.microsoftonline.com/$($tenantID)/oauth2/token?api-version=1.0\u0026#34; -Body $body $token = @{\u0026#39;Authorization\u0026#39; = \u0026#34;$($oauth.token_type) $($oauth.access_token)\u0026#34; } $uri = \u0026#34;https://manage.office.com/api/v1.0/$tenantId/ServiceComms/Messages\u0026#34; $allMessages = Invoke-RestMethod -Uri $uri -Headers $token -Method Get $allMessages.Value After running these commands you get some output like this:\nAffectedWorkloadDisplayNames : {} AffectedWorkloadNames : {} Status : Service restored Workload : SharePoint WorkloadDisplayName : SharePoint Online ActionType : AdditionalDetails : {@{Name=NotifyInApp; Value=True}} AffectedTenantCount : 0 AffectedUserCount : Classification : Incident EndTime : 9/24/2020 6:14:10 PM Feature : customsolutionsworkflows FeatureDisplayName : Custom Solutions and Workflows UserFunctionalImpact : Id : SP222774 ImpactDescription : Users did not receive email notifications from SharePoint Online. LastUpdatedTime : 9/24/2020 6:21:20 PM MessageType : Incident Messages : {@{MessageText=Title: Some users are not receiving email notifications through SharePoint Online User Impact: Users are not receiving email notifications through SharePoint Online. Current status: We\u0026#39;re investigating a potential issue with SharePoint Online where users are not receiving email notifications through SharePoint Online. We\u0026#39;ll provide an update within 30 minutes.; PublishedTime=9/24/2020 6:56:49 AM}, @{MessageText=Title: Some users are unable to receive email notifications in SharePoint Online Next update by: Thursday, September 24, 2020, at 1:00 PM UTC; PublishedTime=9/24/2020 10:37:34 AM}‚Ä¶} PostIncidentDocumentUrl : Severity : Sev2 StartTime : 9/24/2020 6:53:54 AM TenantParams : {} Title : Users did not received email notifications from SharePoint Online Filtering Now we have all the Microsoft 365 health messages. If you want to get messages about specific Microsoft services you need to do some filtering at the API URL. See below for an example for Teams. If you have one tenant this will do, if you have more tenants, like MSP‚Äôs, it will grab Teams messages over every tenant. This will result in false positives if a tenant don‚Äôt use Microsoft Teams.\nhttps://manage.office.com/api/v1.0/$tenantId/ServiceComms/Messages?$filter=Workload eq \u0026#39;Teams\u0026#39; Writing tenant specific filter queries will work if you have a few. If you have many more you should consider an other option. One of the options I will explain below.\nTenant Info There is a way how to determine which services a tenant is using automatically. A M365 tenant has a security page which tells you about your tenants security status. The status page will be refreshed every 24 hours.\nBehind the status page there are some API‚Äôs available which provide security info and tenant specific details like enabled services.\nGET https://graph.microsoft.com/beta/security/securescores?`$top=1 Not using the $top=1 filter will result in 90 results. Only the most recent value is fine for now. After requesting the API the info you will get look like this:\nid : 85bxxxca-b5c4-4xxf-a869-d21xxxx992c9_2020-09-28 azureTenantId : 85bxxxca-b5c4-4xxf-a869-d21xxxx992c9 activeUserCount : 1288 createdDateTime : 9/28/2020 12:00:00 AM currentScore : 70 enabledServices : {HasExchange, HasProject, HasSharePoint, HasOD4B‚Ä¶} licensedUserCount : 502177 maxScore : 238 As you can see it has an object call enabledServices. After doing some PowerShell magic manipulating the string you only have the services itself.\nThe security API ‚Äòlives‚Äô at a Microsoft different part so you will need to get a new token. Authenticate to graph.microsoft.com.\nAfter receiving the security token you can run the following PowerShell command:\n$uri = https://graph.microsoft.com/beta/security/securescores?`$top=1 $query = Invoke-WebRequest -Method $method -Uri $uri -ContentType \u0026#34;application/json\u0026#34; -Headers @{Authorization = \u0026#34;Bearer $token\u0026#34; } -ErrorAction Stop $ConvertedOutput = $query | Select-Object -ExpandProperty content | ConvertFrom-Json $convertedoutput.value.enabledServices.replace(\u0026#34;Has\u0026#34;, $null) Exchange Project SharePoint OD4B Yammer The output after removing ‚Äúhas‚Äù is exactly the same as the message Workload content shown at the beginning of this post.\nWorkload : SharePoint Bring the parts together Having those two parts we can bring these two parts together. First we need to gather all the messages into a variable. After getting the messages we are going to load the tenantinfo in a variable too.\nI wrote a function for getting the tenant information.\nfunction get-tenantInfo { param ( [Parameter (Mandatory = $true)][object] $tenantId, [Parameter (Mandatory = $true)][object] $clientId, [Parameter (Mandatory = $true)][object] $clientSecret ) $results = @() # Construct URI $uri = \u0026#34;https://login.microsoftonline.com/$tenantId/oauth2/v2.0/token\u0026#34; # Construct Body $body = @{ client_id = $clientId scope = \u0026#34;https://graph.microsoft.com/.default\u0026#34; client_secret = $clientSecret grant_type = \u0026#34;client_credentials\u0026#34; } write-host \u0026#34;Get OAuth 2.0 Token\u0026#34; # Get OAuth 2.0 Token $tokenRequest = Invoke-WebRequest -Method Post -Uri $uri -ContentType \u0026#34;application/x-www-form-urlencoded\u0026#34; -Body $body -UseBasicParsing # Access Token $token = ($tokenRequest.Content | ConvertFrom-Json).access_token # Graph API call in PowerShell using obtained OAuth token (see other gists for more details) # Specify the URI to call and method $method = \u0026#34;GET\u0026#34; $uri = \u0026#34;https://graph.microsoft.com/beta/security/securescores?`$top=1\u0026#34; write-host \u0026#34;Run Graph API Query\u0026#34; # Run Graph API query $query = Invoke-WebRequest -Method $method -Uri $uri -ContentType \u0026#34;application/json\u0026#34; -Headers @{Authorization = \u0026#34;Bearer $token\u0026#34; } -ErrorAction Stop write-host \u0026#34;Parse results\u0026#34; $ConvertedOutput = $query | Select-Object -ExpandProperty content | ConvertFrom-Json write-host \u0026#34;Display results`n\u0026#34; foreach ($obj in $convertedoutput.value) { $mainCustomerObject = [PSCustomObject][Ordered]@{ objectId = $obj.id tenantId = $obj.azureTenantId activeUsers = $obj.activeUserCount date = $obj.createdDateTime enabledServices = ($obj.enabledServices).replace(\u0026#34;Has\u0026#34;, $null) currentScore = $obj.currentScore maxPossibleScore = $obj.maxScore } $results += $mainCustomerObject } return $results } $tenantParameters = @{ tenantId = xxx clientId = xxx clientSecret = xxx } $uri = \u0026#34;https://manage.office.com/api/v1.0/$tenantId/ServiceComms/Messages\u0026#34; $allMessages = Invoke-RestMethod -Uri $uri -Headers $token -Method Get $healthMessages = $allMessages.Value $tenantInfo = get-TenantInfo @tenantParameters $allMessages | ? { $tenantInfo.enabledServices -match $_.Workload } Now you have messages fitting the services you use. I‚Äôm using Azure Functions which is checking every x time for new messages.\nThank you for reading my blog monitor active m365 servicehealth services only with powershell. I hope you got a bit inspired. Enjoy your day and happy automating üëã ","date":"September 29, 2020","image":"http://localhost:1313/monitor-active-m365-servicehealth-services-only-with-powershell/image-1146.png","permalink":"/monitor-active-m365-servicehealth-services-only-with-powershell/","title":"Monitor active M365 ServiceHealth services only with PowerShell"},{"categories":["Azure Virtual Desktop","Image Management"],"contents":"This article is part one of a series of posts about AVD image management automated. In this first part, I will describe how to create and connect a new disk (based on a snapshot) to a new Azure VM based on the existing sessionhost configuration. This will save a lot of extra parameters like VM size, network settings, and type. After the VM is started you will get the information on how to connect to the VM by RDP (3389) with specific credentials specially created for this VM.\nUpdated 14-03-2021\nThis post is a part of the series AVD Image Management Automated.\nCreate AVD image version based on existing config with PowerShell ‚Äì Part 1 Save AVD image with Sysprep as Image Gallery version ‚Äì Part 2 Create AVD Session hosts based on Shared Image Gallery version ‚Äì Part 3 AVD housekeeping, removing all unused session hosts, disks, and images ‚Äì Part 4 Monitor Image Versions with Azure Monitor ‚Äì Part 5 Table Of Contents AVD in a nutshell Requirements PowerShell Modules Used Azure components Finding hostpool and session hosts Disk configuration Networking Create VM Generate credentials How to use Results AVD in a nutshell Azure Virtual Desktop is a desktop and app virtualization service that runs on the cloud. For a full description click here\nIn the basic AVD exists in three big parts: clients, AVD, and the Azure VM‚Äôs \u0026amp; Services. As the picture below says only AVD is Microsoft managed. For that part you can use ARM templates and you are out of control at that part.\nThe real fun starts at the dynamic environment like clients and VM‚Äôs. In this article we will focus how to deal with disk- and image management automatically.\nSince AVD has no image provisioning like Citrix, disk management goes a bit different. Before knowing how to automate things it is good to know which elements we need and how the tasks need to be done when an image needs to be updated.\nRequirements PowerShell Modules Microsoft has enrolled a new PowerShell module for Windows Virtual Desktop. For executing commands in this article you need this module. How to setup this module please check https://docs.microsoft.com/en-us/azure/virtual-desktop/powershell-module\nWe also need the az.network and az.compute PowerShell Modules\nUsed Azure components Windows Virtual Desktop Hostpool Virtual Machines (sessions hosts in a hostpool) Disks \u0026amp; Snapshots Network Shared Image Gallery Finding hostpool and session hosts In this case, I assume you already have an AVD environment with a hostpool, and session hosts. I will talk about AVD environment deployment later.\nFirst, we need to know in which hostpool you want to create a new disk. So the script will ask you for that. After all, actually, these are the only variables you really need :). Since the hostpool is the main AVD part we know every other component.\nI‚Äôve added an extra variable $localPublicIp. I will talk about this later in this topic. I‚Äôm also importing the needed modules.\nparam( [parameter(mandatory = $true)][string]$hostpoolName, [parameter(mandatory = $true)][string]$snapshotName, [parameter(mandatory = $true)][string]$localPublicIp ) import-module az.desktopvirtualization import-module az.network import-module az.compute # Get AVD hostpool information $hostpool = Get-AzWvdHostPool | Where-Object { $_.Name -eq $hostpoolname } $hostpoolResourceGroup = ($hostpool).id.split(\u0026#34;/\u0026#34;)[4] # Get current AVD Configurgation $sessionHosts = Get-AzWvdSessionHost -ResourceGroupName $hostpoolResourceGroup -HostPoolName $hostpool.name $sessionHostName = ($sessionHosts.Name.Split(\u0026#34;/\u0026#34;)[-1]).Split(\u0026#34;.\u0026#34;)[0] $currentVmInfo = Get-AzVM -name $sessionHostName $virtualMachineSize = $currentVmInfo.hardwareprofile.vmsize $virtualNetworkSubnet = (Get-AzNetworkInterface -ResourceId $currentVmInfo.NetworkProfile.NetworkInterfaces.id).IpConfigurations.subnet.id Now we have our hostpool and session hosts loaded, the next step is the needed snapshot. This should be a snaphost which has been not syspreped before. This is because of its limits.\nYou can run the Sysprep command up to 8 times on a single Windows image. After running Sysprep 8 times, you must recreate your Windows image. In previous versions of Windows, you could use the SkipRearm answer file setting to reset the Windows Product Activation clock when running Sysprep. If you are using a volume licensing key or a retail product key, you don‚Äôt have to use SkipRearm because Windows is automatically activated.\nMore information about sysprep a Windows installation check the Microsoft docs.\n# Snapshot values for creating a disk try { $snapshot = get-azsnapshot -SnapshotName $snapshotname $resourceGroupName = $snapshot.ResourceGroupName } catch { Throw \u0026#34;No snapshot found, $_\u0026#34; } After loading all the basics it is time to deploy things to Azure. Because of the number of temporary components, I will create a new resource group first. This will help you clean up resources at the end since every component is in the same resource group. Just deleting the resource group will be fine then.\n# Creating a new temporary resource group first $ResourceGroup = New-AzResourceGroup -Name $TempResourceGroup -Location $ResourceGroupLocation Disk configuration When creating a new disk you need to set up a disk configuration. After then you can create a new disk if it not exists already.\n$VirtualMachineName = (\u0026#39;vm_\u0026#39; + $snapshot.name) # Creating a disk $diskConfig = New-AzDiskConfig -SkuName \u0026#34;Premium_LRS\u0026#34; -Location $ResourceGroup.location -CreateOption Copy -SourceResourceId $snapshot.Id $diskname = ($VirtualMachineName.ToLower()+ \u0026#39;-OS\u0026#39;) $disk = Get-azdisk -diskname $diskname try { $disk = New-AzDisk -Disk $diskConfig -ResourceGroupName $ResourceGroup.resourceGroupName -DiskName $diskName } catch { Throw \u0026#34;$diskname allready exits, $_\u0026#34; } Networking The next step is the networking part. This consists of three components. A network interface card, public ip and a network security group. In the first part I will create a public IP. This will be the IP to connect when the virtual machine is finished.\nIn the next step I will create a network interface card and will connect the public IP to it.\nIn the third step I will create a Network Security Group (NSG) to protect the virtual machine for attackers. The NSG will be connected to the virtual machine only. Positive side effect is we leave the production NSG which is on the AVD subnet untouched.\nAt last I will add the RDP port 3389 to the NSG with my private IP only instead of the whole internet. The function Add-FirewallRule can be found in the complete script at my GitHub page.\n$PublicIpParameters = @{ Name = ($VirtualMachineName.ToLower() + \u0026#39;_ip\u0026#39;) ResourceGroupName = $ResourceGroup.ResourceGroupName Location = $ResourceGroup.Location AllocationMethod = \u0026#39;Dynamic\u0026#39; Force = $true } $publicIp = New-AzPublicIpAddress @PublicIpParameters $NicParameters = @{ Name = ($VirtualMachineName.ToLower() + \u0026#39;_nic\u0026#39;) ResourceGroupName = $ResourceGroup.resourceGroupName Location = $ResourceGroup.Location SubnetId = $virtualNetworkSubnet PublicIpAddressId = $publicIp.Id Force = $true } $nic = New-AzNetworkInterface @NicParameters # Creating a temporary network security group $NsgParameters = @{ ResourceGroupName = $ResourceGroup.ResourceGroupName Location = $ResourceGroup.Location Name = ($VirtualMachineName.ToLower() + \u0026#39;_nsg\u0026#39;) } $nsg = New-AzNetworkSecurityGroup @NsgParameters # Adding a security rule to only the network interface card Add-FirewallRule -NSG $NSG -localPublicIp $localPublicIp -port 3389 $nic.NetworkSecurityGroup = $nsg $nic | Set-AzNetworkInterface Create VM Now the network is in place and save let‚Äôs create a new VM with a new disk based on a snapshot.\n# Creating virtual machine configuration $VirtualMachine = New-AzVMConfig -VMName $VirtualMachineName -VMSize $virtualMachineSize # Use the Managed Disk Resource Id to attach it to the virtual machine. Please change the OS type to linux if OS disk has linux OS $VirtualMachine = Set-AzVMOSDisk -VM $VirtualMachine -ManagedDiskId $disk.Id -CreateOption Attach -Windows # Create a public IP for the VM $VirtualMachine = Add-AzVMNetworkInterface -VM $VirtualMachine -Id $nic.Id #Create the virtual machine with Managed Disk $newVm = New-AzVM -VM $VirtualMachine -ResourceGroupName $ResourceGroup.resourceGroupName -Location $ResourceGroup.Location When the VM is created we create an username and password in the VM by installing the VMAccessAgent extention.\nhttps://docs.microsoft.com/en-us/troubleshoot/azure/virtual-machines/support-agent-extensions\nGenerate credentials First we need to create a random username and password. I also created a function for that as well, which can be found in the complete script.\n$userName = create-randomString -type \u0026#39;username\u0026#39; $password = ConvertTo-SecureString (create-randomString -type \u0026#39;password\u0026#39;) -AsPlainText -Force $Credential = New-Object System.Management.Automation.PSCredential ($userName, $password); $CredentialParameters = @{ ResourceGroupName = $ResourceGroup.ResourceGroupName Location = $ResourceGroup.Location VMName = $VirtualMachineName Credential = $Credential typeHandlerVersion = \u0026#34;2.0\u0026#34; Name = \u0026#34;VMAccessAgent\u0026#34; } Set-AzVMAccessExtension @CredentialParameters How to use An example of how to use this script.\n$hostpoolname = \u0026#39;avd-hostpool\u0026#39; $snapshotname = \u0026#39;avd-acc-per-2021.03.12-BS\u0026#39; $TempResourceGroup = \u0026#39;temp-deploy-updates\u0026#39; $ResourceGroupLocation = \u0026#39;westeurope\u0026#39; $localPublicIp = \u0026#39;127.0.0.1\u0026#39; .\\WVD-Create-UpdateVM.ps1 -HostpoolName $hostpoolname -SnapshotName $snapshotname -TempResourceGroup $TempResourceGroup -ResourceGroupLocation $ResourceGroupLocation -localPublicIp $localPublicIp Results At last we combining everything we know together to write the content to our screen.\nif ($newVm) { #Adding the role $publicIp = (Get-AzPublicIpAddress | where { $_.name -match $VirtualMachineName }).IpAddress $bodyValues = [Ordered]@{ Status = $newVm.StatusCode hostPool = $hostpoolName virtualMachineName = $VirtualMachineName resourceGroupName = $ResourceGroup.ResourceGroupName virtualMachinePublicIp = $publicIp username = $userName password = $password | ConvertFrom-SecureString -AsPlainText virtualMachineDisk = $diskname engineersIp = $localpublicIp } } Write-Output $bodyValues The whole script from this article can be found at my Github Windows Virtual Desktop repository.\nIn the next episode I will describe how to finish the disk with an automated sysprep and moving the disk as a version into the Azure Shared Image Gallery.\nThank you for reading my blog post about AVD Image Management Automated. Thank you for reading my blog avd image management automated - part 1 create an avd image version based on the existing config with powershell. I hope you got a bit inspired. Enjoy your day and happy automating üëã ","date":"September 28, 2020","image":"http://localhost:1313/create-avd-image-version-based-on-existing-config-with-powershell/building-blocks.jpg","permalink":"/create-avd-image-version-based-on-existing-config-with-powershell/","title":"AVD Image Management Automated - Part 1 Create an AVD image version based on the existing config with PowerShell"},{"categories":["Graph API","Powershell"],"contents":"The past few years REST API became important more and more. REST API is a common way for communicating against and between applications. Azure and Microsoft 365 also using REST API‚Äôs.\nAs the world of API is growing so the tooling for querying API‚Äôs is growing with the most well known tool is Postman.\nBut what if you need API output for automation purposes, Postman isn‚Äôt the tool you need. So in my case I use Powershell.\nTable Of Contents Register an app in Azure AD Get a token in Powershell Whats next Register an app in Azure AD If you use Powershell you need a couple of things before you can send an authentication request. First of all you need to register an app into the Azure Active Directory. How to accomplish this goal manual you can take a look at the following url: https://docs.microsoft.com/en-us/graph/auth-register-app-v2.If you want to register an application automated with code you can check my post about application registration automated.\nAfter the application is finished and you have set an application secret you can use the applicationId and secret in PowerShell.\nGet a token in Powershell Now the needed application is finished we can start with the Powershell part. Good to know is there are some different Microsoft parts where you can authenticate with.\nFrom Azure perspective most of the time you need the Graph API so you can authenticate with https://graph.microsoft.com.\nWhen you want to query Microsoft 365 you will need https://manage.office.com.\nIn Powershell the code looks like this\n$resource = \u0026#34;https//graph.microsoft.com\u0026#34; $tenantId = \u0026#34;the tenant you want to authenticate to\u0026#34; $clientId = \u0026#34;the application id\u0026#34; $clientSecret = \u0026#34;The secret you have created under secrets\u0026#34; $body = @{grant_type = \u0026#34;client_credentials\u0026#34;; resource = $resource; client_id = $ClientId; client_secret = $ClientSecret } $oauth = Invoke-RestMethod -Method Post -Uri \u0026#34;https://login.microsoftonline.com/$($tenantID)/oauth2/token?api-version=1.0\u0026#34; -Body $body $token = @{\u0026#39;Authorization\u0026#39; = \u0026#34;$($oauth.token_type) $($oauth.access_token)\u0026#34; } Whats next Now you have a token in the $token variable. After then you can query against API‚Äôs. For example the Microsoft 365 ServiceHealth. In this case i‚Äôve used the GET method.\n$uri = \u0026#34;https://manage.office.com/api/v1.0/$($tenantId)/ServiceComms/CurrentStatus\u0026#34; Invoke-RestMethod -Uri $uri -Headers $token -Method Get Some other common used methods are POST (create a new record) or PATCH (partial updating records)\nYou can find a whole explanation about REST API‚Äôs at https://docs.microsoft.com/en-us/graph/use-the-api\nThank you for reading my blog how to use rest api with powershell. I hope you got a bit inspired. Enjoy your day and happy automating üëã ","date":"September 15, 2020","image":"http://localhost:1313/how-to-use-rest-api-with-powershell/Image-1149.png","permalink":"/how-to-use-rest-api-with-powershell/","title":"How to use REST API with Powershell"},{"categories":null,"contents":"Nothing on this page will be visible. This file exists solely to respond to /search URL.\nSetting a very low sitemap priority will tell search engines this is not important content.\n","date":"January 1, 1","image":"http://localhost:1313/how-to-use-rest-api-with-powershell/Image-1149.png","permalink":"/search/","title":"Search Results"}]